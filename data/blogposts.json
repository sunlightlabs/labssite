[{"pk": 106, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-06-01 15:56:02", "author": 6, "timestamp": "2009-06-01 15:56:02", "markup": "markdown", "title": "Us: Transparency, Them: Collaboration", "excerpt": "Today, the White House, via the [Office of Science and Technology Policy](http://ostp.gov) released the Government \"[Conversation on Collaboration](http://www.ostp.gov/galleries/opengov/Conversation+on+Collaboration.html)\" that they've been having since February, in conjuction with the public Open Government Dialogue. This conversation happened on OMBMax wiki, a wiki powered by the Office of Management and Budget.\r\n\r\nSo after our analysis on Friday of [The Open Government Dialogue](http://sunlightlabs.com/blog/2009/05/29/open-government-brainstorm/) this gives us the opportunity to make a comparison-- what are people inside the government saying vs. the general public?\r\n\r\nNow we can see what people are saying inside the government and outside the government. I went ahead and used the rest of Sunlight's \"word cloud\" credit on creating a new word cloud of what people are saying on the inside, so we could put it next to one on the outside. Here it is:", "content": "Today, the White House, via the [Office of Science and Technology Policy](http://ostp.gov) released the Government \"[Conversation on Collaboration](http://www.ostp.gov/galleries/opengov/Conversation+on+Collaboration.html)\" that they've been having since February, in conjuction with the public Open Government Dialogue. This conversation happened on OMBMax wiki, a wiki powered by the Office of Management and Budget.\r\n\r\nSo after our analysis on Friday of [The Open Government Dialogue](http://sunlightlabs.com/blog/2009/05/29/open-government-brainstorm/) this gives us the opportunity to make a comparison-- what are people inside the government saying vs. the general public?\r\n\r\nNow we can see what people are saying inside the government and outside the government. I went ahead and used the rest of Sunlight's \"word cloud\" credit on creating a new word cloud of what people are saying on the inside, so we could put it next to one on the outside. Here it is:\r\n\r\n<img src=\"http://dl-client.getdropbox.com/u/36193/outsideinside-opengov.gif\" />\r\n\r\nNow, this isn't exactly an apples-to-apples comparison. For the Open Government Dialogue word cloud, I used only titles of ideas. Since there were no titles for the OMBMax wiki posting, I used the full text of everything. I tried to remove some of the less substantive words from the wiki posting like month names, and noise-words like \"all\" and \"already\" to get it to a more meaningful post. Also: on the Open Gov Dialogue site, there were over 1000 ideas, whereas the OpenGov wiki had only about 10% of that diversity.\r\n\r\nBut it looks like a disconnect: on the outside, people are talking data and transparency. On the inside, people are talking collaboration and tools. Looking closer at the Open Gov dialogue, we can take the average scores of each category and see what people voted up more, too:\r\n\r\n<img src=\"http://img.skitch.com/20090601-n2gdfdertwarue49xx9wxt6iec.jpg\" alt=\"summary\"/>\r\n\r\n<br/>\r\n\r\nThe average scores tend to correlate with the word cloud for the Open Government Dialogue. Not only did people say transparency more, but on average, making data more available got more votes than building collaborative tools. Remarkably, people aren't into participation or collaboration that much according to the votes on the Open Government Dialogue. If you bundle each subject into its parent category, here's what you end up with for average votes:\r\n\r\n<img src=\"http://img.skitch.com/20090601-jb8unmsb9cay5hryughca8pder.jpg\" alt=\"summary\"/>\r\n\r\n<br/>\r\nThis is a pretty interesting find with the data. It seems to me like there may be a sign that there is a hefty disconnect between what people on the outside want (data, policy changes), and what people on the inside want to build (collaborative tools, web applications). Now here's something that hopefully we'll be engaged in, in terms of discussion for when the next phase of the Open Government Dialogue begins later this week. \r\n\r\nUntil then, Sunlight Labs will be on a brief word cloud hiatus.", "date_published": "2009-06-01 15:58:08", "comment_count": 3, "slug": "what-theyre-saying-what-were-saying", "tags": "opengovdialogue ostp omb wordcloud"}}, {"pk": 105, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-05-29 11:27:19", "author": 6, "timestamp": "2009-05-29 11:27:19", "markup": "markdown", "title": "The Open Government Brainstorm", "excerpt": "", "content": "The [Open Government Brainstorm](http://opengov.ideascale.com/) is wrapping up, and a lot of people had a lot to say. Over 1000 ideas got published and voted on. You can see all of them, too-- from most popular: [\"Support a 72-Hour Mandatory Public Review Period on Major Spending Bills\"](http://opengov.ideascale.com/akira/dtd/2459-4049) to the least popular: [\"Create a new Administration\"](http://opengov.ideascale.com/akira/dtd/2650-4049). My personal favorite was obviously [\"Free Pizza Fridays\"](http://opengov.ideascale.com/akira/dtd/3455-4049) but alas, I guess the community didn't feel as though it had much relevance-- it ended with a score of -25. \r\n\r\nThere's actually some *great* ideas here, and also some interesting metadata. Of course we made our traditional gratuitous tag cloud:\r\n\r\n<a href=\"http://skitch.com/cjoh/b1d6s/wordle-applet\"><img src=\"http://img.skitch.com/20090529-t9pr61xdtrrax4y4dikmihw36d.jpg\" alt=\"Wordle Applet\"/></a>\r\n\r\nI made some editorial choices here: I removed the words: \"Federal\",\"Government\",\"Open\" and \"Dialogue\" as they were extremely prevalent and really didn't have a lot of relevance. Going beyond tag clouds though, you get to see some other interesting data. For instance, looking at the major categories of Transparency, Participation, Collaboration, Capacity Building, Legal & Policy Challenges, and Uncategorized (other) you can see which categories people have the most ideas in: \r\n\r\n<img src=\"http://spreadsheets.google.com/pub?key=r1ZFVJFoIhCbDyx70SduU1w&oid=1&output=image\" />\r\n\r\nTransparency and Participation clearly got the majority of the ideas-- and they must have expected this. The number of ideas in each major category is correlated to their order on the sidebar on the Open Gov dialogue page. But what if you get into votes? How does that fare? Which categories had the best average?\r\n\r\nAll in all, there's some real treasures in there. If you're interested in the [Apps for America 2](http://sunlightlabs.com/contests/appsforamerica2) contest, then you may find some inspiration amongst the ideas. For me personally, I found that the ideas that received the most votes are the ones with institutions behind them-- so they may not be examples of the best ideas, but rather examples of the ones with the best organizers behind them. \r\n\r\nThe real question is: what's next? Apparently the next phase is \"Discuss\" to dig deeper on the ideas discussed on the brainstorm pages. That starts late next week.", "date_published": "2009-05-29 15:58:44", "comment_count": 0, "slug": "open-government-brainstorm", "tags": "opengov dialogue napa executive"}}, {"pk": 104, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-05-28 14:18:06", "author": 6, "timestamp": "2009-05-28 14:18:06", "markup": "markdown", "title": "What I'd Change about Data.gov", "excerpt": "I think Data.gov is pretty awesome. So let me couch my sensational headline with the fact that I'm generally a fan of what Vivek Kundra & Team are trying to do inside of the federal government to make the our country more transparent. Heck, we're so excited about it we're doing our own [contest](http://sunlightlabs.com/contests/appsforamerica2) with cash prizes to celebrate.\r\n\r\nBut I do have a few gripes. So in the interest of full transparency, and the hopes that this will create change, here are my gripes for all to see:", "content": "I think Data.gov is pretty awesome. I'm generally a fan of what Vivek Kundra & Team are trying to do inside of the government to make the our country more transparent. Heck, we're so excited about it we're doing our own [contest](http://sunlightlabs.com/contests/appsforamerica2) with cash prizes to celebrate.\r\n\r\nBut I do have a few gripes. So in the interest of full transparency, and the hopes that this will create change, here are my complaints for all to see:\r\n\r\n### 1. Half the data is from the USGS.\r\nNo offense to our hard working geologists, but seriously-- [copper smelters?](http://www.data.gov/details/16) Really? Why is the first dataset on Data.gov about Copper Smelters? And more importantly, every piece of data that's on the [front catalog page](http://www.data.gov/catalog#raw) of Data.gov has a 1 by it. Is that because they wanted them to appear at the top of the list? So these four (Smelters, Hydrolic Remote Sensing Center, Patent Grants, and Residential Energy Consumption from 2005) datasets were editorially chosen to lead the pack? \r\n\r\nI want better data, and there's a lot of it out there, and there's no excuses for it to be inside of Data.gov. It is data that's already being maintained by the feds. Ones I'd particularly like to see, in no particular order:\r\n\r\n1. How about the data *in* [Data.gov](http://data.gov). Put Data.gov's catalog online in a bulk format for all to see and play with. \r\n2. [FARA](http://www.usdoj.gov/criminal/fara/)\r\n3. [FEC](http://www.fec.gov/finance/disclosure/ftp_download.shtml)\r\n4. [FACA](http://fido.gov/facadatabase/)\r\n5. Personal Financial Disclosure Statements for Cabinet and Key Government Employees.\r\n6. [USASpending.gov Downloads](http://www.usaspending.gov/downloads.php)\r\n7. [The Federal Register](http://bookstore.gpo.gov/collections/eproducts.jsp) -- this one's special, and a little political. But the Government shouldn't be charging __$17,250__ for an electronic copy of the Federal Register.\r\n8. [Census](http://www.census.gov/) All of it. In something other than PDF files, too please.\r\n9. [Bureau of Labor Statistics](http://www.bls.gov/data/) All of it. \r\n10. Bulk data from [FedBizOpps](https://www.fbo.gov/)\r\n11. Of course, all the data on [Recovery.gov](http://www.recovery.gov/)\r\n\r\n\r\nI'm sure there's more than these 10 datasets. According to the feds, there's 200,000+ more coming, so get on with it, hurry up!\r\n\r\n### 2. It is a data catalog, not a data repository\r\n\r\nThis isn't just semantics-- the data on Data.gov links out to external sources that are not standardized. This means it is very hard to wrap programatically. For instance, if you go check out the [Patent Grant Bibliographic Data](http://www.data.gov/details/3#) for instance, you'll see that you can download the file as an XML file from [uspto.gov](http://uspto.gov). This means Data.gov is merely linking off to another site, rather than serving as a single source for the data.\r\n\r\nFine, cool, I can think of a million reasons to do that, especially that whole [Separation of Powers](http://en.wikipedia.org/wiki/Separation_of_powers_under_the_United_States_Constitution) bit. This would make it so maybe Data.gov could link off to congressional information without having to cross the line into the Executive Branch compelling congress to do something or having to wait on legislation (maybe), but the problem is, even the links are non-standardized and not restful. What we want is to be able to presume:\r\n\r\na. the Patent Data has an ID number of 3\r\nb. It has XML data\r\nc. Therefore, to get the XML data, we can go to data.gov/data/3/xml\r\n\r\nAnd have the software point us to the data we want. This kind of REST-ish interface for the website would be particularly useful. That way we could build software similar to [RubyGems](http://www.linuxjournal.com/article/8967) for Data.gov. How cool would that be? My dream? To be able to type in:\r\n\r\ndatagov install census.economic -y 2007 -v csv\r\n\r\nAnd see my terminal download that information directly onto my hard drive in a format that I, as well as my trusty computer can understand. Data.gov can lead us there. Where we need to head is for the data to all be in the same place, with standard formats, and reliability that it will always be there.\r\n\r\n### 3. It doesn't engage us directly\r\n\r\nI don't just want you to put links to the data up there, this is the biggest technical transparency and openness initiative the Government has undertaken in a long time. It is also going to be a hub for developers. So *talk* to us, engage us, have a blog, tell us what's going on and what to expect.\r\n\r\nSo much of dealing with data is narrative, and telling the story of Data.gov on an ongoing basis has so much value to it. We want to know what's going on on the inside, who is working on it, what the process is and who is building it. How are you talking Federal Agencies into putting their data online. What software challenges are you facing? When there's new data, how will we know? (Here at Sunlight we built [our own](http://sunlightlabs.com/blog/2009/05/22/keeping-eye-datagov/) RSS feed for it.)\r\n\r\nThose are my three biggest gripes. But all in all, it is a great contribution to society that I think will make amazing things happen for years to come. Heaps of praise, appreciation and gratitude for the sleepless nights that went into building this site. What would you change?", "date_published": "2009-05-28 14:11:47", "comment_count": 6, "slug": "what-id-change-about-datagov", "tags": "data.gov "}}, {"pk": 102, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-05-22 14:09:22", "author": 18, "timestamp": "2009-05-22 14:09:22", "markup": "none", "title": "Bills of the 110th Congressional Session", "excerpt": "Everyone has their own thoughts and perceptions on what generally happens to bills as they pass through the <a href=\"http://thomas.loc.gov/home/lawsmade.toc.html\">legislative process</a>.  With the thousands of bills that are introduced every year, it is hard to get an overall sense of what is happening. During the last couple of months, our intern Rebecca Shapiro collected and processed data (from the <a href=\"http://thomas.loc.gov/\">Thomas</a> Web site run by the Library of Congress) on each bill in the 110th session of Congress in order to understand what happened to all of them.", "content": "Everyone has their own thoughts and perceptions on what generally happens to bills as they pass through the <a href=\"http://thomas.loc.gov/home/lawsmade.toc.html\">legislative process</a>.  With the thousands of bills that are introduced every year, it is hard to get an overall sense of what is happening. During the last couple of months, I collected and processed data (from the <a href=\"http://thomas.loc.gov/\">Thomas</a> Web site run by the Library of Congress) on each bill in the 110th session of Congress in order to understand what happened to all of them.\r\n\r\n<h3>How the data was processed</h3>\r\nI calculated the stages a bill has entered using a set of heuristics developed at the Labs.  It is not perfect, but it does provide us with a sense of what happened to each bill.  A bill's death date and stage are calculated from the date of the last stage transition the bill makes before the end of the congressional session.  Since there is an overlap in types of actions that happen in both the House and the Senate, I normalized the stage names so <i>all</i> bills can be compared with each other.  Any stage that takes place at \"home\" such as \"Home Committee Referral\" takes place in the chamber of the bill's introduction.  Any stage that takes place at \"other\" such as \"Other Committee Referral\"  takes place in the chamber it was <i>not</i> initially introduced in.  Finally, please note that the \"Passage\" stage means the bill's passage was voted on, it does not necessarily mean that the bill passed.  You can read more about all of the stages and what each stage means <a href=\"http://assets.sunlightlabs.com/billvisualization/index.html#stages\">here</a>.\r\n\r\n\r\n<h3>Let's start with the basics</h3>\r\nDuring the 110th Congress, 11,059 bills were introduced.  7,335 were house bills and 3,724 were Senate bills.  Out of the 11,000+ bills, only 442 (4%) became law.  Most of these bills died as soon as they were referred to the committees in the chamber the bill was introduced in. \r\n\r\n<h3>Where else do bills go to die?</h3>\r\nWe know that most bills die when referred to the home committee(s).  But what other stages are lethal? <br/><br/>\r\n\r\n<a href=\"http://assets.sunlightlabs.com/billvisualization/graphs/bill_death_log.html\"><img src=\"http://assets.sunlightlabs.com/billvisualization/images/flowPic2.jpg\" alt=\"Flow Map Picture\"/></a><br/>\r\n\r\nThis flow map helps us visualize where bills go to die.  The thickness of the line going into any stage corresponds to the number of bills that died in that stage.</br>\r\n\r\n<h3>What stages do bills enter?</h3>\r\nNow that you have seen the final resting spots of bills, it may be nice to know how many bills enter each stage.<br/>\r\n\r\n<a href=\"http://assets.sunlightlabs.com/billvisualization/graphs/stagehistogram.html\"><img src=\"http://assets.sunlightlabs.com/billvisualization/images/barChart2.jpg\" alt=\"Bar Chart Picture\"/></a><br/>\r\n This graph depicts how many bills make it into each stage.<br/>\r\n\r\n<h3>How long do bills live before they die?</h3>\r\nNow that we have a sense of where bills go during the legislative process, let us also take a look at the timing. How much time does a bill spend alive before it finally dies? <br/>\r\n <a href=\"http://assets.sunlightlabs.com/billvisualization/graphs/lifetime_of_a_bill.html\"><img src=\"http://assets.sunlightlabs.com/billvisualization/images/linePic2.jpg\" alt=\"Line Chart Picture\"/></a><br/>\r\nHow long do the bills that don't make it as laws generally live? Suppose all bills in the 110th Congress started on the same day; this chart displays the number of bills that are still alive each day.<br/>\r\n\r\n<h3>Digging deeper...</h3>\r\nStill not satisfied?  Let us take a look at a visualization that combines both timing and the stages bills enter to get the whole picture.<br/>\r\n\r\n<a href=\"http://assets.sunlightlabs.com/billvisualization/billVisualization.swf\"><img src=\"http://assets.sunlightlabs.com/billvisualization/images/visualizationPic.jpg\" alt=\"Visualization Picture\"/></a>\r\n\r\n<p>This visualization will help you have a better understanding as to what happens to each bill over the course of the session. Each bill is represented by a colored box whose size increases as it progresses through the legislative process.  A green box means the bill is still alive, blue means the bill is a law, and red means the bill died. Warning: this Flash applet may lock up your browser while loading; this is normal.</p>\r\n\r\nTo learn more about this project or play with the various features of the visualization please visit <a href=\"http://assets.sunlightlabs.com/billvisualization/index.html\">The Life and Death of Congressional Bills in the 110th Session</a>.", "date_published": "2009-05-26 10:28:43", "comment_count": 4, "slug": "bills-110th-congressional-session", "tags": "visualization legislation"}}, {"pk": 103, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-05-22 16:10:58", "author": 6, "timestamp": "2009-05-22 16:10:58", "markup": "markdown", "title": "Keeping an Eye on Data.gov", "excerpt": "One thing that's curiously missing from Data.gov is an RSS feed for new data feeds. Sort of shockingly, and glaringly left out. We were disappointed, and didn't want to wait. Scraping here is such an easy thing to do that we decided to just build our own. Sunlight Labs' James Turk did it, and it's handy. [Here's the feed](http://data.sunlightlabs.com/data.gov/datafeed.xml) and [here's the source](http://github.com/sunlightlabs/sunlight-scraps/blob/master/scraping/data.gov/datagovrss.py) that makes the feed. This should be useful to anyone who wants to see what new stuff is coming out of Data.gov.", "content": "One thing that's curiously missing from Data.gov is an RSS feed for new data feeds. Sort of shockingly, and glaringly left out. We were disappointed, and didn't want to wait. Scraping here is such an easy thing to do that we decided to just build our own. Sunlight Labs' James Turk did it, and it's handy. [Here's the feed](http://data.sunlightlabs.com/data.gov/datafeed.xml) and [here's the source](http://github.com/sunlightlabs/sunlight-scraps/blob/master/scraping/data.gov/datagovrss.py) that makes the feed. This should be useful to anyone who wants to see what new stuff is coming out of Data.gov.", "date_published": "2009-05-22 16:10:20", "comment_count": 1, "slug": "keeping-eye-datagov", "tags": "data.gov rss appsforamerica"}}, {"pk": 101, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-05-22 10:28:20", "author": 6, "timestamp": "2009-05-22 10:28:20", "markup": "markdown", "title": "Everything We Know About Data.gov", "excerpt": "Everything you wanted to know about Data.gov\r\n\r\nNow that Data.gov's out, I thought I'd take a look under the hood and see what's in there, what's missing, and try and figure out what's coming.\r\n\r\nFirst off searching through twitter for the phrase \"[Data.gov congratulations](http://search.twitter.com/search?q=data.gov+congratulations)\" I'm able to get enough evidence that [hmiller23](http://twitter.com/hmiller23) and [Jerad Speigel](http://twitter.com/jspeigel1) of the [Phase One Consulting Group](http://www.phaseonecg.com/) built the site. I asked them on Twitter, and they said [\"It Uses LAMP\"](http://twitter.com/hmiller23/status/1882722752)\r\n\r\nRight now the site is short on data. **Federal CIOs: There are hundreds of us waiting to do interesting things with your data. Invest in putting it up on Data.gov now. You will be rewarded.**\r\n\r\nRight now the breakdown of the files looks like this:", "content": "Now that Data.gov's out, I thought I'd take a look under the hood and see what's in there, what's missing, and try and figure out what's coming.\r\n\r\nFirst off searching through twitter for the phrase \"[Data.gov congratulations](http://search.twitter.com/search?q=data.gov+congratulations)\" I'm able to get enough evidence that [hmiller23](http://twitter.com/hmiller23) and [Jerad Speigel](http://twitter.com/jspeigel1) of the [Phase One Consulting Group](http://www.phaseonecg.com/) built the site. I asked them on Twitter, and they said [\"It Uses LAMP\"](http://twitter.com/hmiller23/status/1882722752)\r\n\r\nRight now the site is short on data. **Federal CIOs: There are hundreds of us waiting to do interesting things with your data. Invest in putting it up on Data.gov now. You will be rewarded.**\r\n\r\nRight now the breakdown of the files looks like this: \r\n\r\n<img src=\"http://img.skitch.com/20090522-e5rqwwxaw44gpbs1ahu62iaf3t.jpg\" alt=\"Data.gov Format Breakdown\"/>\r\n\r\nIn terms of number of datasets per agency, here's what we're looking at:\r\n\r\n<img src=\"http://img.skitch.com/20090522-gjfrw5atqdrugc47wcciej1w9c.jpg\" alt=\"Untitled\"/>\r\n\r\nSo the US Geological Survey represents roughly half the data (which also may be why the available datasets are in KML or ESRI). \r\n\r\n\r\nThat's the thing that really must change now-- and that's going to be what will determine the success of Data.gov. There's a lot of datasets that the federal government has that have not been included, big datasets like the [FACA Database](http://fido.gov/facadatabase/), the [FARA Database](http://www.usdoj.gov/criminal/fara/links/search.html), and what about OMB's own Federal Budget?\r\n\r\nBut that's not stopping us. Already-- in less than 24 hours, we have one entry to the contest. Go ahead and play [FBI Fugitive Concentration](http://fbi.thatsaspicymeatball.com/)!\r\n\r\n", "date_published": "2009-05-22 10:28:19", "comment_count": 9, "slug": "everything-we-know-about-datagov", "tags": ""}}, {"pk": 100, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-05-20 15:44:51", "author": 6, "timestamp": "2009-05-20 15:44:51", "markup": "markdown", "title": "Apps for America 2: The Data.gov Challenge", "excerpt": "Announcing Apps for America 2: The Data.gov Challenge\r\n\r\nWe've been planning this for awhile. Ever since we heard about [Data.gov](http://data.gov) we have been planning a contest, and if you're reading this blog post, that means Data.gov has finally launched.\r\n\r\nI'm pleased to wave the green flag on [Apps for America 2: The Data.gov Challenge.](/contests/appsforamerica2/) This is a development and visualization challenge to see who can come up with the best application and visualization for data from [Data.gov](http://data.gov).\r\n\r\nThese are exciting times for us-- the walls between Government and Developers are starting to shrink, and we here in Sunlight Labs are terribly excited to get to work on doing great things with the data that's coming out. Government has made a move in the right direction-- now it is time for us to show them what we can do.\r\n\r\nWe're happy to launch Apps for America 2, this time with support from our friends. Google's put in some prize money, as has Craig Newmark, the founder of [Craig's list](http://craigslist.com). O'Reilly and Techweb have provided another wonderful incentive: tickets and table space to Gov2.0 summit for the winners.\r\n\r\nFinally, we think that in building this community of technical talent, that we need not only developers but also talented and creative artists and visualizers. So we've created a special bonus \"visualization and design\" prize that will hopefully bring new ideas and talent to the table.\r\n\r\nTake a look at our [Apps for America 2](/contests/appsforamerica2/) page for all the rules and details. This is going to be the best Apps for America yet.\r\n\r\nPlease, spread the word far and wide! This is the technical community's first and best chance to show our federal government the kind of talent and creativity that we all have, and more importantly to show it what happens when it engages the technical community.\r\n\r\nLet the Games Begin.", "content": "Announcing Apps for America 2: The Data.gov Challenge\r\n\r\nWe've been planning this for awhile. Ever since we heard about [Data.gov](http://data.gov) we have been planning a contest, and if you're reading this blog post, that means Data.gov has finally launched.\r\n\r\nI'm pleased to wave the green flag on [Apps for America 2: The Data.gov Challenge.](/contests/appsforamerica2/) This is a development and visualization challenge to see who can come up with the best application and visualization for data from [Data.gov](http://data.gov).\r\n\r\nThese are exciting times for us-- the walls between Government and Developers are starting to shrink, and we here in Sunlight Labs are terribly excited to get to work on doing great things with the data that's coming out. Government has made a move in the right direction-- now it is time for us to show them what we can do.\r\n\r\nWe're happy to launch Apps for America 2, this time with support from our friends. Google's put in some prize money, as has Craig Newmark, the founder of [Craig's list](http://craigslist.com). O'Reilly and Techweb have provided another wonderful incentive: tickets and table space to Gov2.0 summit for the winners.\r\n\r\nFinally, we think that in building this community of technical talent, that we need not only developers but also talented and creative artists and visualizers. So we've created a special bonus \"visualization and design\" prize that will hopefully bring new ideas and talent to the table.\r\n\r\nTake a look at our [Apps for America 2](/contests/appsforamerica2/) page for all the rules and details. This is going to be the best Apps for America yet.\r\n\r\nPlease, spread the word far and wide! This is the technical community's first and best chance to show our federal government the kind of talent and creativity that we all have, and more importantly to show it what happens when it engages the technical community.\r\n\r\nLet the Games Begin.\r\n\r\n<script type=\"text/javascript\">\r\ndigg_url = 'http://digg.com/programming/Data_gov_Launches_25_000_contest_for_best_app';\r\n</script>\r\n<script src=\"http://digg.com/tools/diggthis.js\" type=\"text/javascript\"></script> ", "date_published": "2009-05-21 11:43:13", "comment_count": 14, "slug": "apps-america-2-datagov-challenge", "tags": "appsforamerica contest data.gov "}}, {"pk": 99, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-05-18 15:42:55", "author": 1, "timestamp": "2009-05-18 15:42:55", "markup": "markdown", "title": "X-UA-Compatible Django Middleware", "excerpt": "Microsoft's Internet Explorer 8 may choose to display your site using an older, less compliant rendering engine. Take control and tell IE which engine to use with our [Django middleware and decorator](http://gist.github.com/113635).", "content": "When Microsoft's [Internet Explorer 8](http://www.microsoft.com/windows/Internet-explorer/default.aspx) was released this past March, it included a fairly decent standards-compliant rendering engine. While a step forward for the web, a compliant rendering engine breaks many web sites that were developed for previous versions of IE. Internal corporate and government networks are notorious for sites that were built for IE 6 or 7 and often took advantage of bugs in the rendering engines. To prevent these sites, and others across the web, from breaking, IE 8 will sometimes use one of the older rendering engines.\r\n\r\nThere are [various conditions](http://farukat.es/journal/2009/05/245-ie8-and-the-x-ua-compatible-situation) that are used to select the engine that will be used to render the page. The good news is that you, as a web developer, can instruct IE 8 to use a specific engine to render your site. This can be done by using the [X-UA-Compatible HTTP header or HTML meta tag](http://www.alistapart.com/articles/beyonddoctype).\r\n\r\nSunlight Labs has written [middleware and a decorator for Django](http://gist.github.com/113635) that will take care of sending the appropriate HTTP header. Include the following in settings.py to enable the middleware:   \r\n\r\n    MIDDLEWARE_CLASSES = (\r\n        ...\r\n        'compatibility.XUACompatibleMiddleware',\r\n    )   \r\n\r\nThe middleware will send a default X-UA-Compatible header of _IE=edge_ for any HTML or XHTML response. The default value can be overridden by including the following in settings.py:   \r\n\r\n    X_UA_COMPATIBLE = 'IE=EmulateIE7'\r\n\r\nIn some cases it may be necessary to set the value of the X-UA-Compatible header on a per-view basis. This can be accomplished using the provided decorator:   \r\n\r\n    from compatibility import xuacompatible\r\n    \r\n    @xuacompatible('IE=EmulateIE7')\r\n    def index(request):\r\n        ...   \r\n\r\n[X-UA-Compatible middleware and decorator on GitHub](http://gist.github.com/113635)", "date_published": "2009-05-18 16:04:10", "comment_count": 5, "slug": "x-ua-compatible-django-middleware", "tags": "django, x-ua-compatible, ie8"}}, {"pk": 98, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-05-15 00:37:11", "author": 1, "timestamp": "2009-05-15 00:37:11", "markup": "markdown", "title": "Downloading House Lobbying Disclosures", "excerpt": "The Lobbying Disclosure Act of 1995 mandates that lobbyist that meet specific requirements are to register with Clerk of the House of Representatives and the Secretary of the Senate. Being the great body that they are, the House provides a [searchable database](http://disclosures.house.gov/ld/ldsearch.aspx) and [bulk download](http://disclosures.house.gov/ld/LDDownload.aspx) of the registration forms. Sure a searchable database is nice, but we can have the most fun with access to the entire data set. The disclosure forms are provided in XML format, divided by year and reporting period (quarerly, semi-annually, annually), and archived.\r\n\r\nIn order to download the disclosure archives, an HTML form must be submitted for each file. This can be a huge pain as the files are large and involves non-trivial human effort whenever files are released or updated. We've written a Python script that simulates the form submissions and automatically downloads all of the archives. In addition to the script, we've uploaded a recent download of the archives to Amazon S3 for easier distribution.\r\n\r\nThe simple download script can be [found on GitHub](http://gist.github.com/111800) and the archives can be [downloaded from S3](http://shrub.appspot.com/data.sunlightlabs.com/ld/).", "content": "The Lobbying Disclosure Act of 1995 mandates that lobbyist that meet specific requirements are to register with Clerk of the House of Representatives and the Secretary of the Senate. Being the great body that they are, the House provides a [searchable database](http://disclosures.house.gov/ld/ldsearch.aspx) and [bulk download](http://disclosures.house.gov/ld/LDDownload.aspx) of the registration forms. Sure a searchable database is nice, but we can have the most fun with access to the entire data set. The disclosure forms are provided in XML format, divided by year and reporting period (quarerly, semi-annually, annually), and archived.\r\n\r\nIn order to download the disclosure archives, an HTML form must be submitted for each file. This can be a huge pain as the files are large and involves non-trivial human effort whenever files are released or updated. We've written a Python script that simulates the form submissions and automatically downloads all of the archives. In addition to the script, we've uploaded a recent download of the archives to Amazon S3 for easier distribution.\r\n\r\nThe simple download script can be [found on GitHub](http://gist.github.com/111800) and the archives can be [downloaded from S3](http://shrub.appspot.com/data.sunlightlabs.com/ld/).", "date_published": "2009-05-15 00:48:31", "comment_count": 0, "slug": "downloading-house-lobbying-disclosures", "tags": "python, lobbying"}}, {"pk": 97, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-05-13 13:49:45", "author": 6, "timestamp": "2009-05-13 13:49:45", "markup": "markdown", "title": "2 Easy Wins for your Congressional Website", "excerpt": "2 Easy wins for your Congressional Website\r\n\r\nLots of members of congress are busily updating their website. It seems these days that we're getting lots of brilliantly designed congressional websites as the innovation in design from the campaigns trickles ever so slowly into government.\r\n\r\nIf you're helping to build your Member of Congress' website, I'd like to ask you to do two things:\r\n", "content": "Lots of Members of Congress are busily updating their website. It seems these days that we're getting lots of brilliantly designed congressional websites as the innovation in design from the campaigns trickles ever so slowly into government.\r\n\r\nIf you're helping to build your Member of Congress' website, I'd like to ask you to do two things:\r\n\r\nFirst __Publish your \"Congressional Project Requests,\" both appropriation requests and non-appropriation requests in a standard, parseable format.__ We even [wrote one for you](http://wiki.sunlightlabs.com/index.php/Earmark-proposal), and if you use it, I promise that we will laud you with praise! When we say parseable, what we mean is, able for computers to figure out what's what on the page by adding consistent, named HTML tags to each project request. \r\n\r\nAlso, publish them all on one page in this format. There's no need to sort them out by legislation. And please, stop posting them as [scanned, non searchable PDFs](http://www.house.gov/alexander/content/approps/approps.pdf) (I'm looking at you, Rodney Alexander.) Just Stop. \r\n\r\nTaking it a step further: if you're office is publishing your earmark requests in PDF instead of a more readable format like HTML, then please consider changing as this is not satisfactory.\r\n\r\nSecond: __If you publish your Calendar online, also publish it in a standard format like iCal.__ Senator Begich, for instance, should be commended for putting it all out there-- [every meeting](http://begich.senate.gov/public/index.cfm?p=Schedule&ContentRecord_id=4998833c-f4b1-4c38-8de2-1f440df4e934&ContentType_id=814100e6-c94d-4027-bb73-bf1486a8ed8a&MonthDisplay=4&YearDisplay=2009) is on his website! But it isn't in a format that anyone can use. While I applaud the Senator for doing a great job on fulfilling his campaign promises, I wish he'd publish this data for his constituents in iCal so that they could make better use of the information.\r\n\r\nI'm sure there's a dozen other things that I could recommend-- things like publishing your personal financial disclosure forms online, and providing the bills that you sponsor in machine readable formats, but really-- just these two things are great!\r\n\r\nSo if you're a hill staffer or even a Member of Congress, please consider these two things. And if you need help, feel free to get in touch. If you're unclear on some of this technical stuff, we're happy to answer any questions you have. We are more than happy to provide you with instructions on specifically how to do this stuff.\r\n\r\n", "date_published": "2009-05-13 13:49:28", "comment_count": 3, "slug": "2-easy-wins-your-congressional-website", "tags": "congress website earmarks calendar recomendations "}}, {"pk": 96, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-05-12 13:11:40", "author": 6, "timestamp": "2009-05-12 13:11:40", "markup": "none", "title": "My Talk at Web2.0 Expo", "excerpt": "", "content": "<embed src=\"http://blip.tv/play/gshV_JdVhrwN\" type=\"application/x-shockwave-flash\" width=\"400\" height=\"300\" allowscriptaccess=\"always\" allowfullscreen=\"true\"></embed> ", "date_published": "2009-05-12 13:11:38", "comment_count": 0, "slug": "my-talk-web20-expo", "tags": "video talks web2.0expo "}}, {"pk": 95, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-05-12 10:44:58", "author": 14, "timestamp": "2009-05-12 10:44:58", "markup": "none", "title": "Post RailsConf", "excerpt": "Open government got a great spotlight at <a href=\"http://en.oreilly.com/rails2009/\">RailsConf</a> this year, in Las Vegas.  <a href=\"http://www.linkstainedwretch.com\">Dan Lathrop</a> organized a panel on Government 2.0 there, and was kind enough to ask me to speak about Sunlight, and <a href=\"http://twitter.com/pengwynn\">Wynn Netherland</a> about his great work on <a href=\"http://tweetcongress.com\">TweetCongress</a>.  We had good turnout, and got plenty of questions.  We even had a guy spontaneously write an IP address of an anonymous FTP server on a piece of paper, approach the podium to hand it to me, and promise us that there was raw legislative data on the other end.", "content": "<p>\r\nOpen government got a great spotlight at <a href=\"http://en.oreilly.com/rails2009/\">RailsConf</a> this year, in Las Vegas.  <a href=\"http://www.linkstainedwretch.com\">Dan Lathrop</a> organized a panel on Government 2.0 there, and was kind enough to ask me to speak about Sunlight, and <a href=\"http://twitter.com/pengwynn\">Wynn Netherland</a> about his great work on <a href=\"http://tweetcongress.com\">TweetCongress</a>.  We had good turnout, and got plenty of questions.  We even had a guy spontaneously write an IP address of an anonymous FTP server on a piece of paper, approach the podium to hand it to me, and promise us that there was raw legislative data on the other end.\r\n</p>\r\n\r\n<p>\r\n<a href=\"http://vimeo.com/4589932\">Video of the panel</a> has arrived online, thanks to <a href=\"http://twitter.com/LuigiMontanez\">Luigi Montanez</a>, who was kind enough to record it on his Flip HD.  Luigi also captured Wynn's talk later during RailsConf about creating purpose-driven apps, entitled <a href=\"http://vimeo.com/4584581\">Build an App, Start a Movement</a>.\r\n</p>\r\n\r\n<p>\r\nLater that evening in the same room, we held a \"hackathon\", but we got so many people (somewhere around 25) that it turned into more of a group brainstorming and discussion session, and at that we were very productive.  Nearly everyone there, including many who had only just been exposed to the open government \"scene\" at the panel, had ideas to talk about.  We couldn't possibly have harnessed all that energy in the time we had, but some people stayed an extra hour and moved into an adjacent room so that we could keep talking.  I eventually degenerated into a broken record that repeatedly requested people join the <a href=\"http://groups.google.com/group/sunlightlabs/\">Labs mailing list</a> and continue gathering momentum for their ideas there, and I hope that some of it bears fruit.\r\n</p>\r\n\r\n<p>\r\nDuring the session, several people asked about how to best continue activity in their home city.  I'll post more about this soon, but a short answer is to simply look at the <a href=\"http://www.meetup.com/opengovnyc/\">Open Government NYC</a> Meetup group (and their <a href=\"http://groups.google.com/group/open-government-nyc\">active mailing list</a>) to see a recent and exciting example.\r\n</p>\r\n\r\n<p>\r\nAll in all, the whole affair <a href=\"http://twitter.com/bguthrie/status/1710486235\">got</a> <a href=\"http://twitter.com/troyd/statuses/1713858375\">some</a> <a href=\"http://twitter.com/marcslove/statuses/1714279979\">nice</a>, <a href=\"http://twitter.com/rkofman/statuses/1714154613\">positive</a> <a href=\"http://twitter.com/bifurcations/statuses/1711085230\">Twitter</a> <a href=\"http://twitter.com/ssoper/statuses/1710750277\">press</a> (Twress?). Thanks to everyone who attended!\r\n</p>", "date_published": "2009-05-12 10:44:43", "comment_count": 0, "slug": "post-railsconf", "tags": "rails ruby railsconf hackathon"}}, {"pk": 94, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-05-11 10:24:25", "author": 6, "timestamp": "2009-05-11 10:24:25", "markup": "none", "title": "What do people want from Recovery.gov?", "excerpt": "We scraped all the ideas off of the latest <a href=\"http://thenationaldialogue.org\">National Dialogue on Recovery.gov</a> to see of we could condense what most of the ideas were about. Here's what we got:<br/>\r\n<img src=\"http://dl-client.getdropbox.com/u/36193/Wordle%20-%20Create.jpg\"><br/>\r\nLooks like most of the ideas were about data. People sure do want to set that data free, don't they?", "content": "We scraped all the ideas off of the latest <a href=\"http://thenationaldialogue.org\">National Dialogue on Recovery.gov</a> to see of we could condense what most of the ideas were about. Here's what we got:<br/>\r\n<img src=\"http://dl-client.getdropbox.com/u/36193/Wordle%20-%20Create.jpg\"><br/>\r\nLooks like most of the ideas were about data. People sure do want to set that data free, don't they?", "date_published": "2009-05-11 10:24:16", "comment_count": 4, "slug": "what-do-people-want-recoverygov", "tags": "recovery.gov nationaldialogue simplify data"}}, {"pk": 93, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-05-08 10:13:17", "author": 6, "timestamp": "2009-05-08 10:13:17", "markup": "markdown", "title": "On the new NYSenate.gov", "excerpt": "The transparency community is abuzz today with the revelation of the New York Senate website. It is shiny and pretty. There are great new features and even our website, [Public Markup](http://publicmarkup.org) gets a [shout out](http://www.nysenate.gov/legislation). Neat!\r\n\r\nWhat's great about it is uniformity. Every Senator has a website that's the same as every other Senator's, with links to their RSS feeds and even twitter accounts. They've got blogs and interestingly enough calendars. Now, the technology for transparency is there. Sadly, it doesn't look like the Senators are using it yet -- I find it hard to believe, for example, that the Senate President has a [clear schedule for the rest of the month.](http://www.nysenate.gov/senator/malcolm-smith/events). But the technology is there and the NY Senate technology team ought to be commended for building that in (and making it export in iCal!).\r\n", "content": "The open government community was abuzz yesterday with the revelation of the New York Senate website. It is shiny and pretty. There are great new features and even our website, [Public Markup](http://publicmarkup.org) gets a [shout out](http://www.nysenate.gov/legislation). Neat!\r\n\r\nWhat's great about it is uniformity. Every Senator has a website that's the same as every other Senator's, with links to their RSS feeds and even twitter accounts. They've got blogs and interestingly enough calendars. Now, the technology for transparency is there. Sadly, it doesn't look like the Senators are using it yet -- I find it hard to believe, for example, that the Senate President has a [clear schedule for the rest of the month.](http://www.nysenate.gov/senator/malcolm-smith/events). But the technology is there and the NY Senate technology team ought to be commended for building that in (and making it export in iCal!).\r\n\r\nWith all this neat stuff though, I must say that on the transparency side of things, the site still leaves a little bit to be desired. For instance, the website features an [\"Open Data\"](http://www.nysenate.gov/open-data) section but the documents contained inside aren't very open. When I think of \"Open Data\" I think of the [open data principles](http://resource.org/8_principles.html), and these documents are a collection of PDF files. They're not machine readable, particularly timely, and are provided out of context without real explanation of what they are.\r\n\r\nIt also seems that the site completely disconnects biographical information from legislative information. Why not have [OpenCongress](http://opencongress.org) style [profile pages](http://www.opencongress.org/person/show/400166_jane_harman) explaining how members have voted, giving them videos of their floor speeches, and showing them what bills they've sponsored and co-sponsored. This is probably because updating their [legislative information system](http://public.leginfo.state.ny.us/menuf.cgi) is perhaps more costly in terms of both time and money, than the team could handle.\r\n\r\nFinally, it'd be nice to see the site feature ethics and accountability information for each member. To push the ball forward on the transparency side, seeing each member's campaign contributions and personal financial disclosure statements would be truly revolutionary.\r\n\r\nMuch of this doesn't have anything to do with the team or technology that they've put in place. My complaints with the site may require more laws to change before they're even possible. But it's great to see the ball move forward in the New York Senate. The groundwork there for change through technology has been laid. Now let's see if they can be as transparent as they are participatory with the new site.", "date_published": "2009-05-08 10:15:33", "comment_count": 5, "slug": "new-nysenategov", "tags": "reviews newyork state"}}, {"pk": 92, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-05-04 11:27:07", "author": 6, "timestamp": "2009-05-04 11:27:07", "markup": "none", "title": "Shameless Plug: Beautiful Data", "excerpt": "<p>O'Reilly is publishing a new book entitled <a href=\"http://bit.ly/9UR4C\">Beautiful Data</a> that you should buy. Why?</p>\r\n<ol>\r\n<li>The two authors of the book, Jeff Hammerbacher and Toby Segaran are really great people. It looks like a great book.</li>\r\n<li><b>Half the proceeds of the book go to us here at the Sunlight Foundation. The other half goes to another great organization, <a href=\"http://creativecommons.org\">Creative Commons</a></b></li>\r\n\r\n<p>So buy the book and give us some much needed support so that we can continue liberating data and shining light in some of the darkest corners of Washington, DC. I'll sweeten the pot, too: If you're one of the first five people to buy the book and send me a picture of you with the book in your hands, I'll send you some very rare Sunlight Foundation/Labs stickers along with a very appreciative thank-you note.</p>\r\n\r\n<p><a href=\"http://bit.ly/9UR4C\"><img src=\"http://dl-client.getdropbox.com/u/36193/beautifuldata.gif\"/></a></p>", "content": "<p>O'Reilly is publishing a new book entitled <a href=\"http://bit.ly/9UR4C\">Beautiful Data</a> that you should buy. Why?</p>\r\n<ol>\r\n<li>The two authors of the book, Jeff Hammerbacher and Toby Segaran are really great people. It looks like a great book.</li>\r\n<li><b>Half the proceeds of the book go to us here at the Sunlight Foundation. The other half goes to another great organization, <a href=\"http://creativecommons.org\">Creative Commons</a></b></li>\r\n\r\n<p>So buy the book and give us some much needed support so that we can continue liberating data and shining light in some of the darkest corners of Washington, DC. I'll sweeten the pot, too: If you're one of the first five people to buy the book and send me a picture of you with the book in your hands, I'll send you some very rare Sunlight Foundation/Labs stickers along with a very appreciative thank-you note.</p>\r\n\r\n<p><a href=\"http://bit.ly/9UR4C\"><img src=\"http://dl-client.getdropbox.com/u/36193/beautifuldata.gif\"/></a></p>\r\n\r\n<p>If you're looking for quick and easy ways to help the Sunlight Foundation, this is an easy way. Buy the book and tell your friends about it too.</p>", "date_published": "2009-05-04 11:29:52", "comment_count": 1, "slug": "beautiful-data", "tags": "plug book oreilly fundraising shameless"}}, {"pk": 91, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-04-24 13:55:48", "author": 14, "timestamp": "2009-04-24 13:55:48", "markup": "none", "title": "Open Government at RailsConf", "excerpt": "This year's <a href=\"http://en.oreilly.com/rails2009/\">RailsConf</a> is going to have a real Open Government presence.  If you're attending RailsConf in Vegas this year, there are a couple of events on Tuesday that should interest you.", "content": "<p>\r\nThis year's <a href=\"http://en.oreilly.com/rails2009/\">RailsConf</a> is going to have a real Open Government presence.  If you're attending RailsConf in Vegas this year, there are a couple of events on Tuesday that should interest you.\r\n</p>\r\n\r\n<p>\r\nFirst of all, <a href=\"http://twitter.com/lathropd\">Dan Lathrop</a>, who is writing a book on Government 2.0 for O'Reilly, has put together a panel titled <a href=\"http://en.oreilly.com/rails2009/public/schedule/detail/9192\">Gov 2.0: Transparency, Collaboration, and Participation in Practice</a>.  The panelists will consist of Dan, <a href=\"http://twitter.com/pengwynn\">Wynn Netherland</a> (one of the creators of <a href=\"http://tweetcongress.org\">TweetCongress</a>), and <a href=\"http://twitter.com/Klondike\">Eric Mill</a> (me).  I'll be talking about the work Sunlight does, Apps for America, and what we hope to see from open government hackers in the future.  It's going down at 2:50pm on Tuesday, in Pavilion 1.\r\n</p>\r\n\r\n<p>\r\nThen, later that night at 8:30pm, in the same space, I'll be running an <a href=\"http://en.oreilly.com/rails2009/public/schedule/detail/9020\">Open Government mini-hackathon</a> Birds of a Feather session.  If you're a hacker and you want to find some projects to work on, or some inspiration for your own, please come by.\r\n</p>\r\n\r\n<p>\r\nBig thanks go to Dan Lathrop for making this happen.  See you in Vegas!\r\n</p>", "date_published": "2009-04-24 13:55:34", "comment_count": 1, "slug": "open-government-railsconf", "tags": "rails ruby railsconf hackathon"}}, {"pk": 90, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-04-24 13:36:34", "author": 6, "timestamp": "2009-04-24 13:36:34", "markup": "markdown", "title": "Are(\"R\") You a visualizer?", "excerpt": "Yesterday, I posted a bit about how [data.gov](http://sunlightlabs.com/blog/2009/04/23/should-datagov-visualize-probably-not/) shouldn't focus on data visualizations, but rather providing clean reliable data to citizens. But what this means is that we as a non-government community really need to start thinking about how to do visualizations when that data becomes available. Right now we're asking a lot of questions about data visualizations inside the Sunlight Offices that need to be shared with the wider Labs community:\r\n\r\n* Should we hire somebody that does good data visualizations full time?\r\n* Should we have a contest for best data visualization?\r\n* What kinds of data visualizations would be successful in our field?\r\n\r\nIn order to kick off the discussion with the larger group, I want to gauge response from the wider design and visualization community and see what kind of people are out there. So I'm asking that if you're interested in working on the visualization and design side of things rather than the application and coding side of things that you fill out the form below. \r\n", "content": "Yesterday, I posted a bit about how [data.gov](http://sunlightlabs.com/blog/2009/04/23/should-datagov-visualize-probably-not/) shouldn't focus on data visualizations, but rather providing clean reliable data to citizens. But what this means is that we as a non-government community really need to start thinking about how to do visualizations when that data becomes available. Right now we're asking a lot of questions about data visualizations inside the Sunlight Offices that need to be shared with the wider Labs community:\r\n\r\n* Should we hire somebody that does good data visualizations full time?\r\n* Should we have a contest for best data visualization?\r\n* What kinds of data visualizations would be successful in our field?\r\n\r\nIn order to kick off the discussion with the larger group, I want to gauge response from the wider design and visualization community and see what kind of people are out there. So I'm asking that if you're interested in working on the visualization and design side of things rather than the application and coding side of things that you fill out the form below. \r\n\r\nFull disclosure: I'm not sure what I'm going to do with the information you provide, but I promise I won't give it out, and I won't subscribe you to any lists you're not already on here at Sunlight. I just want to get a sense of who is out there and what they have to offer.\r\n\r\nHere's the magic Google form:\r\n\r\n<iframe src=\"https://spreadsheets.google.com/embeddedform?key=rulIRabCvVkukLjd_rzSwtg\" width=\"500\" height=\"610\" frameborder=\"0\" marginheight=\"0\" marginwidth=\"0\">Loading...</iframe>", "date_published": "2009-04-24 13:36:28", "comment_count": 1, "slug": "r-you-visualizer", "tags": "visualization "}}, {"pk": 89, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-04-23 13:35:29", "author": 6, "timestamp": "2009-04-23 13:35:29", "markup": "markdown", "title": "Should Data.gov visualize? Probably not.", "excerpt": "A few people who saw our [Data.gov design post](http://sunlightlabs.com/blog/2009/04/16/redesigning-government-datagov/#)  asked for ways to visualize the data on Data.gov. As an organization that's such a proponent of data not only being free, but also using design to provide context to the data, why don't we advocate for data.gov to have visualizations for citizens to make sense of the data?", "content": "A few people who saw our [Data.gov design post](http://sunlightlabs.com/blog/2009/04/16/redesigning-government-datagov/#)  asked for ways to visualize the data on Data.gov. As an organization that's such a proponent of data not only being free, but also using design to provide context to the data, why don't we advocate for data.gov to have visualizations for citizens to make sense of the data?\r\n\r\nWe didn't just leave it out because we didn't think of it. We left it out on purpose, along with lots of other feature ideas and concepts. We think that providing a centralized repository of government data in modern developer-friendly formats is a hard enough problem for government to solve.  Vivek Kundra and his team should be focused on making data available and making it as accessible as possible to people via a small, uniform set of developer friendly formats (say: JSON, XML, KML and CSV). \r\n\r\nWe think that building a system for delivering that data in those formats across the federal government and building a mechanism for people to report back errors, integrity issues or simply comment on the data is far more important than building visualization tools on top of this data. Why?\r\n\r\nThis chart should explain it:\r\n\r\n<a href='http://siteanalytics.compete.com/whitehouse.gov+nytimes.com/?metric=uv'><img src='http://grapher.compete.com/whitehouse.gov+nytimes.com_uv_310.png' /></a>\r\n\r\nIn short: because other people will do that and probably do it better. If the goal is to get the data in front of the most eyeballs possible,  government should be providing the data in usable formats and focusing primarily on that. External entities will always give the data more exposure and treatment than government can.\r\n\r\nThe second reason why government should avoid spending time on adding visualizations or other bells and whistles to Data.gov is because it actually hurts transparency. Visualizations, like any other form of news product, can be editorial-- even inadvertently. If government puts more of a priority on producing great visualizations and user experience than on providing quality accurate data with a great feedback loop, then it runs a pretty good chance of not adhering to the goal of being actually transparent.\r\n\r\nYou can see this on Recovery.gov right now. You get a sense that there's a lot of data underneath, but they've spent a lot of time on user-interface development. Check out, for instance, the [agency summary page](http://www.recovery.gov/?q=content/agency-summary&agency_code=28&startdate=2009-04-17&noofreports=1&status=1&report_id=226) for the SSA. Looks great! Neat Charts!\r\n\r\nThe raw data tells a different story though. In this case, the data is powered by spreadsheets available at the bottom of the page. Open one up and you're likely going to see something like this:\r\n\r\n<img src=\"http://img.skitch.com/20090423-1ehrkrwe4a59s68snns8tw13ga.jpg\" alt=\"financial_and_activity_report_20090410SSA-1\"/>\r\n\r\nSo you can see why we're significantly less interested in government \"totally nailing\" putting bar charts on a website and far more interested in saying \"hey, eye on the ball! make the data come out clean, and reliable, and give us a way to tell you when it isn't.\" ", "date_published": "2009-04-23 13:40:38", "comment_count": 3, "slug": "should-datagov-visualize-probably-not", "tags": "data.gov visualizations"}}, {"pk": 88, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-04-22 13:04:54", "author": 3, "timestamp": "2009-04-22 13:04:54", "markup": "none", "title": "Looking to Help \"The Man\", Turning in My Chief Evangelist Badge", "excerpt": "<i>\u201cWhat we\u2019ve learned from the Open House Project,\u201d said Sunlight\u2019s Greg Elin, \u201cis that whenever we sit outside and build things, we learn that there are people inside who want to do the same thing.\u201d</i>\r\n\r\nThat's me, quoted in Matthew Burton's 2008 essay, <a href=\"http://www.impublished.org/wordpress/helptheman/\"><i>Why I Help \"The Man\", and Why You Should, Too</i></a>. I've been thinking a lot about that quote and Burton's argument.  I've been thinking I might like to be one of those people, \"inside\", taking advantage of the good stuff happening \"outside\". That's why I'm blogging my decision to resign as Chief Evangelist and conclude three years of consultancy with the wonderfully productive Sunlight Foundation in order to explore ways to more directly help the government build great web sites and embrace Government 2.0.", "content": "<p><i>\u201cWhat we\u2019ve learned from the Open House Project,\u201d said Sunlight\u2019s Greg Elin, \u201cis that whenever we sit outside and build things, we learn that there are people inside who want to do the same thing.\u201d</i>\r\n</p>\r\n<p>\r\nThat's me, quoted in Matthew Burton's 2008 essay, <a href=\"http://www.impublished.org/wordpress/helptheman/\"><i>Why I Help \"The Man\", and Why You Should, Too</i></a>. I've been thinking a lot about that quote and Burton's argument.  I've been thinking I might like to be one of those people, \"inside\", taking advantage of the good stuff happening \"outside\". That's why I'm blogging my decision to resign as Chief Evangelist and conclude three years of consultancy with the wonderfully productive Sunlight Foundation in order to explore ways to more directly help the government build great web sites and embrace Government 2.0.\r\n</p>\r\n<p>\r\nIt would not be right to talk transparency and not walk it. That's why I'm turning in my Chief Evangelist badge now, before I really start to look. (We all have badges at Sunlight. You can't see them because they're transparent.) Realistically, it might take a while to find my future role. During that time, I would not want anyone to think I was working at cross-purposes while looking, or worse, that Sunlight was disingenuous when it came to transparency.\r\n</p>\r\n<p>\r\nBut why look in the first place? Isn't it fabulous being a part of an organization like the Sunlight Foundation? You betcha! I get to working with great people, explore technology and travel around talking about it. It's just that, frankly, I relish starting things up and windows of opportunity.  That's why it was so great to have been invited by <a href=\"http://sunlightfoundation.com/people/emiller/\">Ellen Miller</a>, <a href=\"http://sunlightfoundation.com/people/mklein/\">Mike Klein</a>, <a href=\"http://sunlightfoundation.com/people/msifry/\">Micah Sifry</a> and <a href=\"http://sunlightfoundation.com/people/arasiej/\">Andrew Rasiej</a> to start figuring out data and \"one-click disclosure\" back in early 2006. That's why it was great to come to Washington, D.C. to start the \"six-month experiment\" of the <a href=\"http://sunlightlabs.com\">Sunlight Mashup Labs</a>.  Considering how things evolved, <i>how cool is that?</i>\r\n</p>\r\n<p>\r\nI suppose it is that relish for the start of new things that has been making me look seriously at working on the  \"inside\".  I love the tipping point period where things are still being defined but one has interesting ideas, assets and tools with which to work. Even before Barack Obama was elected President, government was approaching a transparency and technology tipping point.  <a href=\"http://blog.sunlightfoundation.com/2009/03/19/the-rise-of-recoverygov-and-the-virtuous-cycle-of-transparency-innovation/\">Seeing 70 Recovery-related federal, state, and local government web sites launch in less than 60 days</a> might have been my own personal tipping point for making this change. Also, the Obama administration will soon publish initial thoughts on the <a href=\"http://www.whitehouse.gov/the_press_office/TransparencyandOpenGovernment/\">Open Government Directive</a>. Considering how things could evolve, <i>how cool will that be?</i>\r\n</p>\r\n<p>\r\nThere's too much actually happening and too much that still needs to be done to ignore the siren's call of working even closer to the source of government data than is currently possible in a non-partisan non-profit. Even more interesting things could happen on the inside with the right cluster of talents, points of view, and critical mass. Building that critical mass is where I see a special opportunity and hopefully many of my peers will see, too.  Also, the rhetoric and trends indicates\u2014at least to me\u2014we will see more open and different collaboration between government and the public in way that makes working on the \"inside\" not so isolated from what is happening on the \"outside\". The blurring of \"inside\" and \"outside\" is what makes the Web \"The Web\". Everyone is connected to each other and to (mostly) the same resources by the same network. That network has a thing for openness. Instead of a revolving door through which only a few circulate between \"inside\" and \"outside\" positions of influence, perhaps we can create a portal where everyone participates more equitably.\r\n</p>\r\n<p>\r\nSo if anyone out there knows of special opportunities, do let me know as I'm only beginning my search.  I'll be wrapping a few last projects during the month of May so I'm still reachable through the usual channels. I'll save a final sign off for later when this transition is complete and I can properly thank all the wonderful people of the greater Sunlight Foundation community. \r\n", "date_published": "2009-04-22 13:04:51", "comment_count": 2, "slug": "looking-help-man-turning-my-chief-evangelist-badge", "tags": ""}}, {"pk": 86, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-04-21 12:41:38", "author": 6, "timestamp": "2009-04-21 12:41:38", "markup": "markdown", "title": "The Apps for America Judging Process", "excerpt": "Since announcing the winners yesterday, a few people have asked for notes about how the Sunlight Foundation selected the winners. The answer is: we didn't. The Apps for America judging process went like this: we got 5 judges to agree to judge the contest-- Adrian Holovaty, Peter Corbett, Xeni Jardin, Aaron Swartz and myself. We built a very lightweight judging app ([screenshot](http://skitch.com/cjoh/bcx74/judge-for-america)) and invited every judge to log in and rate each app according to the attributes we specified on in the [contest rules](http://sunlightlabs.com/appsforamerica). ", "content": "Since announcing the winners yesterday, a few people have asked for notes about how the Sunlight Foundation selected the winners. The answer is: we didn't.  The Apps for America judging process went like this: we got 5 judges to agree to judge the contest-- Adrian Holovaty, Peter Corbett, Xeni Jardin, Aaron Swartz and myself. We built a very lightweight judging app ([screenshot](http://skitch.com/cjoh/bcx74/judge-for-america)) and invited every judge to log in and rate each app according to the attributes we specified on in the [contest rules](http://sunlightlabs.com/appsforamerica). \r\n\r\n4 of those judges showed up to vote, and for the most part every application was voted up by these four judges. The fifth judge, while they agreed to judge this contest, didn't reply to emails or respond to our various inquiries to judge. So we continued the process anyway rather than replacing the missing judge at the last minute.\r\n\r\nEach category was rated on a scale of 0-5 with 1 being the lowest score and 5 being the highest score. The categories, as a refresher from the rules were: \r\n\r\n1. Usefulness to constituents for watching over and communicating with their members of Congress\r\n2. Potential impact of ethical standards on Congress\r\n3. Originality of the application\r\n4. Potential usability of the application\r\n5. Code quality of application\r\n\r\nCode quality of the application was perhaps the most difficult for our judges to judge. One judge opted to not judge code quality and abstained from judging, which left three judges on that category.\r\n\r\nAfter the judging process was complete, we averaged all the scores, listed them in descending order, and that's the order they appear in on the [winner's list](http://sunlightlabs.com/blog/2009/04/20/and-winners-are/). \r\n\r\nA few folks have mentioned things along the lines of these apps are \"Sunlight Foundation\" selected. And while that's partially (25%) the case, the majority of votes cast were from non Sunlight Foundation employees.  If there was any bias in this group, I would suspect that it would be a \"python friendly\" slant between Aaron, Adrian and myself, though I'll point out that our first place winner is a Ruby on Rails application, and our second place one is a CodeIgniter application. \r\n\r\nFor the next round, I think we could stick with four judges and leave the fifth to the community at large somehow, though we'll have to work that out so that the voting does award merit in the application itself, not just how popular or good of an organizer the entrant is. I think we could also have a more open nominations process for who the judges are.\r\n\r\nWhile no one has complained about the judging process, I think its good to be clear for how applications were judged in this process and to think about how the process can be improved for our next round. We're just learning how to do this, and ideas are welcome!", "date_published": "2009-04-21 12:42:53", "comment_count": 4, "slug": "apps-america-judging-process", "tags": "appsforamerica contest judging"}}, {"pk": 85, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-04-20 18:15:08", "author": 5, "timestamp": "2009-04-20 18:15:08", "markup": "restructuredtext", "title": "Announcing Our Summer of Code 2009 Participants", "excerpt": "Yesterday Google announced this years participants in `Google Summer of Code <http://socghop.appspot.com/>`_, Google's program that pays students a stipend to work on an open source project during their summer.  \r\n\r\nThis is Sunlight Labs' first year participating and we were extremely grateful to be granted three slots for this summer.  Interest definitely exceeded our expectations leaving us with the difficult job of going through a lot of excellent proposals to select our top three applicants.  We wish we could have accepted more proposals and hope that some of the students that found out about us through this program will pursue their ideas even though we were not able to accept them at this time.", "content": "Yesterday Google announced this years participants in `Google Summer of Code <http://socghop.appspot.com/>`_, Google's program that pays students a stipend to work on an open source project during their summer.  \r\n\r\nThis is Sunlight Labs' first year participating and we were extremely grateful to be granted three slots for this summer.  Interest definitely exceeded our expectations leaving us with the difficult job of going through a lot of excellent proposals to select our top three applicants.  We wish we could have accepted more proposals and hope that some of the students that found out about us through this program will pursue their ideas even though we were not able to accept them at this time.\r\n\r\nTwo students, **Michael Stephens** and **Rebecca Shapiro** were both accepted for their applications relating to the `Fifty State Project <http://wiki.sunlightlabs.com/index.php/Fifty_State_Project>`_ and will be helping maintain parsers and plan the backend.  The bulk of our student applicants applied for this project, and we hope that those that weren't accepted will consider becoming active contributors as there is a lot of work to go around.  These two students stood out in that they both became active participants in the fifty state project prior to applying for Google Summer of Code, we look forward to seeing the amazing progress that will no doubt result from having these two work together on the project.\r\n\r\nChoosing our third applicant was a tough choice as there were a lot of unique ideas for transparency-related projects, but our mentors chose **Kyle Powers** to work on his proposal for a citizen-feedback tool (`Get Represented <http://wiki.sunlightlabs.com/index.php/Get_Represented>`_).\r\n\r\nWe'll have regular updates on this site on our student's progress over the summer, congratulations to Michael, Rebecca, and Kyle and thanks again to all who applied.", "date_published": "2009-04-21 06:00:00", "comment_count": 1, "slug": "gsoc-2009-participants", "tags": "gsoc2009, gsoc, fiftystates, getrepresented"}}, {"pk": 84, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-04-20 10:50:07", "author": 6, "timestamp": "2009-04-20 10:50:07", "markup": "markdown", "title": "And The Winners Are...", "excerpt": "Judging our first Apps for America contest was difficult: 45 solid, open source applications that solidly moved the ball forward in terms of opening government and providing new methods of communicating to our legislative branch.\r\n\r\nThe entries ranged from highly technical bayesian prediction tools like [Words Vote](http://sunlightlabs.com/appsforamerica/apps/words-vote/), to the super simple and super useful [GovPix](http://sunlightlabs.com/appsforamerica/apps/govpix/). Every entry presented was open source and and amazing commitment on behalf of the development community to open their government. Every single entrant was amazing and I wish we could give prizes to everyone.\r\n\r\nBut we can't. The rules say there's 1 prize for first place, 1 prize for second place, 4 third place prizes and 10 honorable mentions prizes. Which gives us 16 total prizes to give away. \r\n\r\nAnd the winners are...", "content": "Judging our first Apps for America contest was difficult: [40+ solid, open source applications](http://sunlightlabs.com/appsforamerica/apps/) that solidly moved the ball forward in terms of opening government and providing new methods of communicating to our legislative branch.\r\n\r\nThe entries ranged from highly technical Bayesian prediction tools like [Words Vote](http://sunlightlabs.com/appsforamerica/apps/words-vote/), to the super simple and super useful [GovPix](http://sunlightlabs.com/appsforamerica/apps/govpix/). Every entry presented was open source and and amazing commitment on behalf of the development community to open their government. Every single entrant was amazing and I wish we could give prizes to everyone.\r\n\r\nBut we can't. The rules say there's 1 prize for first place, 1 prize for second place, 4 third place prizes and 10 honorable mentions prizes. Which gives us 16 total prizes to give away. \r\n\r\nScoring was done with our own judging application-- if you could call it that. It was just a lightweight Django application that showed each application in an iframe, and then asked each judge to rate the app according to 5 categories. Later on, I'll run down some interesting stats from the 45 different applications, but for now, I'm sure many of you who've waited and waited want to see who has won. \r\n\r\n<h3>First Place for $15,000</h3>\r\n[Filibusted](http://filibusted.us/): \"Hold senators accountable for blocking legislation.\"\r\n<img src=\"http://img.skitch.com/20090420-eedt1e4xg99pxj6xj6jakt94ye.jpg\" alt=\"Filibusted\"/>\r\n<h3>Second Place for $5,000</h3>\r\n[Legistalker](http://legistalker.org/): \"The latest online activity of Congress Members.\"\r\n<img src=\"http://img.skitch.com/20090420-kpq2hsshkf9ff69bgkx211ghfg.jpg\" alt=\"Legistalker\"/>\r\n\r\n<h3>Third Place for $1,000 (4)</h3>\r\n\r\n[Hello Congress](http://hellocongress.org/)\r\n\r\n<img src=\"http://img.skitch.com/20090420-xcapqfwresim3wwbwgq7h6pqhr.jpg\" alt=\"Hello, Congress.\"/>\r\n\r\n[Know Thy Congressman](http://know-thy-congressman.com)\r\n\r\n<img src=\"http://img.skitch.com/20090420-1x5bm5cgm2hyk8y7d8r9m9dbm.jpg\" alt=\"Know thy Congressman\"/>\r\n\r\n[Yeas & Nays](http://www.shiftspace.org/spaces/yeas-and-nays/)\r\n\r\n<img src=\"http://img.skitch.com/20090420-gfdi7qf1gsrty9ks8uicg7axc3.jpg\" alt=\"ShiftSpace | Yeas and Nays\"/>\r\n\r\n[e-PaperTrail](http://e-papertrail.com/)\r\n\r\n<img src=\"http://img.skitch.com/20090420-bhdf5fis3fnm8cr9uhsqya4drg.jpg\" alt=\"e-PaperTrail\"/>\r\n\r\n<h3>Honorable Mentions for $100 (10)</h3>\r\n\r\n[RepresentedBy](http://sunlightlabs.com/appsforamerica/apps/representedby/)\r\n\r\n[Capital Calls](http://sunlightlabs.com/appsforamerica/apps/capital-calls/)\r\n\r\n[iLegislator](http://vimeo.com/3850221)\r\n\r\n[TweetCongress](http://tweetcongress.org)\r\n\r\n[Congress Bills](http://www.hearmyvotes.com/)\r\n\r\n[LocalPolitics.in](http://Localpolitics.in)\r\n\r\n[Words Vote.](http://code.google.com/p/wordsvote/)\r\n\r\n[Expendicus](http://offensivepolitics.net/expendicus)\r\n\r\n[Call Congress](http://callcongress.net/)\r\n\r\n[Hear Me Say This](http://hearmesaythis.org/)\r\n\r\nThis was a great start. For those of you who won, congratulations. For those of you who didn't-- thank you for participating. The good news is, you'll have another shot at this soon as this will not be the last of Apps for America. Thanks to all who participated, and we hope you continue to build and support products to open up our government!\r\n\r\n", "date_published": "2009-04-20 11:59:48", "comment_count": 10, "slug": "and-winners-are", "tags": ""}}, {"pk": 82, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-04-15 14:50:18", "author": 6, "timestamp": "2009-04-15 14:50:18", "markup": "none", "title": "Redesigning the Government: Data.gov", "excerpt": "<p>One thing we&#8217;ve been most excited about here at the Sunlight Foundation is the concept of Data.gov. Due later this year, new federal CIO Vivek Kundra will release a new central repository for government data and research. And while in this series we traditionally <em>re</em>-design federal websites, we thought we&#8217;d actually take the opportunity to design data.gov right off the bat to show you all what we&#8217;d like to see happen.</p>", "content": "<p>One thing we&#8217;ve been most excited about here at the Sunlight Foundation is the concept of Data.gov. Due later this year, new federal CIO Vivek Kundra will release a new central repository for government data and research. And while in this series we traditionally <em>re</em>-design federal websites, we thought we&#8217;d actually take the opportunity to design data.gov right off the bat to show you all what we&#8217;d like to see happen.</p>\r\n\r\n<strong>Here's what we came up with:</strong><br/>\r\n\r\n<a href=\"http://sunlightlabs.com/images/data.gov_front.jpg\"><img src=\"http://dl-client.getdropbox.com/u/36193/data.gov_front_thumb.jpg\"></a>\r\n\r\n<a href=\"http://sunlightlabs.com/images/data.gov_interior.jpg\"><img src=\"http://dl-client.getdropbox.com/u/36193/data.gov_interior_thumb.jpg\"></a><br/>\r\n\r\n<strong>Why we did it:</strong> <br/>\r\n\r\n<p>Providing access to government data is one of the clearest ways to be more transparent&#8212; and it is our hope that Kundra and team nail this with Data.gov. In order to do so, we&#8217;re looking for these things:</p>\r\n\r\n<ol>\r\n<li>Bulk access to data</li>\r\n<li>Accountability for Data Quality</li>\r\n<li>Clear, understandable language</li>\r\n<li>Service and developer friendly file formats</li>\r\n<li>Comprehensiveness</li>\r\n</ol>\r\n\r\n<p> Only raw access bulk data can be completely transparent. So we&#8217;re looking for a http://bulk.data.gov akin to Carl Malamud&#8217;s <a href=\"http://bulk.resource.org\">bulk.resource.org</a>. This will allow developers to browse through a raw directory listing of the judicial, executive, legislative branches as well as independent/miscellaneous/joint agencies and get compressed, bulk files of data via direct download. Getting FEC data, for example should be as easy as clicking on &#8220;Other&#8221;->&#8221;FEC&#8221;->&#8221;Contributions&#8221;->2008_summary.tar.gz. This first and arguably most important part of Data.gov doesn&#8217;t need any design. It needs to look like this:</p>\r\n\r\n<p><img src=\"http://dl-client.getdropbox.com/u/36193/Index%20of%20_data.gov.jpg\" alt=\"Bulk Data Screenshot\" title=\"Bulk Data\"></p>\r\n\r\n<p>Secondly, we want the ability for the public to comment and rate the quality of data government provides. The public should be able to rate, review or comment on the data sets Data.gov publishes just like it does books on Amazon.com. This will help Vivek Kundra and his team find slow patches and erroneous data faster than any form of government quality assurance process could. So take an Amazon.com style approach to data:</p>\r\n\r\n<p><img src=\"http://dl-client.getdropbox.com/u/36193/ratings.jpg\" alt=\"Ratings Shot\" title=\"Rate Data\"></p>\r\n\r\n<p>Cataloging the data sources inside the Federal Government is not good enough. Some data sources are simply just not up to par. Data sets like <a href=\"http://wiki.sunlightlabs.com/FaraDB\">FARAdb</a> are simply unusable as they're being provided by the government to citizens. But we also understand that change cannot happen overnight. In order to make this the most efficient process possible, Government should rely on the customers of its data to pinpoint where the problems are. A reviewing system for the provided data sets does just that.</p>\r\n\r\n<p>We also don&#8217;t think that Data.gov can exist without an editorial staff. You need people to write about the data and explain the data that's being provided. Let&#8217;s face it, traditionally the federal government has mostly written in a voice that lawyers and government officials can understand, but take a look at Data.gov&#8217;s closest equivalent right now: <a href=\"http://fido.gov\">Fido</a>. In looking at the different data samples here, can you tell what any of them actually do? Could your mother? Of course not. The very language that government uses is the antithesis of transparency, so use something like this to make it more friendly and understandable:</p>\r\n\r\n<p><img src=\"http://dl-client.getdropbox.com/u/36193/datagoveditorial.jpg\" alt=\"Editorial Shot\" title=\"Editorial Content\"></p>\r\n\r\n<p>Data.gov should build real, practical descriptions for the data that data.gov provides. It should speak to why each data set is important and beyond relying on the non-transparent federal-speak that is so often used. It should feature data, blog about data, and perhaps even link off to interesting things that other people are doing with the data that comes from Data.gov. But at the heart of this, at bare minimum, Data.gov has to do a better job of explaining the data than Kundra&#8217;s first attempt at this, the <a href=\"http://data.octo.dc.gov/\">DC Data Catalog</a>.</p>\r\n\r\n<p>Human understanding isn&#8217;t enough though, the data that is provided also needs to be understood by machines in formats that are common not only to developers but also to outside services like <a href=\"http://earth.google.com\">Google Earth</a> or Microsoft Excel. Data.gov should make it easy for everyone to get to its data in the format that they want. </p>\r\n\r\n<p><img src=\"http://dl-client.getdropbox.com/u/36193/datagovformats.jpg\" alt=\"File Formats\" title=\"File Formats\"></p>\r\n\r\n<p>That&#8217;s the hardest part about building a real data catalog for the Federal Government. You have databases out there that range from the 30 year old COBOL format at the FEC to the binary access databases that the FCC has been providing! But in order for Data.gov to truly be successful, it has to take these different data sources and make them available in modern data formats that developers and machines can make sense of.</p>\r\n\r\n<p>If Government makes these file formats standardized, and makes the forms that request them standardized too, then groups like Sunlight Labs can create helper classes that help developers automatically browse and interact with the data on a programmatic level rather than just browsing through a web interface. Imagine if this is how you, the developer, interact with Data.gov:</p>\r\n\r\n<p><img src=\"http://dl-client.getdropbox.com/u/36193/datagovcode.jpg\" alt=\"Code Interface\" title=\"Code Interface\"></p>\r\n\r\n<p>Data.gov has to be comprehensive and timely. While the Constitution calls for separation of powers, we do not believe that Data.gov, run by the Executive Branch of Government, should be limited to only Executive Branch information.  It should encompass all branches of Government and every independent agency. (<em>p.s. an OPML based list of all government agencies represented in the natural hierarchy of Government should be a data feed!</em>) And it should constantly be growing. When data isn&#8217;t available, people should be able to ask for it straight off the website. And obviously those requests should be a data feed in and of itself.</p>\r\n\r\n<p>Because this is a government site, we also had to think about how the regular public would interact with the site as well. We made the navigation and search simple, hiding the more complicated asks under an advanced search button, and made the home page consumer friendly by adding a description and a dashboard of the newest and most recently-updated data available. By having these things on the home page, it makes the site browsable and might help users discover data, even if they weren&#8217;t searching for anything in particular.</p>\r\n\r\n<img src=\"http://dl-client.getdropbox.com/u/36193/datagov_search.jpg\">\r\n\r\n<p>In the end, the purpose of the site should predominantly be about the data itself, and not about conclusions that may be drawn from it. It should be clear, organized, and easy to use for anyone visiting the site.</p>\r\n\r\n<p>So, here you have it, the big reveal of Data.gov:</p>\r\n<a href=\"http://sunlightlabs.com/images/data.gov_front.jpg\"><img src=\"http://dl-client.getdropbox.com/u/36193/data.gov_front_thumb.jpg\"></a>\r\n\r\n<a href=\"http://sunlightlabs.com/images/data.gov_interior.jpg\"><img src=\"http://dl-client.getdropbox.com/u/36193/data.gov_interior_thumb.jpg\"></a>", "date_published": "2009-04-16 11:52:19", "comment_count": 12, "slug": "redesigning-government-datagov", "tags": "data.gov kundra vivek redesigning the government design government"}}, {"pk": 83, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-04-16 09:47:54", "author": 3, "timestamp": "2009-04-16 09:47:54", "markup": "none", "title": "OpenSecrets.org Opens 20 Years Worth of Campaign Finance Data", "excerpt": "<p>In big news on the government data and transparency front, the premier provider of federal campaign finance information, Center for Responsive Politics (CRP), have announced <a href=\"http://www.opensecrets.org/news/2009/04/opensecretsorg-goes-opendata.html\" target=\"_blank\">they are opening for bulk download 20 years worth of data</a> used to power their web site <a href=\"http://www.opensecrets.org/\">OpenSecrets.org</a>. More than 200 million records are being made available of itemized contributions, campaign spending, lobbying, personal finance, and sponsored travel.  CRP began tracking campaign contributions in the late 1980s. Their stats and staff are trusted and quoted by the Media as the gold standard reference. The opening of the OpenSecrets.org underlying archive of bulk, standardized and industry-coded data is a seminal event for transparency and Web 2.0 political data. ", "content": "<p><i>This is <a href=\"http://blog.programmableweb.com/2009/04/16/opensecretsorg-opens-20-years-worth-of-campaign-finance-data/\">cross-posted at ProgrammableWeb</a>.</i></p>\r\n<p>In big news on the government data and transparency front, the premier provider of federal campaign finance information, Center for Responsive Politics (CRP), have announced <a href=\"http://www.opensecrets.org/news/2009/04/opensecretsorg-goes-opendata.html\" target=\"_blank\">they are opening for bulk download 20 years worth of data</a> used to power their web site <a href=\"http://www.opensecrets.org/\">OpenSecrets.org</a>. More than 200 million records are being made available of itemized contributions, campaign spending, lobbying, personal finance, and sponsored travel.  CRP began tracking campaign contributions in the late 1980s. Their stats and staff are trusted and quoted by the Media as the gold standard reference.</p>\r\n<p>The opening of the OpenSecrets.org underlying archive of bulk, standardized and industry-coded data is a seminal event for transparency and Web 2.0 political data.  Federal bailouts, <a href=\"http://www.opensecrets.org/news/2008/10/us-election-will-cost-53-billi.html\" target=\"_blank\">$5.3 billion dollar election season</a>, newspaper bankruptcies, and an administration pledging &#8220;unprecedented transparency&#8221; are forces enough to justify making this data archive available. Accessing two decades worth of campaign finance data will make it significantly easier for the many hands of the Web to forensically and predicatively examine influence in federal government.</p>\r\n<p>What makes OpenSecrets.org&#8217;s data so valuable - and this bulk release so significant - is the added standardization and industry-coding OpenSecrets.org applies the source data they acquire from the <a href=\"http://fec.gov\" target=\"_blank\">Federal Election Commission</a> (FEC).  This data conditioning will save many a scholar, reporter, and political junkie many hours of frustration. OpenSecrets.org cleans up the publicly available FEC data as much as possible to identify and standardize all the contributions from the same individual whose name and employer often varies significantly in different FEC filings by different campaigns. Furthermore, OpenSecrets.org relentlessly tracks and assigns a NAIC-like industry code to nearly every individual&#8217;s employer in order to categorize the contribution according to its most likely economic affiliation.</p>\r\n\r\n<p>This categorizing, or &#8220;coding&#8221;, makes it possible to aggregate and sum millions of individual contributions made during an election cycle by employer, industry, and economic sector.  Without someone categorizing employers and therefore contributions, it would be impossible for the rest of us to  reasonably add up the numbers in meaningful ways or look for trends in the contributions systematically.  We can see to whom those working in the banking and financial sector have been financially supporting only because OpenSecrets.org categorizes the individual contributions.</p>\r\n<p>It is this standardized and coded data that is now available in zipped CSV files for each two-year election cycle since 1990 and makes up the biggest piece of CRP&#8217;s newly opened archive. Many of the files are too big for a spreadsheet, however, so it is likely only those experienced with campaign finance or skilled with databases will be working with the data in the near term. The 200MB zipped file for the 2008 election cycle I downloaded expanded to nearly a gigabyte. Thankfully, the 59-page user guide provides a useful initial guide to the data itself and recommendations on how to do some basic calculations. </p>\r\n<p>Although mostly funded by foundations, OpenSecrets.org enjoyed some revenues through selling this bulk data to the largest news organizations. So it isn&#8217;t surprising they are a wee bit nervous about taking this step and have emphasized the attribution aspect of the data&#8217;s Creative Commons Attribution Non-Commercial Share Alike license. The staff of OpenSecrets.org has a commitment to the quality and long-term preservation of this data, and like any good data steward they are concerned the rest of the world use the data in its proper context.</p>\r\n<p>The burden is now squarely on the larger Web and \"Government 2.0\" crowd to show the opening of this data is worth it. Personally, I'm optimistic. There's more data available every day with which to mash this information. I'm particularly looking forward to seeing OpenSecrets.org industry-codes leveraged with contractors receiving <a href=\"http://www.recovery.gov/\">Recovery Act</a> money and looking for patterns of contribution trends over the past two decades.</p>\r\n<p><a href=\"http://www.programmableweb.com/government\" target=\"_blank\">ProgrammableWeb&#8217;s Government vertical</a> did not exist when I first started at the Sunlight Foundation in 2006 and now lists dozens of government APIs. The <a href=\"http://groups.google.com/group/sunlightlabs\" target=\"_blank\">Sunlight Labs developer Google group has 500 developers</a>.  And organizations like the <a href=\"http://developer.nytimes.com/\" target=\"_blank\">New York Times are creating APIs for the first time</a>.  OpenSecrets.org archive is another welcome asset to the growing data commons. </p>\r\n\r\n\r\n", "date_published": "2009-04-16 09:47:47", "comment_count": 0, "slug": "opensecretsorg-opens-data", "tags": ""}}, {"pk": 81, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-04-10 12:11:01", "author": 5, "timestamp": "2009-04-10 12:11:01", "markup": "restructuredtext", "title": "Fifty states Project: April 10th Status Report", "excerpt": "Six weeks ago we `announced on this blog <http://www.sunlightlabs.com/blog/2009/02/26/fifty-state-project/>`_ the `Fifty State Project <http://wiki.sunlightlabs.com/index.php/Fifty_State_Project>`_, our ambitious project to begin building scrapers and storing data for all legislative information from all fifty states.  At the time this project seemed like a longshot, but almost immediately a community rose to the challenge and there are now more than a dozen contributors and more than thirty states in progress.", "content": "Six weeks ago we `announced on this blog <http://www.sunlightlabs.com/blog/2009/02/26/fifty-state-project/>`_ the `Fifty State Project <http://wiki.sunlightlabs.com/index.php/Fifty_State_Project>`_, our ambitious project to begin building scrapers and storing data for all legislative information from all fifty states.  At the time this project seemed like a longshot, but almost immediately a community rose to the challenge and there are now more than a dozen contributors and more than thirty states in progress.\r\n\r\nUp until now most of the discussion has taken place on the `Sunlight Labs google group <http://groups.google.com/group/sunlightlabs>`_, but now that the project is beginning to mature we have launched a dedicated `Fifty States Project group`_.  This list aims to serve two purposes: allow project maintainers to keep up to date on the status of various states (this list is the best place to announce your intent to work on a new state for example), and also to allow for discussion of technical considerations related to writing the parsers (eg. best scraping practices).\r\n\r\nStats Breakdown\r\n---------------\r\n\r\nCurrent stats as of April 9th, for updated stats check the `wiki <http://wiki.sunlightlabs.com/index.php/Fifty_State_Project>`_ or `github <http://github.com/sunlightlabs/fiftystates/>`_.\r\n\r\n**Contributed States** (20)\r\n\r\nAlaska, California, Connecticut, Florida, Georgia, Kentucky, Massachusetts, Maine, Minnesota, Missouri, Mississippi, North Carolina, New Hampshire, New Jersey, Pennsylvania, South Dakota, Texas, Utah, Vermont, West Virginia\r\n\r\n**Claimed States** (22)\r\n\r\nColorado, Washington DC, Delaware, Hawaii, Iowa, Idaho, Illinois, Indiana, Maryland, Michigan, Nebraska, New Mexico, Nevada, New York, Ohio, Oregon, Rhode Island, South Carolina, Tennessee, Virginia, Washington, Wyoming\r\n\r\n**Unclaimed States** (9)\r\n\r\nAlabama, Arkansas, Arizona, Kansas, Louisiana, Montana, North Dakota, Oklahoma, Wisconsin\r\n\r\n**Contributors**\r\n\r\nOf the thirteen existing contributors six attended `PyCon Sprint <http://www.sunlightlabs.com/blog/2009/03/30/thank-you-pycon/>`_ and another three were from Web 2.0 Expo.  These numbers don't tell the whole story as we had about thirty participants in our sprint at PyCon, and a large number of the claimed states that are in progress are claimed by PyCon attendees.  One of the PyCon attendees has also been given commit access given the huge amount of work he has done on a number of states.\r\n\r\nWhat Now?\r\n---------\r\n\r\nThe work that all of you have done is incredible, and this project is far beyond where we thought we would be after less than two months.  All of us here are tremendously grateful for all of our contributors, especially those of you that made it out to PyCon or Web 2.0 expo.\r\n\r\nDespite the remarkable progress there is still a lot of work to do, all fifty states are still lacking vote information, and there are nine states with no work at all.  Now that there are twenty something examples in the repository it should be easier than ever to join in.  \r\n\r\nAlso, don't forget to join the new `Fifty States Project group`_ as that's where most discussion on the project will be taking place from now on.\r\n\r\n\r\n.. _`Fifty States Project group`: http://groups.google.com/group/fifty-state-project", "date_published": "2009-04-10 12:10:54", "comment_count": 1, "slug": "fifty-states-project-april-10th-status-report", "tags": "fiftystates"}}, {"pk": 80, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-04-07 15:56:33", "author": 8, "timestamp": "2009-04-07 15:56:33", "markup": "none", "title": "Redesigning the Government: Lobbyist Disclosure", "excerpt": "<img alt=\"small image from Lobbyist Disclosure comp\" src=\"http://assets.sunlightlabs.com/site/images/lobbyistDisclosure_sm.jpg\"/>\r\n\r\nJohn Wonderlich and I teamed up to show how Sunlight envisions a future executive branch lobbying disclosure site. Check it out on the <a href=\"http://blog.sunlightfoundation.com/2009/04/07/a-vision-of-real-time-lobbying-disclosure/\">Sunlight Foundation blog</a>.", "content": "<img alt=\"small image from Lobbyist Disclosure comp\" src=\"http://assets.sunlightlabs.com/site/images/lobbyistDisclosure_sm.jpg\"/>\r\n\r\nJohn Wonderlich and I teamed up to show how Sunlight envisions a future executive branch lobbying disclosure site. Check it out on the <a href=\"http://blog.sunlightfoundation.com/2009/04/07/a-vision-of-real-time-lobbying-disclosure/\">Sunlight Foundation blog</a>.", "date_published": "2009-04-07 16:42:12", "comment_count": 0, "slug": "redesigning-government-lobbyist-disclosure", "tags": "redesigning the governement, design, lobbyist disclosure, redesigning"}}, {"pk": 79, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-04-04 14:50:26", "author": 6, "timestamp": "2009-04-04 14:50:26", "markup": "markdown", "title": "Our first three months as a community", "excerpt": "Apps for America's entry period has closed, and we're now busy at work judging the applications. I dropped a note to the list, but want to share here too: because of the sheer quantity of the applications submitted, I'm extending the judging period by one week. There's just no way the judges can spend enough time judging each application (45 of them!) in one week. So we're giving ourselves a bit more time, the judging period will end two weeks from yesterday (Friday). \r\n\r\nThis has been an exciting period for our community. Over the past three months:\r\n\r\n1. We received 45 incredible applications based on the APIs and data sources we provide\r\n2. We helped organize [TransparencyCamp](http://www.transparencycamp.org) and made what's being called \"The Best Unconference Software ever written\" (soon to be released as open source)\r\n3. Launched a [50 state project](http://wiki.sunlightlabs.com/50_state_project)\r\n4. Organized two hackathons, one at (PyCon)[http://us.pycon.org] and the other at [Web2.0 Expo](http://web2expo.com/sf)\r\n5. Grew, as a community, from [0 to 460](http://groups.google.com/group/sunlightlabs).\r\n\r\nSo the question is, what's next?\r\n\r\nThat's what we're starting to figure out here at the Sunlight Foundation-- we're asking ourselves how we can better serve this fledgling community of volunteer developers and designers and also looking  ahead to what we think are new, big opportunities for furthering the goals of Open Government. Soon, I think we'll see lots of data being released directly from the Government in better, more developer-friendly formats. And it will largely up to this community to figure out what to do with all that.\r\n\r\nInside the Sunlight Foundation, we're asking ourselves the following questions:\r\n\r\n1. How do we use the sunlightlabs.com site to allow developers to organize and more effectively tackle projects and coordinate with one another?\r\n\r\n2. How do we create ways for non-technical people to help the technical people do things like help clean up data, classify information or other things that need a quick human eye?\r\n\r\n3. How do we keep track of the things inside Government that need to be fixed for us and communicate that effectively.\r\n\r\n4. How do we figure out how not only to parse all of this data, but begin to make sense of it for others?\r\n\r\nWe'd love your input too. \r\n\r\n", "content": "Apps for America's entry period has closed, and we're now busy at work judging the applications. I dropped a note to the list, but want to share here too: because of the sheer quantity of the applications submitted, I'm extending the judging period by one week. There's just no way the judges can spend enough time judging each application (45 of them!) in one week. So we're giving ourselves a bit more time, the judging period will end two weeks from yesterday (Friday). \r\n\r\nThis has been an exciting period for our community. Over the past three months:\r\n\r\n1. We received 45 incredible applications based on the APIs and data sources we provide\r\n2. We helped organize [TransparencyCamp](http://www.transparencycamp.org) and made what's being called \"The Best Unconference Software ever written\" (soon to be released as open source)\r\n3. Launched a [50 state project](http://wiki.sunlightlabs.com/50_state_project)\r\n4. Organized two hackathons, one at [PyCon](http://us.pycon.org) and the other at [Web2.0 Expo](http://web2expo.com/sf)\r\n5. Grew, as a community, from [0 to 460](http://groups.google.com/group/sunlightlabs).\r\n\r\nSo the question is, what's next?\r\n\r\nThat's what we're starting to figure out here at the Sunlight Foundation-- we're asking ourselves how we can better serve this fledgling community of volunteer developers and designers and also looking  ahead to what we think are new, big opportunities for furthering the goals of Open Government. Soon, I think we'll see lots of data being released directly from the Government in better, more developer-friendly formats. And it will largely up to this community to figure out what to do with all that.\r\n\r\nInside the Sunlight Foundation, we're asking ourselves the following questions:\r\n\r\n1. How do we use the sunlightlabs.com site to allow developers to organize and more effectively tackle projects and coordinate with one another?\r\n\r\n2. How do we create ways for non-technical people to help the technical people do things like help clean up data, classify information or other things that need a quick human eye?\r\n\r\n3. How do we keep track of the things inside Government that need to be fixed for us and communicate that effectively.\r\n\r\n4. How do we figure out how not only to parse all of this data, but begin to make sense of it for others?\r\n\r\nWe'd love your input too. \r\n", "date_published": "2009-04-04 14:51:28", "comment_count": 4, "slug": "first-three-months", "tags": "labs checkin q12009"}}, {"pk": 77, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-03-29 23:02:08", "author": 1, "timestamp": "2009-03-29 23:02:08", "markup": "markdown", "title": "Simplifying web development with django-mediasync", "excerpt": "One of the more frustrating aspects of programming for the web is managing the development and deployment of static assets. Everything is fine until your site goes live... then you have to deal with images, CSS, and JavaScript staying in sync and being called correctly from either the dev or production instance. We've developed [django-mediasync](http://github.com/sunlightlabs/django-mediasync/) to rid ourselves of the headaches.", "content": "One of the more frustrating aspects of programming for the web is managing the development and deployment of static assets. Everything is fine until your site goes live... then you have to deal with images, CSS, and JavaScript staying in sync and being called correctly from either the dev or production instance.\r\n\r\nHere in the Labs we host all of our production static content on [Amazon S3](http://aws.amazon.com/s3/). Updating an existing site involves either changing static files on the production site before the code is launched or changing script and link URLs to point locally during development... remembering to change everything back when the code is pushed out. This process causes much frustration among developers and designers as people often lose track of where things are pointing and the current state of content.\r\n\r\nTo make life easier on everyone, we've developed [django-mediasync](http://github.com/sunlightlabs/django-mediasync/) to manage the deployment of our media content. Develop locally, then when the code is ready for production run mediasync to push static media to S3. mediasync will take care of:\r\n\r\n* running gzip+jsmin on JavaScript files\r\n* running gzip+cssmin on CSS files\r\n* adding expires headers to everything (default of 1 year)\r\n* rewriting the CSS files to use the S3 media URL instead of the local development URL\r\n\r\nTemplates that reference JavaScript and CSS files use media template tags to reference the files in the correct location. When DEBUG = True, media is served locally; in production media is served from S3.\r\n\r\n    {% load media %}\r\n    {% js \"jquery.js\" %}\r\n    {% css \"screen.css\" %}\r\n    {% css_ie \"iefix.css\" %}\r\n\r\nThere are many more details in the project's README file. Take a look, use it, contribute back.\r\n\r\n[django-mediasync](http://github.com/sunlightlabs/django-mediasync/)!", "date_published": "2009-03-30 18:05:23", "comment_count": 2, "slug": "simplifying-web-development-django-mediasync", "tags": "django, python, mediasync"}}, {"pk": 78, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-03-30 17:23:29", "author": 6, "timestamp": "2009-03-30 17:23:29", "markup": "markdown", "title": "Thank you, PyCon", "excerpt": "Today was an amazing day for Sunlight Labs. Check this out:\r\n\r\n![pycon hackathon photo](http://dl-client.getdropbox.com/u/36193/1238441435639.jpg)\r\n\r\nThat's our code sprint in PyCon today. Developers have, [according to the wiki](http://wiki.sunlightlabs.com/index.php/Fifty_State_Project), checked out over 20 states to work on, and it looks like amazing, serious progress is being made there.\r\n\r\nSo to the Pycon and greater Python community-- thank you for all that you do. You are amazing.\r\n\r\n[See more of Josh's photos here](http://www.getdropbox.com/gallery/36193/1/pycon?h=7f88b3)", "content": "Today was an amazing day for Sunlight Labs. Check this out:\r\n\r\n![pycon hackathon photo](http://dl-client.getdropbox.com/u/36193/1238441435639.jpg)\r\n\r\nThat's our code sprint in PyCon today. Developers have, [according to the wiki](http://wiki.sunlightlabs.com/index.php/Fifty_State_Project), checked out over 20 states to work on, and it looks like amazing, serious progress is being made there.\r\n\r\nSo to the Pycon and greater Python community-- thank you for all that you do. You are amazing.\r\n\r\n[See more of Josh's photos here](http://www.getdropbox.com/gallery/36193/1/pycon?h=7f88b3)", "date_published": "2009-03-30 17:23:06", "comment_count": 1, "slug": "thank-you-pycon", "tags": "pycon python opensource 50states fiddystate"}}, {"pk": 76, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-03-27 17:02:39", "author": 6, "timestamp": "2009-03-27 17:02:39", "markup": "markdown", "title": "Next Week's Hackathon", "excerpt": "The [PyCon hackathon](http://us.pycon.org/2009/sprints/projects/opengov/) is about to get underway in Chicago-- James and Josh will be manning that operation, and then next week, we'll be doing the [Web2.0 Hackathon](http://bit.ly/FLcz). At both hackathons, we'll be primarily focused on the [50 State Project](http://wiki.sunlightlabs.com/index.php/Fifty_State_Project) and if time/resources permit, we may take on another project as well. We're really excited to have the support of the [Web2.0 expo](http://www.web2expo.com/) and [PyCon](http://us.pycon.org) crews for this next week of great open source development. \r\n\r\nIf you're in the bay area and want to come to our hackathon, please drop me a line at clay at sunlight labs dot com. I can give you a code that will get you a special pass into the hackathon room so you can join us.\r\n\r\n", "content": "The PyCon hackathon is about to get underway in Chicago-- James and Josh will be manning that operation, and then next week, we'll be doing the [Web2.0 Hackathon](http://bit.ly/FLcz). At both hackathons, we'll be primarily focused on the [50 State Project](http://wiki.sunlightlabs.com/index.php/Fifty_State_Project) and if time/resources permit, we may take on another project as well. We're really excited to have the support of the Web2.0 expo and PyCon crews for this next week of great open source development. \r\n\r\nIf you're in the bay area and want to come to our hackathon, please drop me a line at clay at sunlight labs dot com. I can give you a code that will get you a special pass into the hackathon room so you can join us.\r\n\r\n", "date_published": "2009-03-27 17:02:36", "comment_count": 0, "slug": "next-weeks-hackathon", "tags": ""}}, {"pk": 75, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-03-27 13:30:32", "author": 6, "timestamp": "2009-03-27 13:30:32", "markup": "markdown", "title": "Last Weekend for Apps for America", "excerpt": "This weekend is your last weekend for Apps for America work! So if you've procrastinated for the past three months, do some weekend planning and get it done. So far, we've got [7 entries](http://sunlightlabs.com/appsforamerica/apps/) and I've seen about three more on the web that haven't been submitted yet.\r\n\r\nThe apps so far are great! But I know you've got something great up your sleeve too, so make sure to set some time aside this weekend to crank out your project and make something great!", "content": "This weekend is your last weekend for Apps for America work! So if you've procrastinated for the past three months, do some weekend planning and get it done. So far, we've got [7 entries](http://sunlightlabs.com/appsforamerica/apps/) and I've seen about three more on the web that haven't been submitted yet.\r\n\r\nThe apps so far are great! But I know you've got something great up your sleeve too, so make sure to set some time aside this weekend to crank out your project and make something great!", "date_published": "2009-03-27 13:30:01", "comment_count": 0, "slug": "last-weekend-apps-america", "tags": "appsforamerica contest "}}, {"pk": 74, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-03-24 15:31:10", "author": 6, "timestamp": "2009-03-24 15:31:10", "markup": "markdown", "title": "Host your Own Hackathon", "excerpt": "We've gotten a lot of inquiries and ideas about how people can have/host a hackathon in their area. We'd love to have your help in hosting them, getting your friends together and writing some code to change America. So we've set up a [wiki page](http://wiki.sunlightlabs.com/index.php/How_to_host_a_hackathon) that provides a loose shell with some tips and advice for how to host a successful hackathon. Expect the resource to grow as we learn from our own hackathons and others around the country do the same. \r\n\r\nWe're happy to help provide guidance, too, about specific needs and projects that Sunlight needs help on.  So if you're interested in having a hackathon in your area, what are you waiting for? Start [planning one now!](http://wiki.sunlightlabs.com/index.php/How_to_host_a_hackathon).\r\n\r\nA great way for non-developers or designers to help out is to be organizers and conveners instead, organizing events, providing direction and getting people lined up to help face the great technical challenges of liberating government data. If you'd like to host a hackathon, you don't need to necessarily be technical, you just need to be organized. So take a look, read the documentation, and let us know when and where you're planning on hosting one so we can provide you with support and direction if needed. ", "content": "We've gotten a lot of inquiries and ideas about how people can have/host a hackathon in their area. We'd love to have your help in hosting them, getting your friends together and writing some code to change America. So we've set up a [wiki page](http://wiki.sunlightlabs.com/index.php/How_to_host_a_hackathon) that provides a loose shell with some tips and advice for how to host a successful hackathon. Expect the resource to grow as we learn from our own hackathons and others around the country do the same. \r\n\r\nWe're happy to help provide guidance, too, about specific needs and projects that Sunlight needs help on.  So if you're interested in having a hackathon in your area, what are you waiting for? Start [planning one now!](http://wiki.sunlightlabs.com/index.php/How_to_host_a_hackathon)\r\n\r\nA great way for non-developers or designers to help out is to be organizers and conveners instead, organizing events, providing direction and getting people lined up to help face the great technical challenges of liberating government data. If you'd like to host a hackathon, you don't need to necessarily be technical, you just need to be organized. So take a look, read the documentation, and let us know when and where you're planning on hosting one so we can provide you with support and direction if needed. ", "date_published": "2009-03-24 15:30:53", "comment_count": 0, "slug": "host-own-hackathon", "tags": "hackathon organizing opensource community"}}, {"pk": 73, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-03-23 11:09:42", "author": 8, "timestamp": "2009-03-23 11:09:42", "markup": "none", "title": "Redesigning the Government: EPA", "excerpt": "<img src=\"http://assets.sunlightlabs.com/site/images/epa_header.jpg\" alt=\"small image from EPA comp\"/>\r\n<p>Continuing with our redesigning the government project, we have moved on to the <a href=\"http://www.epa.gov\">EPA</a>. Many of you might ask why we would want to redesign the EPA \u2013 it looks like it's been redesigned recently and seems to have a lot of new and up-to-date features. I think what happens with a lot of redesigns is that people start thinking of all the fun things they can add to a site, rather than thinking of the underlying problems and finding good solutions to fit those problems. Good design and good websites aren't just made up of pretty pictures and Web 2.0 features like gradients and podcasts. A good site has structure and organization, and is easy for users to navigate. I think the EPA has started down a good path, and I want to show them that by emphasizing the right things on a page, it makes the content much more accessible and would take their good site and make it a great site.</p>", "content": "<img src=\"http://assets.sunlightlabs.com/site/images/epa_header.jpg\" alt=\"small image from EPA comp\"/>\r\n<p>Continuing with our redesigning the government project, we have moved on to the <a href=\"http://www.epa.gov\">EPA</a>. Many of you might ask why we would want to redesign the EPA \u2013 it looks like it's been redesigned recently and seems to have a lot of new and up-to-date features. I think what happens with a lot of redesigns is that people start thinking of all the fun things they can add to a site, rather than thinking of the underlying problems and finding good solutions to fit those problems. Good design and good websites aren't just made up of pretty pictures and Web 2.0 features like gradients and podcasts. A good site has structure and organization, and is easy for users to navigate. I think the EPA has started down a good path, and I want to show them that by emphasizing the right things on a page, it makes the content much more accessible and would take their good site and make it a great site.</p>\r\n\r\n<h3>EPA Applause</h3>\r\n\r\n<p>First of all, I have to say that the EPA did a really good job visually redesigning their site. I think they used great colors, the graphic elements are very fitting and pretty, and I think that their first couple levels of navigation are well thought out. I like that on their home page they have a feature box at the top of the page for their important information and also think that having the search box at the top as well was a great decision. Finally, they weren't afraid of using new technology, which is a step in the right direction.</p>\r\n\r\n<h3>Let the Rethinking Begin</h3>\r\n\r\n<p>The goal was not to redesign the graphics for this site but instead to rethink the user experience. So starting off, when you have a huge site like this which is relaying news and information to the public, take cues from people who do it well already: news organizations like <a href=\"http://www.msnbc.com\">MSNBC</a>. On their site, they have many structured sections where you can quickly and easily see the news that is most important to the user. The EPA looks as if they started down this road with their blue section bars, but they fall short by not showing enough of their content below those bars. With news stories, usually the headlines are good, short gripping snippets that will grab attention, and are usually pretty explanatory. The EPA needs that same hook, but since their content isn't as extreme as many news stories, they need to show more information to draw the user in and make him want to learn more.</p>\r\n\r\n<p>Along the same lines, another thing that MSNBC does well is relevant grouping of data. The mistake a lot of governments sites make is sectioning out data too much. For example, on the EPA's home page they have an entire section dedicated to multimedia. Instead, like on news websites, take those multimedia segments and place them into the context of the stories that they fit into. If it's a news story put it alongside other news stories with video icon, or if it's a story about teaching children about acid rain then put it into the educational section so that the particular users that piece was meant for can quickly find it. People primarily look for content about something they're interested in, not the form it takes.</p>\r\n\r\n<p>Finally, so many of the links and buttons on the EPA's newly-redesigned site actually take you back to their <a href=\"http://www.epa.gov/lawsregs/\">old site</a>, which can really leave users confused. On this old site, they seem to have a lot of great information and data, but it seems a bit scattered, with lists of links that take you to more links before you get to the relevant content. Adding good maps, graphic visualizations, adding more content below links, and fitting this content into their newly designed template would really go a long way on these pages to help users find the content and to avoid confusion. The EPA holds vast amounts of data about local communities-- that data needs to come forward and be visualized for end users for it to truly have an impact on their lives. Hopefully the EPA is currently working on these deeper sections of their site.</p>\r\n\r\n<h3>The Reveal</h3>\r\n\r\n<p>Click on the images below to see the full comps.</p>\r\n<a href=\"http://sunlightlabs.com/images/epa.jpg\"><img src=\"http://assets.sunlightlabs.com/site/images/epa_sm.jpg\" alt=\"EPA comp 1\"/></a>\r\n\r\n<a href=\"http://sunlightlabs.com/images/epa_laws.jpg\"><img src=\"http://assets.sunlightlabs.com/site/images/epa_smLaws.jpg\" alt=\"EPA comp 2\"/></a>\r\n\r\n<a href=\"http://sunlightlabs.com/images/epa_watershed.jpg\"><img src=\"http://assets.sunlightlabs.com/site/images/epa_smWatershed.jpg\" alt=\"EPA comp 3\"/></a>\r\n\r\n<h3>Conclusion</h3>\r\n\r\n<p>So when thinking about redesigning your site, like the EPA has, don't just think about making it more modern looking, but think about structure, and the experience you are creating for your end user. Some other small tips are:</p>\r\n\r\n<ul>\r\n\t<li>White space can go a long way. It helps break up the content and allows for a users eyes to breathe instead of getting overwhelmed. Just like music without rests is just noise and notes, design without white space is just cluttered pictures and text.</li>\r\n\t\r\n<li>Try to use the same color for links and actions. Many users scan a page before deciding what to click on, having all links be the same color helps them find where to click when they find their relevant section.</li>\r\n\r\n<li>Putting a box around important content doesn't always make it stand out more. When so much of the page's content is contained in boxes, more boxes just serve to clutter and confuse.  Explore other ways of calling content out \u2013 even subtle changes can go a long way.</li>\r\n\r\n<li>If you have a mobile site, don't place the link at the bottom of the page. People will have to wait for the whole page to load before they find it. Place it at the top so that your users can find it quickly and easily, but remember that it doesn't need to be huge for mobile users to find it.</li>\r\n</ul>", "date_published": "2009-03-23 11:32:03", "comment_count": 7, "slug": "redesigning-government-epa", "tags": "redesigning, design, epa, redesigning the government"}}, {"pk": 72, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-03-19 11:46:01", "author": 3, "timestamp": "2009-03-19 11:46:01", "markup": "none", "title": "usgovxml.gov - Directory of Government XML", "excerpt": "<a href=\"http://www.usgovxml.com\"><img src=\"http://blog.sunlightfoundation.com/media//2009/03/wwwusgovxmlcomindexaspx-clipped.png\" align=\"left\" /></a> Alex Madrigal (who put together <a href=\"http://howto.wired.com/wiki/Open_Up_Government_Data\">Wired's wiki on government data</a>) pointed me to <a href=\"http://www.usgovxml.com\">usgovxml.com's wonderful gem of a reference of XML resources in the Federal government<a>. Might hide some great ideas and data for use in Apps for America entries. \"<b>This site is an attempt to document, in one place and in a uniform manner, the web services and XML data sources that are provided by the US government.\"</b>", "content": "<a href=\"http://www.usgovxml.com\"><img src=\"http://blog.sunlightfoundation.com/media//2009/03/wwwusgovxmlcomindexaspx-clipped.png\" align=\"left\" /></a> Alex Madrigal (who put together <a href=\"http://howto.wired.com/wiki/Open_Up_Government_Data\">Wired's wiki on government data</a>) pointed me to <a href=\"http://www.usgovxml.com\">usgovxml.com's wonderful gem of a reference of XML resources in the Federal government<a>. Might hide some great ideas and data for use in Apps for America entries. \r\n<br /><br />\r\n<div style=\"margin:0px 16px 0px 16px\">\"<i><b>This site is an attempt to document, in one place and in a uniform manner, the web services and XML data sources that are provided by the US government.</b> The site includes source code snippets to help developers better understand how the data services can be used. The projects in which those snippets were extracted are available for download as zip files. When you click on the Example: link, you will be redirected to a location that contains the zip file. To use these projects, developers will need Visual Studio 2008 (or later), .Net Framework 3.5 (or later) and a connection to the internet.\"</i> (Emphasis added.)</div> \r\n\r\n", "date_published": "2009-03-19 11:49:35", "comment_count": 0, "slug": "cool-find-usgovxmlgov", "tags": ""}}, {"pk": 71, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-03-12 14:44:59", "author": 6, "timestamp": "2009-03-12 14:44:59", "markup": "markdown", "title": "Do we need a GetSatisfaction for Congress?", "excerpt": "On Sunlight's \"twitter lobbying\" efforts and building a more effective means for communicating to congress. Do we need a GetSatisfaction.com for Congress?", "content": "On Thursday, the Sunlight Foundation launched a campaign to \"twitter lobby\"  Congress, asking citizens to ask their Senators who are on Twitter to co-sponsor [S.482](http://www.opencongress.org/bill/111-s482/show). By my count, about [203 tweets](http://search.twitter.com/search?q=%23482) were sent out by dozens of individuals. There are 17 members of the United States Senate on Twitter, meaning that through the duration of the campaign (which lasted a few hours yesterday), the members on Twitter received an average of about 12 tweets (11.9411 to be more precise).\r\n\r\nSo far as a result, two members of the United States Senate announced, over twitter, that they support the bill: [@barbara_boxer](http://twitter.com/barbara_boxer) and [@clairecmc](http://twitter.com/clairecmc). Most surprisingly they announced their support *via* the medium itself, as well. Judging from the number of contacts, and the rapidity of response, it is one of the most successful grassroots lobbying campaigns in history! 203 Contacts for two public announcements of sponsorship! I'll say this: it would take more than 11 phone calls or emails to persuade a member of Congress to vote on something. \r\n\r\nPhilosophically, raises an interesting question. On one hand, if Barbara Boxer wasn't a United States Senator, I probably wouldn't care if she was on Twitter. She's not a friend of mine or someone I know. The reason I want her on twitter is so she'll listen to me and hopefully vote the way I want her to. The point of getting members of Congress on Twitter is so that they will listen and so that we can hear from them as real-world human beings rather than vetted, inauthentic communication.\r\n\r\nOn the other hand this experience may inevitably unleash a monster on Members of Congress. It may not be long before we're looking at swarms of people lobbying their members of congress to vote rendering Twitter completely useless as a medium for members of Congress. And like E-mail, it may become a [less effective method of interacting with your representatives.](http://www.cmfweb.org/index.php?option=com_content&task=view&id=63&Itemid=55). Soon our twitter streams may be filled with tweet-spam political agendas from our friends, neighbors and the friendly neighborhood lobbyist.\r\n\r\nBut that Sunlight did this, or that Twitter is set up the way it is, isn't the problem. The solution isn't that Twitter should somehow change, or that people shouldn't voice their opinions to their representatives in a way that best suits them. The problem is that the mediums we have available at our disposal aren't effective or **transparent** enough to be useful to citizens, members, or organizers. No matter what medium it is, if members of Congress are on it, lobbying, grassroots or otherwise, will quickly follow.\r\n\r\nWhat we need to be thinking about are new mediums that use the collaborative power of the Internet and computational power of machines to effectively communicate **with** (as opposed to _send messages to_) Congress through the various mediums that the Internet allows: email, Web, Twitter, and whatever else is out there. It should be a service for *both* citizens and representatives-- advocacy groups, and heck, even lobbyists to communicate effectively to members of Congress in a transparent way that's open and free.\r\n\r\nI've often looked at [GetSatisfaction.com](http://getsatisfaction.com) as a model for this.  GetSatisfaction flipped customer-service on its head by allowing employees to have a voice and help customers solve problems, but also allow customers to solve problems too. And they allow people to aggregate their requests and responses so that they get effectively transmitted up the food chain. I personally think the **key** is aggregation and transparency of message. It's far more valuable, for instance, for Congress to get a message saying \"Don't Club Baby Seals\" +400 next to \"Club Baby Seals\" +100 than it is to get a message saying \"Don't Club Baby Seals\" 400 times and \"Club Baby Seals\" 100 times. \r\n\r\nWe began building out a project in mid-2008 to use this model, and called it GetRepresented, but I think it is time we dusted it off and opened it up a bit. Perhaps it is worth taking a look at building a similar service for Congress in Sunlight Labs? What would it look like? I'm not sure it works to just add our 535 members of Congress to GetSatisfaction, as you've got to do things like verify that a person is from that district. And you probably want to equip some district specific aggregation as well.\r\n\r\nI've added a [Get Represented](http://wiki.sunlightlabs.com/index.php/Get_Represented) wiki page for us to brainstorm and think about this. I'd love to hear your thoughts.\r\n\r\n", "date_published": "2009-03-12 14:43:42", "comment_count": 7, "slug": "do-we-need-getsatisfaction-congress", "tags": "getrepresented twitter congress getsatisfaction tools ideas"}}, {"pk": 70, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-03-10 15:00:19", "author": 6, "timestamp": "2009-03-10 15:00:19", "markup": "markdown", "title": "Not going to SxSW? Tip: Here's how to make twitter not annoying", "excerpt": "A lot of folks from the Sunlight Foundation will be heading to [SxSW Interactive](http://sxsw.com/interactive) and we hope members of the Sunlight Labs community will [follow us on twitter](http://wiki.sunlightlabs.com/index.php/People_on_Twitter) and find us wherever we're at to meet up. It'd be great to put some faces to the names of the people in our community.\r\n\r\nFor those that aren't going to SxSW though, and do use twitter, SxSW represents an annual flood of conference information that pollutes your twitter stream with stuff you don't care about. Last year, my friend and neighbor Mike for instance, ended up unfollowing all his friends on Twitter that were at SxSW because it got so overwhelming.\r\n\r\nThis tip will only help out Mac users, but I've experimented with it and found it useful. \r\n", "content": "A lot of folks from the Sunlight Foundation will be heading to [SxSW Interactive](http://sxsw.com/interactive) and we hope members of the Sunlight Labs community will [follow us on twitter](http://wiki.sunlightlabs.com/index.php/People_on_Twitter) and find us wherever we're at to meet up. It'd be great to put some faces to the names of the people in our community.\r\n\r\nFor those that aren't going to SxSW though, and do use twitter, SxSW represents an annual flood of conference information that pollutes your twitter stream with stuff you don't care about. Last year, my friend and neighbor Mike for instance, ended up unfollowing all his friends on Twitter that were at SxSW because it got so overwhelming.\r\n\r\nThis tip will only help out Mac users, but I've experimented with it and found it useful. \r\n\r\nFirst: Switch your twitter client to [twitterriffic](http://iconfactory.com/software/twitterrific). It is a great little twitter client in its own right, but the other thing that it does is-- it has a hidden preference that allows you to filter out tweets with a particular string, like, for instance, a hashtag.\r\n\r\nNext, Open up the terminal and type this in it:\r\ndefaults write com.iconfactory.Twitterrific tweetTextFilter -string \"sxsw|SxSW|SXSW|\\\\#sxsw\"\r\n\r\nThis will filter out all the tweets mentioning SxSW to the best of this regular expression's ability. Of course, you can use this for other purposes too. Thought I'd share a handy trick in hopes that you don't get deluged with ours or others' tweets from SxSW. But if you're there, [let's connect](http://twitter.com/cjoh)!", "date_published": "2009-03-10 15:00:12", "comment_count": 6, "slug": "not-going-sxsw-heres-how-make-twitter-not-annoying", "tags": "sxsw twitter"}}, {"pk": 69, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-03-09 14:21:05", "author": 6, "timestamp": "2009-03-09 14:21:05", "markup": "markdown", "title": "50 State Project: Momentum", "excerpt": "It has been about 10 days since we announced our goal to write parsers for all 50 state legislation websites. While 10 days ago we had nothing, I'm excited to report that today our github account shows substantial progress on [8 states](http://bit.ly/fmNBf) and the District of Columbia. DC actually got two parsers-- one in Ruby and one in Python.\r\n\r\nWe've started these projects with just one function-- get_legislation that grabs all the bills of a particular state. That's the first step. We'll be grabbing votes, legislators and other things as we make progress. 8 states in 10 days is substantial progress! Great job!\r\n\r\nTo get involved and claim your state, check out our [wiki page](http://wiki.sunlightlabs.com/index.php/State_Legislation_Page) and our [github project](http://bit.ly/fmNBf).\r\n\r\nThanks for all that you do!", "content": "It has been about 10 days since we announced our goal to write parsers for all 50 state legislation websites. While 10 days ago we had nothing, I'm excited to report that today our github account shows substantial progress on [8 states](http://bit.ly/fmNBf) and the District of Columbia. DC actually got two parsers-- one in Ruby and one in Python.\r\n\r\nWe've started with just one function-- get_legislation that grabs all the bills of a particular state. That's the first step. We'll be grabbing votes, legislators and other things as they become applicable. But 8 states is substantial progress!\r\n\r\nTo get involved and claim your state, check out our [wiki page](http://wiki.sunlightlabs.com/index.php/State_Legislation_Page) and our [github project](http://bit.ly/fmNBf).\r\n\r\n", "date_published": "2009-03-09 14:19:53", "comment_count": 0, "slug": "50-state-project-momentum", "tags": ""}}, {"pk": 68, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-03-06 15:10:05", "author": 6, "timestamp": "2009-03-06 15:10:05", "markup": "markdown", "title": "Hiring Developers to Help Open Up Government", "excerpt": "The Sunlight Foundation's technology team is growing. Help us manage the Sunlight Labs community by becoming a developer here at Sunlight. Right now we're hiring for a few positions:\r\n\r\n1. [A developer with strong data and analytics skills for our Data Commons team](http://transparencyjobs.com/jobs/67/)\r\n2. [Two creative, well versed developers for our Tools and Engagement team](http://transparencyjobs.com/jobs/75/)\r\n3. [One more developer for our SubsidyScope team](http://transparencyjobs.com/jobs/56/)\r\n\r\nIf you've been thinking about a career-change, or if you're interested in getting into the realm of using your skills to change government, working on our tech team is a great opportunity.\r\n\r\nI lead our team. What I'm committed to is, and what I believe my full time job is, is creating a compelling, fun work environment that keeps our team members and community engaged. What we're trying to do on our team is three-fold:\r\n\r\n1. Build a community of volunteer sophisticated open source developers who want to change their government. We do this through things like our [hackathons](http://feedback.sunlightfoundation.com/fec/) and our [Apps for America](http://sunlightlabs.com/appsforamerica) contest. \r\n\r\n2. Build a tech team at the Sunlight Foundation that builds great tools for citizens and journalists to better see what their government is doing, who is influencing it, and how their money is being spent.\r\n\r\n3. Support the overall mission of the [Sunlight Foundation](http://sunlightfoundation.com)\r\n\r\nIf you're a great developer who wants to start using their skills to make a real difference in America, this is an opportunity for you. Send in your resume and let's talk.\r\n\r\n", "content": "The Sunlight Foundation's technology team is growing. Help us manage the Sunlight Labs community by becoming a developer here at Sunlight. Right now we're hiring for a few positions:\r\n\r\n1. [A developer with strong data and analytics skills for our Data Commons team](http://transparencyjobs.com/jobs/67/)\r\n2. [Two creative, well versed developers for our Tools and Engagement team](http://transparencyjobs.com/jobs/75/)\r\n3. [One more developer for our SubsidyScope team](http://transparencyjobs.com/jobs/56/)\r\n\r\nIf you've been thinking about a career-change, or if you're interested in getting into the realm of using your skills to change government, working on our tech team is a great opportunity.\r\n\r\nI lead our team. What I'm committed to is, and what I believe my full time job is, is creating a compelling, fun work environment that keeps our team members and community engaged. What we're trying to do on our team is three-fold:\r\n\r\n1. Build a community of volunteer sophisticated open source developers who want to change their government. We do this through things like our [hackathons](http://feedback.sunlightfoundation.com/fec/) and our [Apps for America](http://sunlightlabs.com/appsforamerica) contest. \r\n\r\n2. Build a tech team at the Sunlight Foundation that builds great tools for citizens and journalists to better see what their government is doing, who is influencing it, and how their money is being spent.\r\n\r\n3. Support the overall mission of the [Sunlight Foundation](http://sunlightfoundation.com)\r\n\r\nIf you're a great developer who wants to start using their skills to make a real difference in America, this is an opportunity for you. Send in your resume and let's talk.\r\n\r\n", "date_published": "2009-03-06 15:09:58", "comment_count": 10, "slug": "hiring-developers-help-open-government", "tags": ""}}, {"pk": 67, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-03-05 14:47:45", "author": 6, "timestamp": "2009-03-05 14:47:45", "markup": "markdown", "title": "Data.gov: The Vivek Kundra Opportunity", "excerpt": "Over the past few months, I've had the pleasure of meeting and working with Vivek Kundra in his job as the CTO of Washington, DC. Today he was today he was appointed to the new position of CIO of the federal government. He's a visionary -- a leader in the field of eGovernment who understands that technology can be used to change the way government operates, can be used to save money, as a way to inform citizens -- all for the sake of our democracy for its citizens. From my interactions with him it is clear he believes in three things:", "content": "Over the past few months, I've had the pleasure of meeting and working with Vivek Kundra in his job as the CTO of Washington, DC. Today he was appointed to the new position of CIO of the Federal Government. He's a visionary -- a leader in the field of eGovernment who understands that technology can be used to change the way government operates, can be used to save money, as a way to inform citizens -- all for the sake of our democracy for its citizens. From my interactions with him it is clear he believes in three things:\r\n\r\n**Using Alternative Market Models to Reduce Cost**\r\n\r\nA great example of an alternative market model is [Apps for Democracy](http://appsfordemocracy.org), the project that he did with [iStrategyLabs](http://istrategylabs.com) to get lots of ideas and applications developed for the District of Columbia for a pittance. While Kundra didn't invent the contest model, he was the first person inside the government to use it, and it was a smart move. Not only does it reduce the cost of building early apps, it raises awareness and identifies talent. While I suspect the operations of the Government will not be supplanted by running a bunch of contests, I suspect we'll see some significant cost savings through contest models and open source development.\r\n\r\n**Data driven decisions**\r\n\r\nKundra's into [using markets](http://www.nascio.org/awards/2008Awards/portfolioManagement.cfm) to make data driven decisions. I took a tour of his DC OCTO office a few months ago and he showed me his \"trading floor\" of Government projects. Flat panel screens of DC OCTO projects, their cost, their milestones, the teams associated with them and a big score. Scores were associated with names as well as projects, helping Kundra make decisions about how likely a project was to succeed, and find inefficiencies. Each project was given a \"buy\", \"sell\", or \"hold\" rating which helped Kundra make decisions on whether or not to continue projects.\r\n\r\n**Operational Data is Public Data**\r\n\r\nPerhaps his most profound move is to recognize that there should be no difference between the data that government useS to make decisions and the  data available to the public. Government obviously needs to protect some information-- we all agree that, for instance, there shouldn't be a feed of everybody's social security numbers. But Kundra\u2019s understanding that there is no sense in creating a \"public data source\" and an \"operational data source\" is revolutionary.\r\n\r\nAnother interesting thing about Kundra in his DC role is that unlike many government agencies, Kundra had developers working for him rather than contractors or outside firms. His research and development team in DC was led by [Dmitry Kachaev](http://www.linkedin.com/in/dmitrykachaev), a man with real technical skill who worked full time for the DC government.\r\n\r\nIf Kundra can push all three of these philosophies inside of the government, we're in for a lot of change. It isn't exactly going to be easy. The federal government has a lot more inertia in it than the District of Columbia. But I suspect you can look for some strong shifts very soon. If I was a developer looking for a job, I'd be scouring USAJobs.gov for opportunities in Office of Management and Budget or perhaps a newly created Office of the Federal CIO. We'll see where the legal home is of this new position. Some very interesting things are about to happen.", "date_published": "2009-03-05 14:59:37", "comment_count": 3, "slug": "datagov-vivek-kundra-opportunity", "tags": "kundra egovernment cio usa"}}, {"pk": 66, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-03-04 18:04:32", "author": 6, "timestamp": "2009-03-04 18:04:32", "markup": "markdown", "title": "You know we can't resist word clouds", "excerpt": "Yesterday we sent out a request for people to tell us what they thought of TransparencyCamp. Here's a word cloud of their responses I thought we'd share:\r\n\r\n![Word Cloud](http://dl-client.getdropbox.com/u/36193/Wordle%20-%20What%20People%20Thought%20of%20TransparencyCamp-2.jpg \"Feedback Word Cloud\")", "content": "Yesterday we sent out a request for people to tell us what they thought of TransparencyCamp. Here's a word cloud of their responses I thought we'd share:\r\n\r\n![Word Cloud](http://dl-client.getdropbox.com/u/36193/Wordle%20-%20What%20People%20Thought%20of%20TransparencyCamp-2.jpg \"Feedback Word Cloud\")", "date_published": "2009-03-04 18:04:26", "comment_count": 0, "slug": "you-know-we-cant-resist-word-clouds", "tags": "transparencycamp wordcloud feedback wordle"}}, {"pk": 65, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-02-27 14:03:17", "author": 6, "timestamp": "2009-02-27 14:03:17", "markup": "markdown", "title": "Hackathons!", "excerpt": "Our second big announcement of the day is that we're launching a pilot project: Sunlight Labs Live Hackathons.\r\n\r\nThe idea really came from Tim O'Reilly and O'Reilly media, who said \"Hey, why don't you come out to Web 2.0 expo and see if you can get some projects done with some of the developers there.\" We said \"Great Idea!\"\r\n\r\nAt the same time, James opened up the doors for us at PyCon and they said \"Great Idea!\"\r\n\r\nSo now the question is: what do we do? We think that's a community decision, so we put up a [feedback page](http://feedback.sunlightfoundation.com/hackathon/) where people can vote for what their favorite ideas are to work on. [Go vote now!](http://feedback.sunlightfoundation.com/hackathon/)", "content": "Our second big announcement of the day is that we're launching a pilot project: Sunlight Labs Live Hackathons.\r\n\r\nThe idea really came from Tim O'Reilly and O'Reilly media, who said \"Hey, why don't you come out to Web 2.0 expo and see if you can get some projects done with some of the developers there.\" We said \"Great Idea!\"\r\n\r\nAt the same time, James opened up the doors for us at PyCon and they said \"Great Idea!\"\r\n\r\nSo now the question is: what do we do? We think that's a community decision, so we put up a [feedback page](http://feedback.sunlightfoundation.com/hackathon/) where people can vote for what their favorite ideas are to work on. Then at each hackathon scheduled, we'll send some team leaders out to help lead the projects. [Go vote now!](http://feedback.sunlightfoundation.com/hackathon/)\r\n\r\nWe're also into your ideas, so if you have one, feel free to submit yours. A note: we're naturally averse to advocacy type ideas. We'll likely not do them. It isn't that we don't believe in democracy though! It is because we're a c3 organization, and are legally prohibited in certain ways from doing very much in the form of advocacy.\r\n\r\nBut transparency, we dig! So submit your ideas and help us choose what we all work on next and leave your comments on the ideas too. We're anxious to hear what you think.\r\n\r\nIf you're going to be at [web2.0 Expo in San Francisco](http://www.web2expo.com/), [PyCon](http://us.pycon.org/2009/sprints/projects/opengov/) or even [TransparencyCamp](http://transparencycamp.org) then we'd love to have you participate. \r\n\r\nIf you're interested in **hosting** a hackathon, let me know too! You can email me at clay at sunlight labs dot(that's with a .) com and we'll talk. \r\n\r\n", "date_published": "2009-02-27 14:02:50", "comment_count": 2, "slug": "hackathons", "tags": "hackathon opensource development "}}, {"pk": 64, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-02-26 17:33:13", "author": 9, "timestamp": "2009-02-26 17:33:13", "markup": "none", "title": "Our next big goal, The Fifty State Project", "excerpt": "While no single developer has the time to volunteer writing a custom scraper for each state, the goal of having data for all fifty states is entirely attainable if we come together and share the workload.  This is where you come in.  We need your help databasing state legislation.", "content": "<p>Those of you who are familiar with <a href=\"http://opencongress.org\">Open Congress</a> know that its power lies not in making legislative information <em>available</em>, but instead in how it makes legislation <em>accessible</em> by allowing people to interact with and repurpose what Congress produces. Unfortunately, hurdles remain in creating a better democracy at the local level and shedding light on state legislation.  At Sunlight Labs, we've been thinking about this problem for a while and now is the time for a fix.</p>\r\n\r\n<p>When Open Congress was launched and the source code released, it didn't take long for <a href=\"http://caralis.typepad.com\">Jim Caralis</a> to build <a href=\"http://www.openmass.org/\">Open Mass</a>, a site that follows the legislative events of Massachusetts.  Just like Open Congress, Open Mass makes data accessible that was once merely available by keeping track of the latest news, hot issues, and popular bills and legislators in the Massachusetts legislature.  The release of Open Mass created the hope that one by one, open sites for each state legislature would be built, but sadly, the cascading effect that we all hoped for didn't happen.  The problem?  Data for state legislatures is available, but not yet accessible.</p> \r\n\r\n<p>Although many of the tools are already available that would make an Open Congress for all fifty states possible, the one (and most important) missing link is that an openly available structured database of state legislation does not yet exist. Just as Open Congress relied on structured legislative data made available by <a href=\"http://govtrack.us\">GovTrack.us</a>, usable data must exist for state level legislation before we can start knocking out sites for every state.</p>\r\n\r\n<p>Out of curiosity, I visited the website of the <a href=\"http://www.lrc.ky.gov/\">Kentucky Legislature</a> to see how simple it would be to scrape each bill from the site and store it in a database.  It turned out to be surprisingly simple.  Using Python and the Beautiful Soup library, I quickly set up a script to scrape and store bills from sessions of the Kentucky Legislature dating back to 2001.  The entire process took about five hours and the vast majority of that was spent staring at the screen watching the scraper do its thing.  Granted, not all states will be as simple as Kentucky, but it remains a relatively simple task to squeeze state legislation into a structured format.</p>\r\n\r\n<p>While no single developer has the time to volunteer writing a custom scraper for each state, the goal of having data for all fifty states is entirely attainable if we come together and share the workload.  This is where you come in.  We need your help databasing state legislation.  To coordinate, we've set up project pages on the <a href=\"http://wiki.sunlightlabs.com/index.php/State_Legislation_Page\">Sunlight Labs Wiki</a> and <a href=\"http://github.com/sunlightlabs/fiftystates/tree/master\">github</a> to share scraping utilities, data, and ideas.  We also will be promoting the Fifty State Project at a series of \"<a href=\"http://feedback.sunlightfoundation.com/hackathon/\">hackathons</a>\" that we're hosting at various events around the country.  The hope is that soon, we'll have a standardized database and APIs to work from, putting the goal of \"an Open Congress for all fifty states\" within reach.</p>\r\n\r\n<p>Give it a shot.  Pick a state and start scraping!  It doesn't matter what language you're proficient in, the point is to get the data to a state where we can work together to make sense of it. If your experience is anything like mine was, it will only take a few hours of your time and you'll leave with the satisfaction of knowing that you freed important government information, allowing it to be used in new and meaningful ways.</p>", "date_published": "2009-02-26 18:32:42", "comment_count": 12, "slug": "fifty-state-project", "tags": "opencongress, govtrak, hackathon, fiftystate, legislation, opensource, database, scrapers, openmass, tupaclives"}}, {"pk": 63, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-02-24 17:03:20", "author": 6, "timestamp": "2009-02-24 17:03:20", "markup": "markdown", "title": "Did the Navy Spend ~400k on Facebook? (Why Bulk Data is Important)", "excerpt": "In our work with government data we encourage governments to develop [great](http://sunlightlabs.com/blog/2009/02/04/redesigning-government-fec/) [looking](http://www.sunlightlabs.com/blog/2009/01/23/rethinking_usagov/) websites with [fantastic](http://www.state.ri.us/govtracker/services/) [APIs](http://usgovxml.com/default.aspx). But you can't have true transparency unless you go all the way down to the source giving citizens access to the raw bulk data.", "content": "In our work with government data we encourage governments to develop [great](http://sunlightlabs.com/blog/2009/02/04/redesigning-government-fec/) [looking](http://www.sunlightlabs.com/blog/2009/01/23/rethinking_usagov/) websites with [fantastic](http://www.state.ri.us/govtracker/services/) [APIs](http://usgovxml.com/default.aspx). But you can't have true transparency unless you go all the way down to the source giving citizens access to the raw bulk data.\r\n\r\nTake for instance, [USAspending.gov](http://usaspending.gov) often lauded by our community as a great boon to transparency. It certainly was a great leap forward-- the website gives citizens access to descriptions of federal contracts has XML feeds, and a great search engine. But take a look at this [$375,000 expenditure](http://www.usaspending.gov/fpds/fpds.php?reptype=r&database=fpds&record_id=20483188&fiscal_year=2008&detail=3&datype=T&sortby=i) by the Department of the Navy to make Facebook \"sites\". On the surface, this is pretty incredible but since I have a summary of the contract, not the actual contract, I can't see what actually went on here. It is pretty easy to make some harsh conclusions.\r\n\r\nBefore you rush to judgement though, ask some key questions: what are the technical specifications of the project, is it really just Facebook? Was it for advertising? Who authorized the purchase? Was there hardware purchased? Was it to buy a license that is valid for other things?  What does it mean by \"NAVY OFFICER FACEBOOK SITES,\" because I didn't spend any money on my Facebook site. \r\n\r\nIn all likelihood this is an advertising expenditure. But to get that, I probably have to FOIA for it which can take time and money and could just not happen. Despite USASpending providing an adequate API, XML feeds, CSV files, and a nice search engine, it isn't adequate enough because the raw data isn't available. Without the raw data, it isn't transparent (or disclosed).\r\n\r\n\r\n\r\nThe interesting thing is that with the raw data, the rest of the stuff can be built by non government types (like for instance, us!) in ways that may provide an interesting take on the data that Government can't or won't provide. This is very clearly what our friends over at [OpenSecrets.org](http://opensecrets.org) do. They take data from the Federal Election Committee and combine it with sector and industry codes and provide quality control that makes searching through OpenSecrets data a more informative experience than looking at the FEC data on [FEC.gov](http://fec.gov).\r\n\r\nGovernment should be providing us access to data in three steps:\r\n\r\n1. First, give us bulk access to the data in its rawest form. Give us scanned images of checks and contracts. Give us the campaign contribution information as it comes in from the campaigns at the FEC (they do!).\r\n\r\n2. Second, give us machine readable APIs so that we can take the data and mash it up with data across agencies.\r\n\r\n3. Finally, create a user experience that allows non-technical citizens to access the data.\r\n\r\nBut if you start with providing a website and don't provide bulk data access or an API (like for instance, [THOMAS](http://thomas.loc.gov)) then we've got to go through the effort of parsing all the pages using something like [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/) to turn them into things like [OpenCongress.org](http://www.opencongress.org). What a drag!\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "date_published": "2009-02-24 17:05:09", "comment_count": 5, "slug": "did-navy-spend-400k-facebook-why-bulk-data-important", "tags": ""}}, {"pk": 62, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-02-23 11:47:30", "author": 6, "timestamp": "2009-02-23 11:47:30", "markup": "markdown", "title": "Content Management Systems just don't work.", "excerpt": "Somebody asked me the other day what I thought of [Recovery.gov](http://recovery.gov) using [Drupal](http://drupal.org) and it got me thinking about content management systems. In my consulting days, I watched companies and  political campaigns and non-profits sometimes spend hundreds of thousands of dollars annually trying to make their content management system do what they wanted it to do for their online campaigns. As an honest developer and honest consultant, this made me apoplectic-- I knew that at the end of the day, technically what they wanted was fairly easy to build. But they had to pay hundreds of dollars an hour to get a \"Drupal Specialist\" at an outside consultancy to set up simple pages because they could not figure out how to get the stuff to work right.\r\n\r\nI think if your budget for your website is $40,000/yr or more, you shouldn't be worrying about Content Management Systems at all. You should be worrying about hiring. I'll explain why after the jump.", "content": "Somebody asked me the other day what I thought of [Recovery.gov](http://recovery.gov) using [Drupal](http://drupal.org) and it got me thinking about content management systems. In my consulting days, I watched companies and  political campaigns and non-profits sometimes spend hundreds of thousands of dollars annually trying to make their content management system do what they wanted it to do for their online campaigns. As an honest developer and honest consultant, this made me apoplectic-- I knew that at the end of the day, technically what they wanted was fairly easy to build. But they had to pay hundreds of dollars an hour to get a \"Drupal Specialist\" at an outside consultancy to set up simple pages because they could not figure out how to get the stuff to work right.\r\n\r\nNow, this experience isn't solely limited to Drupal. From working on the Dean Campaign to consulting for hundreds of clients at Blue State Digital, to working at a non-profit like the Sunlight Foundation, I don't think I've ever been happy with a content management system. I've tried dozens, from the ever so simple blog engines like [TextPattern](http://textpattern.net/wiki/index.php?title=Index) and [Wordpress](http://wordpress.org), to the more complex [Drupal](http://drupal.org), [MovableType](http://movabletype.com/), and [ExpressionEngine](http://expressionengine.com/), to the full-scale content management systems like [Typo3](http://typo3.com/), [Bricolage](http://www.bricolage.cc/), I feel like I've worked on and with as many content management systems as I possibly can in my career. And I can't help but feel as though I've never been satisfied with any of them.\r\n\r\nAmazingly, I can't find anybody that's used their content management system that doesn't seem to either be resigned to using it, or have an amazing amount of contempt for it. People really hate their content management systems and I think the reason why is because full-blown Content Management Systems just don't work for any kind of sophisticated online presence. At the end of the day, you're probably going to want to do something that Content Management System can't do or worse-- you're going to run into something that your content management system _can_ do and does poorly. \r\n\r\nSee-- the problem with a full scale [Content Management System](http://en.wikipedia.org/wiki/Web_content_management_system) is that it has too many opinions. Those opinions were though of by somebody other than you and the needs of your organization. The more developed a content management system (or any piece of software, really) the more \"opinions\" it has. And the more \"opinions\" it has, the more likely one of them is going to really tick you off. Especially in the world of Content Management. Those opinions could be anything from \"What should the admin interface of the website look like\" to \"how should workflow work for the editorial process\" to \"What kind of user registration system should there be\" or a plethora of other opinions like \"how should data be stored\" and \"how does a designer interact with me.\" More often than not, one of these pieces of functionality is bound to create frustration and anger-- potentially rage for either you or your users.\r\n\r\nOur alternative is to take a step back from Content Management Systems. You have [software frameworks](http://en.wikipedia.org/wiki/Software_framework)-- generally a programmer's toolkit used to build things like Content Management Systems or other web applications. Instead of spending lots of money on a content management system and paying a specialist integrator to create and install and maintain it, you're better off finding a competent developer that is well versed in a framework like [Django](http://www.djangoproject.com/) or [Ruby on Rails](http://rubyonrails.org/). One competent developer using either of these frameworks can weave together a content management _solution_ and constantly be developing tools inside of this solution that make sense for your organization.\r\n\r\nWith Government in particular you end up with even more problems going the standard CMS route. When Government picks a CMS over a framework, Government is basically saying that they're going to trust the CMS's standard way of delivering data or work to change those standards and replace them with new ones. It seems to me that especially in Government it is far more effective to be using frameworks than CMSes. Recovery.gov, for instance, isn't a content management problem but a data delivery problem. Government should be focused on delivering that data in the most thorough way possible and when you factor all the different data sources that recovery.gov is going to have to talk to, I'd much rather be using python than trying to hack within the confines Drupal to do the job.\r\n\r\nThere's some arguments against taking this route. The most knee-jerk one is \"you're throwing away so much functionality\" when you don't use something like Drupal. But the point of frameworks like Django and Ruby on Rails is so that much of the common functionality (user logins, registration, even things like social networks and shopping carts) can easily be baked in to any application in the way a developer and a customer chooses. They can be built custom or sewn together through components and plug-ins.\r\n\r\nThe second argument is: \"But Drupal/ExpressionEngine/[Silverstripe](http://silverstripe.com) is a framework, too!\" and it isn't. It just simply isn't. Drupal and the like are content management systems that may have modular hooks in them to build other software, but the software crosses the line into content management systems when it starts providing default user-experiences out of the box. This means you have to un-do the way default behavior works and apply what you want as desired behavior rather than writing behavior from scratch. That's where the primary difference is between a Content Management System and a Framework are: a framework provides a minimal amount of user-facing functionality (it may provide simple ways to create it) but generally focuses on providing tools that make a developer's life easier.\r\n\r\nThe third is: \"Integration\" which for most organizations is a quixotic windmill chase if there ever was one. In my years of working on voter databases, targeting tools, contribution systems and email systems people constantly asked for all these systems to be integrated, but when asked for practical reasons why they often came up short. When you do have those practical reasons-- those integrated queries you want to run on your user data -- your developer can make that happen to you probably better than you can with squeezing your data needs into a CMS.\r\n\r\nThere's also some gray area here -- a \"Third Way\" if you will. Sometimes a framework gets extracted out of a content management system. [CodeIgniter](http://codeigniter.com/), for instance, was extracted out of ExpressionEngine, and [Sapphire](http://silverstripe.org/sapphire) was extracted out of Silverstripe. You may be tempted to say \"Hey, this is the best of both worlds!\" and it could be, but keep in mind that you're still choosing to adopt a bunch of opinions in your software from people who don't have your organization's priorities in mind. The point is to start with a framework that allows you to build solutions rapidly rather than to buy off on somebody else's solutions. Also, you have content management systems like [Ellington](http://www.ellingtoncms.com/) and [Mephisto](http://mephistoblog.com/) that are built on these pre-existing frameworks. Same deal applies-- just because the content management system is built on a framework, even one that you like,  doesn't mean that the design of the content management system works for your organization. It is likely better and cheaper to build a solution that works for you out of the framework.\r\n\r\nThis isn't to say that you need to build every application in your web presence from scratch. At Sunlight, for instance, we use [Blue State Digital](http://bluestatedigital.com)'s[1] mailer and advocacy tools and integrate them seamlessly into our campaigns when they fit the bill. They're a great provider of services that fit around what we need. We're happy to drop in other off the shelf solutions as well-- the [Sunlight Foundation blog](http://blog.sunlightfoundation.com), being a blog and all, is powered by WordPress. But when it comes to content management and end user experience, we want to control the user experience as much as possible.\r\n\r\nNow, this isn't a decision for everyone to make. But if you are sitting there thinking about your content management needs and trying to figure out how to \"power\" your website and a vendor has given you a bid somewhere close to or over $40,000 for the first year of purely technical work, consider shopping around for a full time Django or Ruby on Rails developer instead. You'll find yourself with the ability to do more, faster.\r\n\r\nOne last caveat: never hire anybody who is religious about their software. If somebody believes that \"Rails is the solution!\" or engages you in a debate about why \"Drupal creates so much possibility for our movement\" and tries to inject some form of zealotry into your thought process, get away from them as quickly as you can. Writing good software is, at the end of the day, not about the framework you use or the content management system you're an expert at, but about being a good developer. A good developer is going to seem nearly agnostic about the tools that they use and instead be able to converse with you and develop the solution that is right for the problem. A language like Python or Ruby or PHP doesn't matter nearly as much as the competency of a developer. A great developer is going to be able to do amazing things in any language or framework. \r\n\r\n**Update 5:06pm** *It should be noted here that this isn't a Drupal specific post, though it seems like there's a lot of anger from the Drupal community about this post. To those that think that this is a direct attack on Drupal/Your Favorite CMS, I ask that you take another look through the article and keep a level head. It isn't. Another misunderstanding is that this is specifically geared towards the Open Source community, which it isn't.*\r\n\r\nNext, there's a new argument I didn't account for that I should have: \"doing it all yourself means if your developer gets hit by a bus, you're going to be out of luck!\" That's not true exclusively of frameworks vs. content management systems. That's where the competency comes in: if you hire a competent ruby developer to build a Ruby on Rails solution for you and help you maintain it, that code will be maintainable, just as if you hired a competent ExpressionEngine developer to make modifications to EE Core. And if you hire an incompetent Ruby developer their code will not be maintainable, just like when you hire an incompetent EE developer. The point is **competency**.\r\n\r\n[1] it should be noted that I was a founding partner in Blue State Digital. There are plenty of other solutions out there for mass-mailing and advocacy, like Democracy in Action, Convio, and Vertical Response, and Constant Contact.", "date_published": "2009-02-23 12:39:44", "comment_count": 55, "slug": "content-management-systems-just-dont-work", "tags": "cms"}}, {"pk": 61, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-02-23 09:28:53", "author": 3, "timestamp": "2009-02-23 09:28:53", "markup": "none", "title": "Weekly Lab Report 2009.06", "excerpt": "The Labs is preparing for a sold out TransparencyCamp (Feb 28, Mar 1). Meanwhile, interesting tidbits continue. Here's what happened this past week at Sunlight Labs...\r\n<br /><br />\r\n<p>\r\n<a href=\"http://sunlightlabs.com/blog/2009/02/17/open-government-hackathon-chicago/\"><b>48 Hour Open Govt Hack-a-thon Announced.</b></a> Labs Developer James Turk is attending PyCon in Chicago at end of March (Mar 29 - 31) where Sunlight Labs will be hosting a 48 hour Hackathon. Great chance for developers to talk to people at the Labs.", "content": "The Labs is preparing for a <a href=\"http://transparencycamp.eventbrite.com/\">sold out TransparencyCamp (Feb 28, Mar 1)</a>. Meanwhile, interesting tidbits continue. Here's what happened this past week at Sunlight Labs...\r\n<br /><br />\r\n<p>\r\n<a href=\"http://sunlightlabs.com/blog/2009/02/17/open-government-hackathon-chicago/\"><b>48 Hour Open Govt Hack-a-thon Announced.</b></a> Labs Developer James Turk is attending PyCon in Chicago at end of March (Mar 29 - 31) where Sunlight Labs will be hosting a 48 hour Hackathon. Great chance for developers to talk to people at the Labs.\r\n\r\n<p>\r\n<a href=http://feedback.sunlightfoundation.com/oogl/\"><b>Our Open Govt List - OOGL!</b></a> Sunlight is gathering ideas/votes for the most important elements to tell Obama and the new CTO (whoever she may be) to add to the planned Transparency/Open Government Directive. #microtask: <a href=\"http://feedback.sunlightfoundation.com/oogl/\">Vote or add yours.</a> \r\n\r\n<p>\r\n<a href=\"http://www.nytimes.com/marketing/timesopen/\"><b>NYTIMES Open Developer Day.</b></a> NYTimes has been doing some incredible work opening up their own API. Labs Director Clay Johnson attended their Developer Day in NYC last Friday.\r\n<p>\r\n\r\n<b>Cookies, Analytics, and Government.</b> <a href=\"http://www.cdt.org\">Center for Democracy and Technology</a> and Sunlight co-hosted a group discussion on revising the Federal's government persistent cookie policy as it (mostly) pertained to web analytic data. Clay Johnson and Greg Elin attended. Conclusions: Policy needs revising (duh). Everyone are ready to revise (yea!). Next: defining a \"safe-harbor\" of practices where web managers/developers can implement without head of agency waivers.\r\n<p>\r\n\r\n<b>Learning about Map/Reduce</b> Labs Developers Garret Schure and Jeremy Carbaugh attended Hadoop training in California -- an important new skill set. Anyone out there a Hadoop expert or map/reduce expert eager to apply that expertise to government information?\r\n\r\n<p>\r\n<b>Evangelizing Data Sharing.</b> Ever evangelizing, Greg Elin visited the <a href=\"http://www.heritage.org/cda/rmodels.cfm\">Heritage Foundation Center for Data Analysis</a> to encourage them to get more of their data and economic models online.\r\n\r\n<p>\r\n<b>Link Round Up.</b> \r\n<a href=\"http:recovery.gov\">Recovery.gov gets some content</a> &bull; \r\n<a href=\"http://www.stimuluswatch.org/\">Stimulus Watch launched by Jerry Brito after Eileen Norcross</a> &bull; \r\n<a href=\"http://blog.sunlightfoundation.com/2009/02/19/the-feds-are-blogging/\">Interesting list of Federal blogs</a>\r\n\r\n<p>\r\n<b>Labs Tweet of the Week:</b> @jroo tweeted: <i>\"useful python methods for dealing with congressional data: <a href=\"http://gist.github.com/jroo\">http://gist.github.com/jroo/</a>\"\r\n\r\n", "date_published": "2009-02-23 07:44:07", "comment_count": 0, "slug": "weekly-lab-report-200906", "tags": ""}}, {"pk": 60, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-02-17 13:13:17", "author": 5, "timestamp": "2009-02-17 13:13:17", "markup": "restructuredtext", "title": "Open Government Hackathon: Chicago, March 29-31", "excerpt": "Sunlight Labs is proud to be hosting the first ever Open Government Hackathon, March 29-31 in Chicago as part of PyCon.\r\n\r\nThis will be a 48 hour sprint where developers that are interested in contributing to an open source project that will free or otherwise enhance government data can gather to brainstorm and hack on various projects related to government data.", "content": "Sunlight Labs is proud to be hosting the first ever Open Government Hackathon, March 29-31 in Chicago.\r\n\r\nThis will be a 48 hour sprint where developers that are interested in contributing to an open source project that will free or otherwise enhance government data can gather to brainstorm and hack on things.\r\n\r\nWho Should Attend?\r\n==================\r\n\r\nAnyone that is interested in contributing to an open source project that will \r\nfree or otherwise enhance government data can gather to brainstorm and hack on \r\nthings.  \r\n\r\nAlthough the sprint takes place at `PyCon <http://us.pycon.org/2009/about/>`_ you don't have to be attending \r\nthe conference to join us.  Participation is free and open to anyone.\r\n\r\nWhat Happens at the Open Government Hackathon?\r\n==============================================\r\n\r\nThe nature of the sprint will be fairly freeform, two Sunlight Labs members \r\nwill be on hand guiding users that want to contribute to opening up \r\nthe government. Users can come to the sprint without any background \r\nand we would help coordinate them and place them on a project where there \r\nskills would be useful (there is always a ton of scraping that needs to be \r\ndone, building mini-web apps to showcase gov't data, etc.)\r\n\r\nMost likely we'll spend the first session on Sunday talking about some \r\nproject ideas and then interested parties will break off into groups to work \r\non their projects.  Sunlight Labs developers will be available in person and \r\nonline to provide guidance and share expertise.\r\n\r\nFor ideas see the `Big Issues <http://wiki.sunlightlabs.com/index.php/>`_ and \r\n`Project Ideas <http://wiki.sunlightlabs.com/index.php/Project_Ideas>`_ sections of the Sunlight Labs Wiki (and feel free to contribute your own)\r\n\r\nAlso, the `Apps for America <http://sunlightlabs.com/appsforamerica/>`_ deadline is on March 31st, so it isn't too late to work on that last minute contest entry and win $100 to $15,000 dollars.\r\n\r\nI'm Sold, When/Where Is It?\r\n===========================\r\n\r\nThere will be an introductory session at 4pm on Sunday March 29th \r\nwhere we'll get oriented and start working on some ideas.  The sprint itself \r\nwill take place March 30th and 31st (with potential to go longer depending on \r\nturnout/interest).\r\n\r\nThe sprint will be taking place at the Crowne Plaza Chicago O'Hare hotel.\r\n\r\nIf you are interested in attending please either contact James (jturk@sunlightfoundation.com) or add your name to the `PyCon wiki page <http://us.pycon.org/2009/sprints/projects/opengov/>`_.", "date_published": "2009-02-17 13:12:53", "comment_count": 1, "slug": "open-government-hackathon-chicago", "tags": "python django pycon hackathon"}}, {"pk": 59, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-02-12 13:14:55", "author": 6, "timestamp": "2009-02-12 13:14:55", "markup": "markdown", "title": "What is this Don't Click business?", "excerpt": "This afternoon, a friend of mine tweeted \"Don't Click: <link>\" \r\n\r\nI, being a naturally curious human being, proceeded to click the button and saw a page with another button that says \"Don't Click\"\r\n\r\nI clicked on that button as well and then noticed that in my Twitter feed, I had in fact tweeted the same link even though I never consented to do so.\r\n\r\nHuzzah! the first twitter social virus! \r\n\r\nIt seems mostly harmless, just perpetuating itself and breeding. You can check out the graph of its use here:\r\n\r\n\r\nHere's how it works:", "content": "This afternoon, a friend of mine tweeted \"Don't Click: (link)\" \r\n\r\nI, being a naturally curious human being, proceeded to click the button and saw a page with another button that says \"Don't Click\"\r\n\r\nI clicked on that button as well and then noticed that in my Twitter feed, I had in fact tweeted the same link even though I never consented to do so.\r\n\r\nHuzzah! the first twitter social virus! \r\n\r\nIt seems mostly harmless, just perpetuating itself and breeding. You can check out the graph of its use here:\r\n\r\n\r\nHere's how it works:\r\n\r\nYou can actually link to twitter and auto-fill a message box quite easily. All you have to do is write a link like this:  [\"http://twitter.com/home?status=Sunlight Labs post on Don't Click:http://bit.ly/kj1z9\"](http://twitter.com/home?status=Sunlight Labs on Don't Click: http://bit.ly/kj1z9). What this \"virus\" does is, it creates an iframe of the page, hides it, and when you click that button and you're logged into Twitter, it makes you post that message (even though you don't see it). There's not a bit of javascript involved. The only javascript on the page is their Google Analytics code.\r\n\r\nShockingly it works amazingly well. Now we can see [all the people](http://search.twitter.com/search?q=Don%27t+Click) who probably as children defied their parents when they were told not to do something. Check out this graph of the volume of the Don't Click bug:\r\n\r\n![don't click graph](http://img.skitch.com/20090212-kn9u11dgwd7wjg5115qw1cmps2.jpg)\r\n\r\nIt also appears like all the source is available. The \"virus\" should be totally harmless. It doesn't look like your account info was stolen or anything of the sort. Here's the [source of the virus](http://translate.google.com/translate?prev=hp&hl=en&u=http%3A%2F%2Fwww.korben.info%2Fpetit-cours-de-twitt-jacking.html&sl=fr&tl=en) translated into English. ", "date_published": "2009-02-12 13:14:41", "comment_count": 10, "slug": "what-dont-click-business", "tags": ""}}, {"pk": 58, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-02-09 12:49:59", "author": 6, "timestamp": "2009-02-09 12:49:59", "markup": "markdown", "title": "I heart Bit.ly", "excerpt": "We've been using [Bit.ly](http://bit.ly) a lot here at Sunlight Labs-- I've become somewhat of an evangelist of using it over many similar shortcut services. What gets me excited about [bit.ly](http://bit.ly) is the advanced tracking capabilities it has. Did you know....\r\n\r\n1. Bit.ly tracks [friendfeed](http://friendfeed.com) and [twitter](http://www.twitter.com) conversations?\r\n2. It tracks each individual click and tells you where they're coming from? \r\n3. It actually scrapes the page you link to with it and provides you with semantic data on the page?!\r\n4. All of this is completely \"transparent?\" You can view the stats of any bit.ly link\r\n\r\nAll you have to do is put /info/ in between bit.ly and the random characters it assigns to your link. So for instance, the bit.ly link to our recent \"Redesigning the Government\" post was [http://bit.ly/LONF](http://bit.ly/LONF) But if you just put that magic /info/ in it, you can see the stats: [http://bit.ly/info/LONF](http://bit.ly/info/LONF)\r\n\r\nWe've been using it to track Twitter's effectiveness at driving traffic to the Sunlight Labs website we're quite excited to see it using [OpenCalais](http://opencalais.com) to scrape semantic data off the page as well. It is quite a unique blend of interesting information about something as simple as a link.\r\n\r\nAlso: I highly recommend the Firefox extension. It allows you to see some basic information about a bit.ly or other shortcutted link when you mouse-over it like how many clicks it has received and more importantly, what the full URL is.\r\n\r\nThanks Bit.ly!", "content": "We've been using [Bit.ly](http://bit.ly) a lot here at Sunlight Labs-- I've become somewhat of an evangelist of using it over many similar shortcut services. What gets me excited about [bit.ly](http://bit.ly) is the advanced tracking capabilities it has. Did you know....\r\n\r\n1. Bit.ly tracks [friendfeed](http://friendfeed.com) and [twitter](http://www.twitter.com) conversations?\r\n2. It tracks each individual click and tells you where they're coming from? \r\n3. It actually scrapes the page you link to with it and provides you with semantic data on the page?!\r\n4. All of this is completely \"transparent?\" You can view the stats of any bit.ly link\r\n\r\nAll you have to do is put /info/ in between bit.ly and the random characters it assigns to your link. So for instance, the bit.ly link to our recent \"Redesigning the Government\" post was [http://bit.ly/LONF](http://bit.ly/LONF) But if you just put that magic /info/ in it, you can see the stats: [http://bit.ly/info/LONF](http://bit.ly/info/LONF)\r\n\r\nWe've been using it to track Twitter's effectiveness at driving traffic to the Sunlight Labs website we're quite excited to see it using [OpenCalais](http://opencalais.com) to scrape semantic data off the page as well. It is quite a unique blend of interesting information about something as simple as a link.\r\n\r\nAlso: I highly recommend the Firefox extension. It allows you to see some basic information about a bit.ly or other shortcutted link when you mouse-over it like how many clicks it has received and more importantly, what the full URL is.\r\n\r\nThanks Bit.ly!", "date_published": "2009-02-09 12:52:08", "comment_count": 0, "slug": "clay-hearts-bitly", "tags": "bit.ly"}}, {"pk": 57, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-02-09 09:55:01", "author": 3, "timestamp": "2009-02-09 09:55:01", "markup": "none", "title": "Weekly Lab Report 2009.04", "excerpt": "More evidence of The Sunlight Foundation's pursuing its catalyst mission\u2014using the power of the Internet to catalyze greater government openness and transparency\u2014at the Sunlight Labs. Catalysts initiate reactions that precipitate the desired solution. Here's what happened this past week at Sunlight Labs...\r\n<br /><br />", "content": "More evidence of The Sunlight Foundation's pursuing its catalyst mission\u2014using the power of the Internet to catalyze greater government openness and transparency\u2014at the Sunlight Labs. Catalysts initiate reactions that precipitate the desired solution. Here's what happened this past week at Sunlight Labs...\r\n<br /><br />\r\n<p>\r\n<a href=\"http://transparencyjobs.com/jobs/67/\"><b>Sunlight Labs Application Developer sought.</b></a> Our tiny contribution to stimulate the economy. If you are interested in a large data warehouse project and live or want to live in DC, we may be interested in you for this full time position. Tag as #job.\r\n\r\n<p>\r\n<a href=\"http://wiki.sunlightlabs.com/index.php/Project_Ideas\"><b>Please Hack a Project.</b></a> You can be a Lab's contributor to government transparency without leaving your job or moving to DC. Check out the growing list of project ideas like blog plugins, ping the president, and recaptcha for federal data.  Tag as #ideas.\r\n\r\n<p>\r\n<a href=\"http://wiki.sunlightlabs.com/index.php/Stimulus_and_bailout_web_projects#Stimulus\"><b>Here a Stimulus. There a Stimulus. Everywhere a Stimulus.</b></a>#microtask: Please add any projects tracking stimulus-related data to this wiki page.\r\n<p>\r\n\r\n<a href=\"http://sunlightlabs.com/blog/2009/02/04/redesigning-government-fec/\"><b>Ali's Take on Opportunities to Improve FEC.gov.</b></a> Designer Ali Felski presented another alternate home page and site vision for a government site, FEC.gov.\r\n<p>\r\n\r\n<a href=\"http://sunlightlabs.com/blog/2009/02/02/gop-anywhere-api/\"><b>GOP goes API. James Turk reviews.</b></a> In addition to reviewing GOP's first big API effort, James has contributed a <a href=\"http://gist.github.com/58177\">a python code snippet of using the API</a>.\r\n<p>\r\n\r\n<a href=\"http://www.shiftspace.org/spaces/yeas-and-nays/\"><b>Sunlight Labs API + Asterix + FF Plugin</b></a> ShiftSpace.org is using data from Sunlight Labs API to enabling calling of Representatives from your browser. This is a great example of the economies of scale that APIs and data sharing should exist. Tag as #api, #successstory.\r\n<p>\r\n\r\n<b>Labs Tweet of the Week:</b> @EllnMllr tweeted: <i>\"Everyone thinks alike about how government data should be presented so we can use it.  <a href=\"http://bit.ly/tsm4\">http://bit.ly/tsm4</a>\"\r\n\r\n", "date_published": "2009-02-09 08:05:24", "comment_count": 0, "slug": "weekly-lab-report-200904", "tags": ""}}, {"pk": 56, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-02-07 13:06:43", "author": 3, "timestamp": "2009-02-07 13:06:43", "markup": "none", "title": "Get Your Open Source On", "excerpt": "If you know any Government folks, you might want to share with them <a href=\"http://news.cnet.com/8301-13505_3-10157924-16.html?part=rss&subj=news&tag=2547-1_3-0-5\">CNET's Matt Assay report that open source mandates are coming to enterprises</a>. Shouldn't Government be looking even harder into open source? We are. \r\n", "content": "<p>If you know any Government folks, you might want to share with them <a href=\"http://news.cnet.com/8301-13505_3-10157924-16.html?part=rss&subj=news&tag=2547-1_3-0-5\">CNET's Matt Assay report that open source mandates are coming to enterprises</a>. Shouldn't Government also be looking harder into open source? We are. \r\n</p>\r\n<p>\r\nAssay explains the mandates are coming because companies are tired of paying and re-paying for code written long ago. Physical goods have per-unit material and labor costs. Digital goods do not. Exposure to quality open source software process has a tendency to reveal the hidden inefficiency of much of the proprietary software business model. \r\n</p>\r\n<p>\r\nWe've always embraced open source at <a href=\"http://sunlightfoundation.com\">The Sunlight Foundation</a>. We are also upping our stake and <a href=\"http://sunlightlabs.com/blog/2008/12/19/labs2/\">making our open source effort larger and more formalized</a>. There's also the <a href=\"http://sunlightlabs.com/appsforamerica/\">Apps for America Contest</a> to write open source software\u2014with the requirement of using at least one Sunlight Foundation sponsored open APIs.  You can see the open source digital goods we are putting online at <a href=\"http://github.com/sunlightlabs\">http://github.com/sunlightlabs</a>.\r\n</p>\r\n<p>\r\nThe take away for Government is pretty obvious: \"If the private sector is <i>mandating</i> open source, then government may not be far behind.\" At the very least, Government needs its procurement practices to be friendly to open source.\r\n", "date_published": "2009-02-07 14:10:54", "comment_count": 0, "slug": "get-your-open-source-on", "tags": ""}}, {"pk": 55, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-02-05 10:58:00", "author": 6, "timestamp": "2009-02-05 10:58:00", "markup": "markdown", "title": "What are you doing with the Stimulus?", "excerpt": "Stimulus stimulus stimulus, all we're hearing about is the stimulus packages these days. Everyone in our field is figuring out different ways to parse bills, reports and other such things while we wait on [Recovery.gov](http://www.recovery.gov/) to launch. Whether it is [Stimulus Watch](http://stimuluswatch.org), [Read the Stimulus](http://readthestimulus.org), or even Pew's [Subsidy Scope](http://SubsidyScope.org) (for which we are providing technical assistance), it seems like everybody is trying to parse and find data related to either bailouts or the stimulus packages. \r\n\r\nWe've started a [wiki page](http://wiki.sunlightlabs.com/index.php/Stimulus_and_bailout_web_projects) to track what everybody is up to. Please add your own project here so people can check it out!\r\n\r\nA note: to limit spam on our wiki, we require you to register before you can edit pages. ", "content": "Stimulus stimulus stimulus, all we're hearing about are the stimulus packages these days. Everyone in our field is figuring out different ways to parse bills, reports and other such things while we wait on [Recovery.gov](http://www.recovery.gov/) to launch. Whether it is [Stimulus Watch](http://stimuluswatch.org), [Read the Stimulus](http://readthestimulus.org), or even Pew's [Subsidy Scope](http://SubsidyScope.org) (for which we are providing technical assistance), it seems like everybody is trying to parse and find data related to either bailouts or the stimulus packages. \r\n\r\nWe've started a [wiki page](http://wiki.sunlightlabs.com/index.php/Stimulus_and_bailout_web_projects) to track what everybody is up to. Please add your own project here so people can check it out!\r\n\r\nA note: to limit spam on our wiki, we require you to register before you can edit pages. ", "date_published": "2009-02-05 12:16:00", "comment_count": 0, "slug": "what-are-you-doing-stimulus", "tags": ""}}, {"pk": 54, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-02-04 11:07:41", "author": 8, "timestamp": "2009-02-04 11:07:41", "markup": "none", "title": "Redesigning the Government: FEC", "excerpt": "<img src=\"http://assets.sunlightlabs.com/site/images/fec_logo.jpg\" alt=\"new FEC logo\"/>\r\n\r\n<p>We got a lot of great comments and discussions happening from our redesign of USA.gov, so we thought we\u2019d continue the series and call it \u201cRedesigning the Government.\u201d At Sunlight, we deal a lot with FEC information, both internally and through one of our grantees, OpenSecrets.org so we thought we\u2019d take a look at prototyping some ways that the FEC could better disclose campaign finance information through the Web.</p>", "content": "<img src=\"http://assets.sunlightlabs.com/site/images/fec_logo.jpg\" alt=\"new FEC logo\"/>\r\n\r\n<p>We got a lot of great comments and discussions happening from <a href=\"http://www.sunlightlabs.com/blog/2009/01/23/rethinking_usagov/\">our redesign of USA.gov</a>, so we thought we\u2019d continue the series and call it \u201cRedesigning the Government.\u201d At Sunlight, we deal a lot with FEC information, both internally and through one of our grantees, <a href=\"http://www.opensecrets.org/\">OpenSecrets.org</a> so we thought we\u2019d take a look at prototyping some ways that the FEC could better disclose campaign finance information through the Web.</p>\r\n\r\n<h3>The Old</h3>\r\n\r\n<p>So lets visit <a href=\"http://www.fec.gov\">fec.gov</a> as a starting point to see what needs to be improved on the site. The first thing users will notice is that there is a very large headline that states: \u201cAdministering and Enforcing Federal Campaign Finance Laws\u201d, which is fine for lawyers who come to visit the site but I think it scares away a lot of the regular public. In fact, the FEC\u2019s primary goal according to its charter is disclosure, not administration or enforcement. We think the design and functionality of the FEC website should reflect that primary mandate. Looking at this current site I don\u2019t see a lot that would suggest that right away.</p>\r\n\r\n<img src=\"http://assets.sunlightlabs.com/site/images/fec_homepage.jpg\" alt=\"FEC title on home page\"/>\r\n\r\n<p>The second thing most users will notice, if they happen to come to the the site on a day when there is an alert, is that there is scrolling text on the bottom of the webpage. It certainly does grab your attention, but because it\u2019s scrolling across the screen, it\u2019s more difficult to read and very distracting when you\u2019re trying to read something else on the page.</p>\r\n\r\n<p>Next, we move on to the navigation. There are too many menus! There are menus on 3 sides of the site and when you rollover buttons on 2 of the sides, drop down menus appear. This creates too much confusion for the user when they are trying to find information and navigate the site. Then there is a skip navigation button that appears above the top menu that shouldn\u2019t even be visible to the regular user. This is simply used for screen readers so that when a person with a disability is using the site the screen reader won\u2019t list off every single button in menus (thanks to Jeremy for this explanation). Finally, when you resize the browser window, the layout breaks and the search box disappears behind the menu. Considering that search is a primary means of navigation for people these days, this deserves fixing.</p>\r\n\r\n<img src=\"http://assets.sunlightlabs.com/site/images/fec_fullHome.jpg\" alt=\"FEC current home page\"/>\r\n\r\n<h3>The New</h3>\r\n\r\n<p>So I started attacking the site by pay attention to the needs of the normal user. I surfaced and focused on what they would generally be interested in: campaign funds and candidate profiles. I placed some of this information on the home page but most importantly I placed radio buttons with these options below the search box to provide a faceted search allowing users to more quickly access the content they\u2019re seeking. The rest of the home page feels more accessible and open to the public by adding the seminar schedule (if a user wanted more information and wanted to attend a conference), and the commission calendar.</p>\r\n\r\n<p>Another important part of the home page is the feature section. By adding this, there is now a place for the FEC to place current information without distracting from all other content on the page and a great space to feature new and interesting visuals.</p>\r\n\r\n<p>As for the navigation, I did a quick inventory of the information that is currently on fec.gov. In doing this I found that there were a lot of things that could be paired down. For example, instead of having a search for each individual database why not just have one search but give the user more options up front so that they can narrow their search. I also placed the search box in a more prominent place so that users would use this as the main way to navigate the site.</p>\r\n\r\n<p>On the interior pages I just organized and structured the data by adding striping to the charts and adding graphics for more interest where they made sense.</p>\r\n\r\n<p>Finally, we also spent some time considering the backend. I\u2019m no developer, but I\u2019m told that the current system doesn\u2019t allow for data to be exported particularly well. We\u2019re recommending that the data be provided in open and exportable formats like XML, JSON, and CSV, which you would be able to get from any page with data on it. The developer-types have more details about this, so they might chime in with comments.</p>\r\n\r\n<h3>The Reveal</h3>\r\n\r\n<p>Click on the images below to see the full comps.</p>\r\n<a href=\"http://sunlightlabs.com/images/fec_comp1.jpg\"><img src=\"http://assets.sunlightlabs.com/site/images/fec_smcomp1.jpg\" alt=\"FEC comp 1\"/></a>\r\n<a href=\"http://sunlightlabs.com/images/fec_comp2.jpg\"><img src=\"http://assets.sunlightlabs.com/site/images/fec_smcomp2.jpg\" alt=\"FEC comp 2\"/></a>\r\n<a href=\"http://sunlightlabs.com/images/fec_comp3.jpg\"><img src=\"http://assets.sunlightlabs.com/site/images/fec_smcomp3.jpg\" alt=\"FEC comp 3\"/></a>\r\n\r\n<h3>Conclusion</h3>\r\n\r\n<p>The FEC obviously has a lot of data to think through on their site to see what is relevant and what isn\u2019t before getting to this point. But when they do, I think some of these elements could work for them or hopefully at the very least get them thinking about structure and design.</p>\r\n", "date_published": "2009-02-04 12:52:24", "comment_count": 6, "slug": "redesigning-government-fec", "tags": "redesigning, design, fec, redesigning the government"}}, {"pk": 53, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-02-03 13:05:08", "author": 3, "timestamp": "2009-02-03 13:05:08", "markup": "none", "title": "Call Congress from Your Browser", "excerpt": "<p>\"Using Yeas and Nays, a citizen can connect via phone to speak with her representatives, and the resulting shift keeps a record of the call located on the website that informed it.\" Uses Sunlight Labs API. <a href=\"http://www.shiftspace.org/spaces/yeas-and-nays/\">Link</a></p>\r\n\r\n<p><embed src=\"http://blip.tv/play/Aen9BJOWPQ\" type=\"application/x-shockwave-flash\" width=\"400\" height=\"292\" allowscriptaccess=\"always\" allowfullscreen=\"true\"></embed></p> ", "content": "<p>\"Using Yeas and Nays, a citizen can connect via phone to speak with her representatives, and the resulting shift keeps a record of the call located on the website that informed it.\" Uses Sunlight Labs API. <a href=\"http://www.shiftspace.org/spaces/yeas-and-nays/\">Link</a></p>\r\n\r\n<p><embed src=\"http://blip.tv/play/Aen9BJOWPQ\" type=\"application/x-shockwave-flash\" width=\"400\" height=\"292\" allowscriptaccess=\"always\" allowfullscreen=\"true\"></embed></p>", "date_published": "2009-02-04 09:44:26", "comment_count": 0, "slug": "call-congress-your-browser", "tags": ""}}, {"pk": 52, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-02-02 16:33:01", "author": 5, "timestamp": "2009-02-02 16:33:01", "markup": "restructuredtext", "title": "Taking a look at the GOP Anywhere API", "excerpt": "The Republicans in Congress have launched a new GOP.gov which includes an API.  While a commendable effort it leaves a lot to be desired but could serve to pave a path towards an official congressional API.", "content": "The Republicans in Congress have launched a new GOP.gov_ which on the surface looks pretty good.  The site has a nice design and places for all sorts of useful content and could has potential to serve as a portal for all sorts of useful information on current legislation (albeit from an admittedly partisan point of view).\r\n\r\nThe thing that caught my eye however was the prominent inclusion of a section on the site titled RSS & API.  This section shows RSS feeds for various sections on the site, House committees, and GOP 'Solutions' to pressing issues including the Economy, Energy, and Healthcare.  Unfortunately as of the time of writing none of these links work.\r\n\r\nHoping I would have more luck with the API I clicked over and was at first quite impressed.  The `GOP Anywhere API documentation`_  includes methods to get details on members of congress, committees, bills, votes, and various other documents.  In other words, the same thing that APIs like the `NY Times Congress API`_, `Project Vote Smart API`_, and many others aim to include -- but potentially from a more official and accurate source.\r\n\r\nI immediately signed up for an API key, signup was relatively painless, asking for the regular information (name, zipcode, email) and the examples looked good as well so I figured I'd be using the API almost immediately.  Upon firing up an interactive python session to play around I found that the simple registration process had created a false sense of security as instead I was greeted with a host of problems:\r\n\r\n* It is fairly confusing where to get your API key (both a coworker and I stumbled here at first), the URL referenced in the examples does not exist, but the key is shown to logged in users on the front page of the API documentation.\r\n* All of the API methods have to be accessed via `HTTP POST` when in fact all of the methods are simply retrieving (or GET-ting) data.  This should be relatively simple to fix, and would do wonders for allowing users to experiment with the API without a client library.\r\n* The documentation implied that you had to use an https URL (although this doesn't appear to be true and hopefully never is enforced) \r\n* There are four examples of how to make a call to the API, but written in what seems like a peculiar choice of languages: perl, VBScript, ASP.NET, and JSP.  None of these languages are exactly the languages thriving in the kind of mashup communities that an API presumably serves.  I would have expected to see at least one example in PHP, Ruby, or Python.\r\n* members.get for a Democratic district returns \"ERROR: Member Does Not Exist\" and for districts in which a Republican was ousted in the 2008 elections seems to return the defeated Republican with no indication that the representative in question is no longer in office. (an example of this is NY-29th's Randy Kuhl)\r\n* Unfortunately it is difficult to judge the rest of the API as most of the documented methods do not appear to work. see [1]_ \r\n* Finally, are Senators included?  The site seems to make no mention of this but I don't see why they shouldn't be.\r\n\r\nMany of these questions would best be discussed for most APIs on a mailing list or with some sort of API contact, but no such official list exists and the only feedback mechanism is an email address listed with a disclaimer stating that the API is not supported.  This is hardly the way to foster a community of developers building upon and improving your tools.\r\n\r\nI'm sure that some of these problems can be addressed (most likely the API simply isn't entirely online yet and the missing documentation and methods will be up shortly) but some of these are more serious.  As far as examples of access in other languages, the community can provide those (if and when the API becomes fully functional, I'll offer to do the python ones)  Although this is GOP.gov, it seems worthwhile for Republicans and Democrats alike to be returned in methods like member.get.  Allow filtering by party of course, but to simply return no data or incorrect data seems to do a disservice to all would-be users of this potentially valuable service.\r\n\r\nThe GOP is clearly trying to make an effort to offer data to the public in new and interesting ways and for that they should be commended.  I would hope that they see fit to include Democratic legislators as well (even if the GOP.gov site exists to serve primarily partisan interests, tracking the votes of Democrats seems valuable enough).  \r\n\r\nPerhaps with some improvements and a little bit of work from both sides of the aisle the GOP.gov API can become the house.gov API.\r\n\r\n.. [1] **update:** just shortly after posting I received a reply to an email I had sent to the email contact (same-day which is encouraging) letting me know that due to a rush to launch only five high-priority methods are available at the moment (bill.get, bill.getall, member.get, vote.get and vote.member)\r\n\r\n\r\n.. _`GOP.gov`: http://gop.gov\r\n.. _`GOP Anywhere API documentation`: http://www.gop.gov/api\r\n.. _`NY Times Congress API`: http://sunlightlabs.com/blog/2009/01/08/nytimes-congress-api/\r\n.. _`Project Vote Smart API`: http://api.votesmart.org/docs/index.html\r\n\r\n", "date_published": "2009-02-02 16:31:35", "comment_count": 5, "slug": "gop-anywhere-api", "tags": ""}}, {"pk": 50, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-02-02 08:36:17", "author": 3, "timestamp": "2009-02-02 08:36:17", "markup": "none", "title": "Weekly Lab Report 2009.03", "excerpt": "It was a quiet (code) week in Sunlight Labs, more a week of conversations and information sharing than production. The fact practitioners can so easily move between \"producing\" and \"knowledge sharing\" is one of the things I like about the Web and its mix of blogs and wikis and email lists and twitter, etc. Here's what happened this past week at Sunlight Labs... ", "content": "It was a quiet (code) week in Sunlight Labs, more a week of conversations and information sharing than production. The fact practitioners can so easily move between \"producing\" and \"knowledge sharing\" is one of the things I like about the Web and its mix of blogs and wikis and email lists and twitter, etc. Here's what happened this past week at Sunlight Labs... \r\n<br /><br />\r\n<p>\r\n<a href=\"http://wiki.sunlightlabs.com/index.php/How_you_can_help\"><b>How You Can Help Wiki Page.</b></a> There's a now a very rough wiki page outlining different tasks developers, designers, and evangelist-inclined can dive into to make government more transparent. \r\n\r\n<p>\r\n<a href=\"http://sunlightlabs.com/blog/2009/01/27/section-508-compliance-easier-you-think/\"><b>Accessibility is Easier via Web Standards.</b></a> Who knew Senior Developer Jeremy Carbaugh use was once did Section 508 compliance testing? Who knew that 80% of compliance can be achieved via today's web standards? Tag as #reference.\r\n\r\n<p>\r\n<a href=\"http://sunlightlabs.com/blog/2009/01/28/govt-hackers-know-these-challenges/\"><b>Required reading for Government Hackers.</b></a> Some great documents online describe the challenges Government faces in implementing technologies the larger Web takes for granted.  Greg Elin put together this handy set of links to those documents.  Tag as #reference.\r\n\r\n<p>\r\n<a href=\"http://wiki.sunlightlabs.com/index.php/Sunlight_Labs_Wordlist_API\"><b>Stop! In the Name of Words.</b></a> Before stopwords distort your frequency counts... Removing stopwords\u2014frequently occurring words that are filtered when algorithmically analyzing content\u2014is a little wheel everyone keeps re-inventing. So, Sunlight Labs is experimenting with a stopwords removal API service. \r\n<p>\r\n\r\n<a href=\"http://wiki.sunlightlabs.com/index.php/State_Legislation_Page\"><b>#microtask Add Your State's Legislation Website.</b></a> I love lists. We're creating one of State Legislation web pages. \r\n<p>\r\n\r\n<p>\r\n<b>Maven Tim's Restaurant Pick:</b> <a href=\"http://www.gwhospital.com\">George Washington University Hospital</a> So eager was Tim to eat here, he broke his collarbone to do it. Our wishes for a speedy recovery.\r\n<p>\r\n<b>Labs Tweet of the Week:</b> @jroo tweeted: <i>\"@EllnMllr sent this out to the Sunlight staff list. I think everyone should see it: <a href=\"http://bit.ly/FeXk\">http://bit.ly/FeXk</a>\"\r\n\r\n", "date_published": "2009-02-01 12:00:00", "comment_count": 0, "slug": "weekly-lab-report-200903", "tags": ""}}, {"pk": 43, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-19 10:40:31", "author": 3, "timestamp": "2009-01-19 10:40:31", "markup": "none", "title": "Government Hackers - Know These Challenges", "excerpt": "The value proposition of Web 2.0 changing government is no longer debated. But your govt hacker street cred depends on understanding the real challenge is interoperability with policies and rules created for a federal government whose founders embraced the disruptive technology of their time, the printing press.\r\n<p>\r\nThe following links are your briefing book...", "content": "<a href=\"http://www.wired.com/politics/onlinerights/magazine/17-02/ff_obama?currentPage=4\"><img src=\"http://blog.sunlightfoundation.com/media//2009/01/wired-govt-obstacles-2.png\" width=\"410\" title=\"Screenshot excerpt of the obstacles for a wired presidency from January 2009 Wired article on the topic.\r\n\"/></a>\r\n<p>\r\nThe value proposition of Web 2.0 changing government is no longer debated.  But if you want to be a player  improving transparency and democracy, your street cred depends on understanding the real challenge is interoperability with policies and rules created for a federal government whose founders embraced the disruptive technology of their time, the printing press.\r\n<p>\r\nThe following links are your briefing book...\r\n<ul>\r\n<li><a href=\"http://www.wired.com/politics/onlinerights/magazine/17-02/ff_obama?currentPage=1\">Evan Ratliff Wired Magazine article summarizing on what Obama faces to create a Wired Presidency</a>. Read it all.</li>\r\n<li><a href=\"http://www.usa.gov/webcontent/technology/other_tech.shtml\">WebContent.gov's Social Media and Web 2.0 in Government reference page</a>. Be sure and read Dec 2008's <a href=\"http://www.usa.gov/webcontent/documents/SocialMediaFed%20Govt_BarriersPotentialSolutions.pdf\">Barriers and Solutions to Implementing Social Media and Web 2.0 in Government</a></li>\r\n<li>Federal Web Managers Council's White Paper for Transition Team <a href=\"http://www.usa.gov/webcontent/documents/Federal_Web_Managers_WhitePaper.pdf\">Putting Citizens First: Transforming Online Government</a></li>\r\n<li><a href=\"http://blog.cdt.org/2009/01/08/a-new-cookie-policy-for-egov-20-%E2%80%93-part-i/\">CDT's Ari Swartz on an E-Govt Cookie Policy Part I</a> and <a href=\"http://blog.cdt.org/2009/01/09/a-new-cookie-policy-for-e-gov-20-part-2/\">Part 2 which the Labs helped inform</a></li>\r\n<li><a href=\"http://resource.org/8_principles.html\">8 Principles of Open Data</a></li>\r\n<li><a href=\"http://www.mysociety.org/2009/01/07/top-5-internet-priorities-for-the-next-government-any-next-government/\">Tom Steinberg's Top 5 Internet Priorities for the Next Government (any next Government)</a></li>\r\n<li><a href=\"http://blog.sunlightfoundation.com/2009/01/06/federal-cto-wishlist/\">Sunlight's Federal CTO Wishlist (John Wonderlich)</li>\r\n<li><a href=\"http://blog.sunlightfoundation.com/2008/12/03/yes-we-canuse-comments-web-services-on-government-web-sites/\">Sunlight Foundation post: Yes we can use comments, web services on Government web sites (Greg Elin)</a></li>\r\n<li><a href=\"http://www.ombwatch.org/article/archive/551\">Right to Know Recommendations</a> - Long document, includes more general policy issues beyond technology</li>", "date_published": "2009-01-28 09:55:47", "comment_count": 2, "slug": "govt-hackers-know-these-challenges", "tags": ""}}, {"pk": 49, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-27 09:57:33", "author": 1, "timestamp": "2009-01-27 09:57:33", "markup": "markdown", "title": "Section 508 compliance is easier than you think", "excerpt": "Worried about having to comply with Section 508 accessiblity standards? Don't! 80% of the work is done for you if you follow web standards when developing a site. Part one of a two-part series.", "content": "_This is part one of a two-part post. Part one covers the basics of web standards and progressive enhancement and Section 508 standards \u00a71194.22 (a)-(f)._\r\n\r\nLabs designer Ali Felski [recently posted](http://sunlightlabs.com/blog/2009/01/23/rethinking_usagov/) about an experiment in redesigning the web site of [USA.gov](http://usa.gov). One of the commenters on the post felt that the design would not be feasible as it is not compliant with [Section 508 accessibility standards](http://www.section508.gov/index.cfm?FuseAction=Content&ID=12). I disagree. It is nearly impossible to infer a site's compliance with Section 508 from the design.\r\n\r\nSeveral years ago I worked (contracted) as a Section 508 tester for the Treasury Department on their [Governmentwide Accounting and Reporting Modernization](http://www.fms.treas.gov/gwa/) project. Since then I have developed applications for the IRS and FBI that have had to implement Section 508 standards. There seems to be quite a bit of confusion over the implications of Section 508 and the effort involved in implementing the standards... even amongst those dealing with it each day. In this two-part post I will dispel some of the misconceptions and show how easy it is to accomplish when sites are developed with web standards and progressive enhancement.\r\n\r\n### Developing with web standards\r\n\r\nWhen developing with web standards, there are three important things to keep in mind:\r\n\r\n*   HTML is content\r\n*   CSS is presentation\r\n*   JavaScript is behavior\r\n\r\nThink about it for a minute and let it sink in. Most people think of HTML as that markup language that is used to layout web pages. This, however, is a violation of the separation of layers. HTML is meant to give semantic meaning to your content. __p__ tags denote paragraphs. __div__ tags indicate logic divisions within content. __table__ tags are used for tabular data and _not for layout_... as tempting as it is.\r\n\r\n__img__ tags are another good example. You would probably think that the Sunlight Labs logo at the top of this page is rendered with an __img__ tag in the HTML. By doing this we would be including presentation elements in our content. The proper way to implement this is by using an __h1__ tag containing 'Sunlight Labs' and using CSS [image replacement techniques](http://css-discuss.incutio.com/?page=ImageReplacement) to display the logo image on the page instead of the text. One of the benefits of this method is that our organization name is included in the content of the page rather than locked away in an image.\r\n\r\n### Progressive enhancement\r\n\r\nSo far we've seen an example of maintaining a separation of content and style. [Progressive enhancement](http://www.alistapart.com/articles/understandingprogressiveenhancement) is a method of separating content and behavior.\r\n\r\nWhat happens if we don't maintain of separation of content and behavior and the user has JavaScript disabled? In most cases the page will either be missing content or not function correctly. By creating fully functional web sites without JavaScript and then layering on behavioral features, it is possible to create functional and rich user experiences.\r\n\r\n### Section 508 Guidelines\r\n\r\n*   #### (a) A text equivalent for every non-text element shall be provided (e.g., via \"alt\", \"longdesc\", or in element content).\r\n\r\n    If we properly separate our three layers, we remove most of the situations in which we would have to provide text equivalents. Markup should only include __img__ tags when the image is actually part of the content of the page (i.e. [Flickr](http://flickr.com) or [Boston.com's The Big Picture](http://www.boston.com/bigpicture/)).\r\n\r\n    _The lesson:_ Logos, navigation, buttons and other content elements are not proper uses of __img__ tags. When non-text content is necessary, use title and alt attributes.\r\n\r\n*   #### (b) Equivalent alternatives for any multimedia presentation shall be synchronized with the presentation.\r\n\r\n    That fancy Java slideshow applet that adds ripple effects to the slides probably isn't necessary. Simplify your life and make basic HTML pages styled with CSS instead. \r\n\r\n    _The lesson:_ If you do need multimedia capabilities, use technologies that have accessibility options. \r\n\r\n*   #### (c) Web pages shall be designed so that all information conveyed with color is also available without color, for example from context or markup.\r\n\r\n    Users with colorblindness or screen readers can't tell a red icon from a green icon. If all you have in your HTML is an __img__ tag, you are hiding content from people that are unable to see the page. Be sure to have meaningful text content in your HTML whenever you are conveying information with color.\r\n\r\n\t_The lesson:_ Color is presentation, not content. Convey all information as text in HTML and use styles heets to make it pretty and colorful.\r\n\r\n*   #### (d) Documents shall be organized so they are readable without requiring an associated style sheet.\r\n\r\n    Remember our three layers of separation?\r\n\r\n    _The lesson:_ If you have created a page that needs CSS or JavaScript to be readable, you have violated our \"HTML is content\" principle. Semantic markup without styles creates HTML that is meaningful even if it is boring to look at.\r\n\r\n*   #### (e) Redundant text links shall be provided for each active region of a server-side image map.\r\n\r\n    Don't use image maps as they are almost always unnecesary. Simple as that.\r\n\r\n    _The lesson:_ Provide basic text interactions with plain old HTML. If rich interactions are necessary, enable them by using progressive enhancement techniques.\r\n\r\n*   #### (f) Client-side image maps shall be provided instead of server-side image maps except where the regions cannot be defined with an available geometric shape.\r\n\r\n    _The lesson:_ Didn't we agree to not use image maps?\r\n\r\nStay tuned for part two where we explore standards (g)-(p)!", "date_published": "2009-01-27 23:33:20", "comment_count": 1, "slug": "section-508-compliance-easier-you-think", "tags": "section508 webstandards"}}, {"pk": 48, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-26 01:32:31", "author": 3, "timestamp": "2009-01-26 01:32:31", "markup": "none", "title": "Weekly Lab Report 2009.02", "excerpt": "You probably heard a new President was inaugurated this past week and that his first memo from the White House was about TRANSPARENCY. (Yhaaa.)  We were back in our Lab coats nevertheless. Here's what happened last week at Sunlight Labs. ", "content": "<p>\r\n<b>Around here, we use APIs.</b> <a href=\"http://capitolwords.org/api/\">CapitolWords.org has an API</a> linked from the home page, but our Systems Guru Tim Ball noticed someone scraping the site dictionary word by dictionary word.  I know, it's hard to let go of one's dysfunctions, but you we <i>want</i> you to use our data. We have an API.  \r\n<p>\r\n<a href=\"http://thisweekindjango.com/links/2009/jan/24/django-secretballot/\"><b>Our Django Voting App gets some love.</b></a> ThisWeekinDjango.com threw more whuffie our way by highlighting our open source module for allowing users to vote without having to login. \r\n<p>\r\n<a href=\"http://en.wikipedia.org/wiki/Brian_Behlendorf\"><b>Touched by Brian Behlendorf.</b></a> Brian was a key developer behind Apache, Apache Foundation, and Subversion. <a href=\"http://oreilly.com/catalog/opensources/book/brian.html\">Brian also wrote this great essay in 1999 about open source and platforms</a> and visited our offices this week in 2009 to discuss open source in government. Also visiting was Deb Bryant from Oregon State University's <a href=\"http://osuosl.org/hosting/clients\">Open Source Lab who's infrastructure hosts the BIG open source projects</a>. \r\n<p>\r\n\r\n<p><a href=\"http://sunlightlabs.com/blog/2009/01/23/rethinking_usagov/\"><b>See Ali blog. Blog. Blog. Blog.</b></a> Senior Designer Ali Felski, who joined Sunlight in the fall and makes these beautiful pages, blogged here for the first time this week, writing about her mockup of a redesigned USA.gov site.\r\n\r\n<p><b>Apps for America already Rocking!</b>\r\n It's a big deal when the author <a href=\"http://obiefernandez.com/\">The Rails Way</a> Obie Fernandez <a href=\"http://blog.obiefernandez.com/content/2009/01/getting-our-national-service-groove-on-with-apps-for-america.html\">spends his Day of Service working on a transparency app</a>. It's also pretty amazing when guy who's you only know by his weird blog name hears about our <a href=\"http://sunlightlabs.com/appsforamerica\">Apps for America Contest</a> and creates a hack to use our API and Google to automagically discover the RSS feed every Member of Congress! \r\n\r\n<p><a href=\"http://spreadsheets.google.com/pub?key=p-kX0k_KXgtwwzHnvEn4eCA\">Stimulus package spreadsheet</a> Clay and Jeremy put together a public Google spreadsheet of spending in the proposed stimulus package. Read the post <a href=\"http://sunlightlabs.com/blog/2009/01/21/what-were-doing-stimulus-bill/\">here</a>.\r\n\r\n<p>\r\n<b>Maven Tim's Restaurant Pick:</b> <a href=\"http://www.yelp.com/biz/nava-thai-noodle-and-grill-wheaton\">Nava Thai</a> Because it is \"friendly\" and \"cheap\" after you spent all your money on inauguration celebrations.\r\n<p>\r\n<b>Labs Tweet of the Week:</b> @jroo tweeted: <i>\"'Transparency and the rule of law will be the touchstones of this presidency': <a href=\"http://tinyurl.com/aul7os\">http://tinyurl.com/aul7os</a>\"\r\n", "date_published": "2009-01-25 17:00:00", "comment_count": 0, "slug": "weekly-lab-report-200902", "tags": ""}}, {"pk": 47, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-23 13:12:13", "author": 8, "timestamp": "2009-01-23 13:12:13", "markup": "none", "title": "Rethinking usa.gov", "excerpt": "<img class=\"sideImage\" src=\"http://assets.sunlightlabs.com/site/images/usaGov_newLogo.jpg\" alt=\"header on usa.gov\"/>\r\n\r\n<p>With Obama in the President\u2019s seat now, and many new people coming into the vast executive branch, they now have an opportunity to revisit their presence on the web and explore the possibilities of getting the American people more interested and more informed about what their government is doing. The hub for all this information is a site known as <a href=\"http://www.usa.gov\">usa.gov</a>.</p>", "content": "<img class=\"sideImage\" src=\"http://assets.sunlightlabs.com/site/images/usaGov_newLogo.jpg\" alt=\"header on usa.gov\"/>\r\n\r\n<p>With Obama in the President\u2019s seat now, and many new people coming into the vast executive branch, they now have an opportunity to revisit their presence on the web and explore the possibilities of getting the American people more interested and more informed about what their government is doing. The hub for all this information is a site known as <a href=\"http://www.usa.gov\">usa.gov</a>.</p>\r\n\r\n<p>So what would it be like if the new administration were to rethink their presence on the web and turn usa.gov into a more open facing site\u2013a site where people could pick and choose the content that was relevant to them and display it all in a way that was organized and appealing. At the <a href=\"http://www.sunlightfoundation.com\">Sunlight Foundation</a> we decided to spend a little time thinking about these challenges and came up with a short design exercise that will hopefully get people thinking in that direction.</p>\r\n\r\n<h3>The Old</h3>\r\n\r\n<p>First of all, let\u2019s visit the current usa.gov site to see what\u2019s been done well and what misses the mark completely. Starting at the top of the page, the header is too cluttered. The logo doesn\u2019t have enough space around it which makes it hard for a users eyes to focus there first, and the design itself is dated. I do agree with having the search box as the main source of navigation at the top of the page because there is so much content on usa.gov. However, I think the search options you are given\u2013government web, images, news, maps, and usa.gov\u2013could be eliminated by having a better search results page and a more prominent news section on the home page.</p>\r\n\r\n<img src=\"http://assets.sunlightlabs.com/site/images/usaGov_header.jpg\" alt=\"header on usa.gov\"/>\r\n\r\n<p>Next is the navigation below the header. It\u2019s kind of nice to be able to navigate based on audience, but at the same time who really thinks of himself as an audience? With people changing all the time wouldn\u2019t you want your government site to change with you instead of pigeon-holing you?</p>\r\n\r\n<img src=\"http://assets.sunlightlabs.com/site/images/usaGov_navBar.jpg\" alt=\"navigation on usa.gov\"/>\r\n\r\n<p>Finally, the main content of the site just seems generally random. To get to the content you\u2019re after, it involves many clicks and there is little to draw the user\u2019s eye to the most important content on a page.</p>\r\n\r\n<img src=\"http://assets.sunlightlabs.com/site/images/usaGov_content.jpg\" alt=\"content on usa.gov\"/>\r\n\r\n<h3>The New</h3>\r\n\r\n<p>In thinking about a new structure and new design for usa.gov we had a few goals that we wanted to accomplish. They basically were: letting the user customize and personalize the content that was displayed on the site, having better structure and navigation, and just having a cleaner, more powerful overall look to the site.</p>\r\n\r\n<p>I think a great way to make the government more accessible to the public is to ensure content is relevant to each person by allowing them to customize usa.gov. With a simple login, users could save information that was relevant to them instead of painfully sifting though links and then having to do it all over again when they might want the same content down the road.</p>\r\n\r\n<p>Taking that idea a step further would be pulling content from other government websites. For example, if you have student loans and what to see your balance, or if you just need to know where you\u2019re currently registered to vote, wouldn\u2019t it be nice to see all of that content in one place? There is lots of data out there like this\u2013medicare, social security, where your local post office is or even what kind of expenditures have been made by the federal government\u2013that when opened up, would allow the Government to serve the public better and give the user a better experience.</p>\r\n\r\n<p>For navigation, I again placed the search box front and center, and then decided on three small tabs: my government (which would have your personal government information), my community (just want it sounds like: federal community events, school data and where federal money is being spent in your area), and my elected officials (who your federal elected officials are, how to contact them, where they get their money, etc.). I think breaking things out this way makes the user feel a sense of ownership (their cognitive ownership bias coming into play) which will encourage them to come back more frequently and invest time customizing their window into the government.</p>\r\n\r\n<p>Finally, I needed to organize and declutter the interface. I went with a clean look using neutral colors and pulled cues from change.gov for some continuity between government sites. This was accomplished by adding more white space, using the more primary colors (blue and red) when I needed to call attention to the content, and visualizing data so that users would be able to better understand it.</p>\r\n\r\n<h3>The Reveal</h3>\r\n\r\n<p>Click on the smaller images below to view the full comps.</p>\r\n\r\n<a href=\"http://sunlightlabs.com/images/usa_gov_mygovFinal.jpg\"><img src=\"http://assets.sunlightlabs.com/site/images/usaGov_comp1.jpg\" alt=\"content on usa.gov\"/></a>\r\n\r\n<a href=\"http://sunlightlabs.com/images/usa_communityFinal.jpg\"><img src=\"http://assets.sunlightlabs.com/site/images/usaGov_comp2.jpg\" alt=\"content on usa.gov\"/></a>\r\n\r\n<a href=\"http://sunlightlabs.com/images/usa_gov_myrepsFinal.jpg\"><img src=\"http://assets.sunlightlabs.com/site/images/usaGov_comp3.jpg\" alt=\"content on usa.gov\"/></a>\r\n\r\n<h3>Conclusion</h3>\r\n\r\n<p>Of course there were many challenges in doing this. For example, any personalization of a site would clearly have security and privacy concerns, especially with the government law concerning persistent cookies. But would leaving vital information like account numbers, social security numbers, addresses and full names help mitigate this concern? And technically all this data could be pulled instantly, so the combination of this data would only exist inside of a session. So with that in mind this isn\u2019t necessarily something that the government should immediately take and implement, but will hopefully be a conversation starter about the possibilities that are out there for an overall better face to the government.</p>\r\n", "date_published": "2009-01-23 14:10:12", "comment_count": 20, "slug": "rethinking_usagov", "tags": "redesigning, design, usa.gov, redesigning the government"}}, {"pk": 46, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-23 09:39:42", "author": 6, "timestamp": "2009-01-23 09:39:42", "markup": "markdown", "title": "Does your Rep have an RSS Feed?", "excerpt": "Sunlight Labs contributor \"wubbahead\" comes up with an ingenious and automated way to find out whether or not your representative has an RSS feed by using the [Sunlight Labs API](http://services.sunlightlabs.com/api) the [Google AJAX Feed API](http://code.google.com/apis/ajaxfeeds/documentation/reference.html#lookupFeed) and some JavaScript. [Make sure to check it out](http://wubbahed.com/2009/01/22/does-your-us-representative-have-an-rss-feed/)", "content": "Sunlight Labs contributor \"wubbahead\" comes up with an ingenious and automated way to find out whether or not your representative has an RSS feed by using the [Sunlight Labs API](http://services.sunlightlabs.com/api) the [Google AJAX Feed API](http://code.google.com/apis/ajaxfeeds/documentation/reference.html#lookupFeed) and some JavaScript. [Make sure to check it out](http://wubbahed.com/2009/01/22/does-your-us-representative-have-an-rss-feed/)", "date_published": "2009-01-23 09:39:35", "comment_count": 1, "slug": "does-your-rep-have-rss-feed", "tags": ""}}, {"pk": 45, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-21 18:08:34", "author": 6, "timestamp": "2009-01-21 18:08:34", "markup": "markdown", "title": "What we're doing with the stimulus bill", "excerpt": "Short answer: we're trying to do some interesting things with it and we may need your help. Originally, we thought \"hey, let's put this into [Public Markup](http://publicmarkup.org)\" but unfortunately the bill's complexity actually was incompatible with Public Markup's data model. At the end of the day, the relevant parts of the bill wouldn't have fit into the commenting/displaying architecture we've used for bills in the past. \r\n", "content": "Short answer: we're trying to do some interesting things with it and we may need your help. Originally, we thought \"hey, let's put this into [Public Markup](http://publicmarkup.org)\" but unfortunately the bill's complexity actually was incompatible with Public Markup's data model. At the end of the day, the relevant parts of the bill wouldn't have fit into the commenting/displaying architecture we've used for bills in the past. \r\n\r\nSo we started on a different route, of getting the bill into a different data model so that people could comment on that. We then experimented with services like [disqus](http://disqus.com) for commenting so that we could get it out the door quickly. But during the design phase, we really struggled to find a good, usable way to comment. Then the question came up: why are we doing this? How does this change things? Allowing people to comment on this bill, section by section didn't seem up our alley or particularly revolutionary in terms of resource expenditures. So we got thinking: what if we just came up with a way to easily visualize the spending in the bill. That would be useful.\r\n\r\nSo Jeremy and I started parsing through the bill both with digital parsers and manually and managed to build a spreadsheet of all of the expenditures we could find. But the thing is-- we only were able to come up with $356,421,500,000 worth of stimulus. This is a far reach from 60% of \"875 billion dollars\" (the other 40% are tax cuts). We started getting worried that maybe our non-lawyery minds weren't able to find all the expenditures. This led to a chain of events where we said \"hey, you know what, if we can't be accurate, we'd better not go any further,\" at least officially as the Sunlight Foundation.\r\n\r\n\r\nIt seems Congress should not only embrace XML to show bills to the public but it should also create new ways for Congress to express that information so that machines (and people) can provide context to them. \r\n\r\nSo we're sort of at a loss for what to do next. We got the data into a [spreadsheet](http://spreadsheets.google.com/pub?key=p-kX0k_KXgtwwzHnvEn4eCA) and verified it against the bill text three times. With two sets of eyes. We're not saying we are perfect, but that's been checked over a few times. But where's the rest of the money? Can you help us find it? What have our non-lawyerly minds missed? [Here's the bill](http://appropriations.house.gov/pdf/RecoveryBill01-15-09.pdf)\r\n\r\n\r\n\r\n\r\n", "date_published": "2009-01-21 18:08:24", "comment_count": 3, "slug": "what-were-doing-stimulus-bill", "tags": ""}}, {"pk": 44, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-19 14:25:54", "author": 6, "timestamp": "2009-01-19 14:25:54", "markup": "markdown", "title": "Visualizing the Citizen's Briefing Book", "excerpt": "The Citizen's Briefing Book is an interesting little participatory function on change.gov, too. You can get a good read for what people are concerned about by looking at the number of ideas per category.\r\n\r\nJust a simple little ditty thanks to IBM's [ManyEyes](http://manyeyes.alphaworks.ibm.com/manyeyes/visualizations/idea-submissions-on-changegov).\r\n\r\n<img src=\"http://dl.getdropbox.com/u/36193/ManyEyes%20Change.gov%20Briefing%20Book.jpg\">", "content": "The Citizen's Briefing Book is an interesting little participatory function on change.gov, too. You can get a good read for what people are concerned about by looking at the number of ideas per category.\r\n\r\nJust a simple little ditty thanks to IBM's [ManyEyes](http://manyeyes.alphaworks.ibm.com/manyeyes/visualizations/idea-submissions-on-changegov).\r\n\r\n<img src=\"http://dl.getdropbox.com/u/36193/ManyEyes%20Change.gov%20Briefing%20Book.jpg\">", "date_published": "2009-01-19 14:27:16", "comment_count": 0, "slug": "visualizing-citizens-briefing-book", "tags": ""}}, {"pk": 42, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-19 00:42:01", "author": 3, "timestamp": "2009-01-19 00:42:01", "markup": "none", "title": "Weekly Lab Report 2009.01", "excerpt": "So much happens with Transparency Technology these days, it's a good time to start a Weekly Lab Report. Here's what happened last week at Sunlight Labs.\r\n<p><br />\r\n<a href=\"http://sunlightlabs.com/appsforamerica/\"><b>Labs launches Application Programmers <i>Incentive</i>.</b></a> WIN $15,000! WIN $15,000!  Make something useful\u2014or at least interesting\u2014with APIs from Sunlight for our Apps for America contest we officially announced this week.\r\n<p>\r\n<a href=\"http://fec.gov/law/policy/enforcement/2009/comments/comments.shtml\"><b>Clay comments old school at FEC (aka, testifies).</b></a> Who said developers are anti-social? Head Labs geek Clay Johnson testifies before FEC  commissioners. <a href=\"http://www.fec.gov/law/policy/enforcement/2009/comments/comm28.pdf\">Read Sunlight's filed comments.</a>\r\n\r\n", "content": "So much happens with Transparency Technology these days, it's a good time to start a Weekly Lab Report. Here's what happened last week at Sunlight Labs.\r\n<p><br />\r\n<a href=\"http://sunlightlabs.com/appsforamerica/\"><b>Labs launches Application Programmers <i>Incentive</i>.</b></a> WIN $15,000! WIN $15,000!  Make something useful\u2014or at least interesting\u2014with APIs from Sunlight for our Apps for America contest we officially announced this week.\r\n<p>\r\n<a href=\"http://fec.gov/law/policy/enforcement/2009/comments/comments.shtml\"><b>Clay comments old school at FEC (aka, testifies).</b></a> Who said developers are anti-social? Head Labs geek Clay Johnson testifies before FEC  commissioners. <a href=\"http://www.fec.gov/law/policy/enforcement/2009/comments/comm28.pdf\">Read Sunlight's filed comments.</a>\r\n</p>\r\n<p>\r\n<a href=\"http://www.onthemedia.org/transcripts/2009/01/16/07\"><b>On the Media puts Greg on the air.</b></a> Evangelist Greg Elin interviewed by NPR's <i>On The Media</i>.  Explains in  six minutes the significance of <a href=\"http://www.usaspending.gov/apidoc.php\">USASpending.gov's API to federal contracts</a>. \r\n</p>\r\n<p>\r\n<a href=\"http://infosthetics.com/archives/2009/01/capitol_words_word_frequency_from_the_congressional_record.html\"><b>A Picture <i>from</i> a Thousand Words.</b></a> Eye candy blog Infosthetics.com gave some love to the Lab's <a href=\"http://capitalwords.com/\">CapitalWords</a> project. \r\n</p>\r\n<p>\r\n<b>Pito researching an API cheat-sheet.</b> New Labs contributor and <a href=\"http://en.wikipedia.org/wiki/Pito_Salas\">Pivot Table creator Pito Salas</a> discussed creating a guide/cheat-sheet to linking the various API's in the transparency space with Labs API-wranger James Turk and Greg.\r\n</p\r\n</p>\r\n<p><b>Sunlightlabs.com web traffic briefly eclipsed SunlightFoundation.com.</b> It may only be temporary, but we took some pride in outshining the mothership.\r\n</p>\r\n<p>\r\n<p><b>Great Threads:</b> <a href=\"http://groups.google.com/group/open-government/browse_thread/thread/e0b47441d5cc110b\">Open Government thread on opening govt science data</a> &bull; <a href=\"http://groups.google.com/group/openhouseproject/browse_thread/thread/5a6ca78f7ef4b48a\">Congressional YouTube Channel risks and opportunities</a> \r\n</p>\r\n<p>\r\n<b>Labs Tweet of the Week:</b> @jamesturk tweeted:  <i>@bitb ain't no portable lavatory like a North Carolina portable lavatory\"</i>\r\n ", "date_published": "2009-01-18 18:40:20", "comment_count": 0, "slug": "weekly-lab-report-200901", "tags": ""}}, {"pk": 39, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-14 13:06:16", "author": 3, "timestamp": "2009-01-14 13:06:16", "markup": "none", "title": "Mapping Government Information Flows", "excerpt": "We could take some inspiration from this video of scientists pouring very liquid concrete into an ant hill in order to preserve its structure for study. What could we \"pour\" into the government in order to create a representation of the structure of bureaucracy and information flows inside?", "content": "<p>We could take some inspiration from this video of scientists pouring liquid concrete into an ant hill in order to preserve its structure for study. What could we \"pour\" into the government in order to create a representation of the structure of bureaucracy and information flows inside? If doctors use radioactive dye to trace blood flow in the body, shouldn't we be thinking of some simple ways to tag data going into the system and learn about how data (and dollars) flow? Shouldn't this be possible for the TARP, Stimulus Package, and other government spending?</p>\r\n\r\n<object width=\"318\" height=\"258\"><param name=\"movie\" value=\"http://www.youtube.com/v/ozkBd2p2piU&hl=en&fs=1\"></param><param name=\"allowFullScreen\" value=\"true\"></param><param name=\"allowscriptaccess\" value=\"always\"></param><embed src=\"http://www.youtube.com/v/ozkBd2p2piU&hl=en&fs=1\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\"318\" height=\"258\"></embed></object>", "date_published": "2009-01-16 17:46:17", "comment_count": 0, "slug": "mapping-government-information-flows", "tags": ""}}, {"pk": 41, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-15 15:34:27", "author": 6, "timestamp": "2009-01-15 15:34:27", "markup": "markdown", "title": "Change.gov Popular Words", "excerpt": "Following up on yesterday's post where [Opened up Change.gov](http://sunlightlabs.com/blog/2009/01/14/opening-your-seat-table/) I just took the titles of all the documents and ran them through [Wordle](http://wordle.net), removing some of the blatantly noisy words (Recommendation, Transition, Policy). Here's what we got, which may be a pretty nice way of seeing what people \"at the table\" are talking about.\r\n\r\n![change.gov popular words](http://dl-client.getdropbox.com/u/36193/Wordle%20-%20Change.gov%20Recommendation%20Titles-small.jpg \"Change.gov popular words\") \r\n\r\n[Here's the big version](http://www.wordle.net/gallery/wrdl/441539/Change.gov_Recommendation_Titles)", "content": "Following up on yesterday's post where we [Opened up Change.gov](http://sunlightlabs.com/blog/2009/01/14/opening-your-seat-table/) I just took the titles of all the documents and ran them through [Wordle](http://wordle.net), removing some of the blatantly noisy words (Recommendation, Transition, Policy). Here's what we got, which may be a pretty nice way of seeing what people \"at the table\" are talking about.\r\n\r\n![change.gov popular words](http://dl-client.getdropbox.com/u/36193/Wordle%20-%20Change.gov%20Recommendation%20Titles-small.jpg \"Change.gov popular words\")\u00a0\r\n\r\n[Here's the big version](http://www.wordle.net/gallery/wrdl/441539/Change.gov_Recommendation_Titles)", "date_published": "2009-01-15 15:34:23", "comment_count": 0, "slug": "changegov-popular-words", "tags": ""}}, {"pk": 40, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-14 14:53:10", "author": 6, "timestamp": "2009-01-14 14:53:10", "markup": "markdown", "title": "Opening Your Seat at the Table", "excerpt": "After my [post](http://sunlightlabs.com/blog/2009/01/12/learning_lessons_from_changegov/) yesterday about [Change.gov](http://change.gov)'s [Your Seat at the Table](http://change.gov/open_government/yourseatatthetable) feature, it got us thinking: what if this website disappears on January 21st? What if all this data goes away?\r\n\r\nI posted (half seriously) on our Yammer account about 3 hours ago \"Big gold star to anyone who can scrape and capture every 'your seat at the table' document in a Sunlight repository. I'm getting nervous that change.gov is going to disappear in a week.\"\r\n\r\nJames and Jeremy independently took up the challenge. And now, three hours later we have our repository. We thought we'd share the [code](http://github.com/sunlightlabs/sunlight-scraps/) for you to do it too if you'd like, and also this [handy csv file](http://dl-client.getdropbox.com/u/36193/yourseat.csv) of all of the documents.", "content": "After my [post](http://sunlightlabs.com/blog/2009/01/12/learning_lessons_from_changegov/) yesterday about [Change.gov](http://change.gov)'s [Your Seat at the Table](http://change.gov/open_government/yourseatatthetable) feature, it got us thinking: what if that website disappears on January 21st? What if all that information goes away?\r\n\r\nI posted (half seriously) on our Yammer account about 3 hours ago \"Big gold star to anyone who can scrape and capture every 'your seat at the table' document in a Sunlight repository. I'm getting nervous that change.gov is going to disappear in a week.\"\r\n\r\nJames and Jeremy independently took up the challenge. And now, three hours later we have our repository. We thought we'd share the [code](http://github.com/sunlightlabs/sunlight-scraps/) for you to do it too if you'd like, and also this [handy csv file](http://dl-client.getdropbox.com/u/36193/yourseat.csv) of all of the documents. Looks like there's some interesting stuff in here. One can probably take this CSV file, put it in a database and start getting some context for the transition. It could answer \"What are common themes the transition team is meeting about\" and \"What are the most popular words in documents.\" Or even simply \"which organization has provided the most documentation to the Transition Team.\"\r\n\r\nAnyhow, please enjoy and use wisely. You can use this to create your own archive of the \"Your Seat at the Table,\" as we have. This isn't an official data-dump and won't be considered in the [contest](/appsforamerica) and we won't support the source as it is fairly specific to change.gov. But feel free to play as you see fit!\r\n\r\n\r\n\r\n", "date_published": "2009-01-14 15:04:22", "comment_count": 2, "slug": "opening-your-seat-table", "tags": ""}}, {"pk": 38, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-12 13:36:27", "author": 6, "timestamp": "2009-01-12 13:36:27", "markup": "markdown", "title": "Learning Lessons from Change.gov", "excerpt": "Early on in the Presidential Transition, [Change.gov](http://change.gov announced an incredibly compelling), never-before-done process: [Your Seat at the Table](http://change.gov/open_government/yourseatatthetable). They announced that every document that the transition team received in a meeting where there was three or more attendees would be posted online. By anybody's standards -- much less a presidential transition this was an *awesome* step and the Change.gov team should be commended for taking it.\r\n\r\nThat said Change.gov team is learning as they go and looking at the implementations on Change.gov is an interesting opportunity to get some new transparency technology learning opportunities for the new administration. ", "content": "Early on in the Presidential Transition, [Change.gov](http://change.gov) announced an incredibly compelling, never-before-done process: [Your Seat at the Table](http://change.gov/open_government/yourseatatthetable). They announced that every document that the transition team received in a meeting where there was three or more attendees would be posted online. By anybody's standards -- much less a presidential transition this was an *awesome* step and the Change.gov team should be commended for taking it.\r\n\r\nThat said Change.gov team is learning as they go and looking at the implementations on Change.gov is an interesting opportunity to get some new transparency technology learning opportunities for the new administration. \r\n\r\nMy two takeaways:\r\n\r\n1. Government should be focusing *first* on providing access to data in as unrestricted of a way as possible and inviting developers to build interfaces on top of it, and\r\n2. Often times it is possible in this field to try too hard and to do too much work. A simple solution is sometimes a superior solution.\r\n\r\nHere's why-- as of today on the \"Your Seat at the Table\" page: \r\n\r\n1. Several of the documents on the front page have dates in the future. \r\n2. You can only search once every 15 seconds. \r\n3. It looks like it just got caught up, but for several weeks (from Dec. 22 to about January 9th) the site was nearly dormant and had weeks of missing data.\r\n4. There's not a clear message of what's going to happen to the data after January 20th.\r\n\r\nI'm *certain* that there's a strong amount of effort being put on this by the administration and they're taking it very seriously, but I wonder if they can really keep up with the documents they're getting. Maybe there's another way to do it.\r\n\r\nHere's the simpler approach. Maybe they should start by standardizing the names of the documents, and put them on an FTP server or an index-free http page; updating that FTP server when documents come in and are sanitized; writing one blog post pointing to the repository. In the blog post, say \"people are encouraged to build their own search interfaces and browsing interfaces to the data as they see fit\" \r\n\r\nThe result? You'll likely have the Sunlight Labs [community](http://sunlightlabs.com) scrambling to build an interface, [Watchdog.net](http://watchdog.net) parsing the data and incorporating it, and media organizations like the New York Times working on their own too. More eyeballs would be on the data, and they'd earn praise from the media and technology community for being participatory and more open.\r\n\r\nLet me add one big caveat here: This effort by the Transition team is a special case. We know they have been moving at the government equivalent of light speed, and in a situation where they had no infrastructure to work with. What I suggest above would have the effect of deputizing outside help for an urgent and atypical problem. I'm not sure approach whether what I suggest above should be a general model for such work (though I am inclined that way), but it would have been away to do it better in this instance. \r\n\r\nThat being said, if you take a look at the  [data](http://siteanalytics.compete.com/opensecrets.org+fec.gov/?metric=uv) you'll see a trend-- when the data can be taken from the Government, people do more interesting things with it and it gets in front of more eyeballs. The Government should always be keeping a keen eye towards *first* making the data available to citizens, and *then* building tools and interfaces on top of it.\r\n\r\nThe point is this: This transparency business gets really tough if you make it tough, but if you're looking for the easy way out, there's likely a much simpler solution. And often the simpler ones end up being more sophisticated and impressive anyhow. Often times, the hardest part of being transparent and open is doing too much, rather than not enough.", "date_published": "2009-01-12 13:35:27", "comment_count": 1, "slug": "learning_lessons_from_changegov", "tags": ""}}, {"pk": 37, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-10 18:15:30", "author": 6, "timestamp": "2009-01-10 18:15:30", "markup": "markdown", "title": "How You Can Help", "excerpt": "We've added a significant page to the wiki: [\"How You Can Help\"](http://wiki.sunlightlabs.com/index.php?title=How_you_can_help)\r\n\r\nIf you are wondering how you can use your skills to help make our Government more transparent, this web page is for you. It talks through how developers, designers and activists can be a part of the Sunlight Labs community and lend a hand to our efforts. Make sure to check it out.\r\n\r\n", "content": "We've added a significant page to the wiki: [\"How You Can Help\"](http://wiki.sunlightlabs.com/index.php?title=How_you_can_help)\r\n\r\nIf you are wondering how you can use your skills to help make our Government more transparent, this web page is for you. It talks through how developers, designers and activists can be a part of the Sunlight Labs community and lend a hand to our efforts. Make sure to check it out.\r\n\r\n", "date_published": "2009-01-10 21:33:21", "comment_count": 0, "slug": "how_you_can_help", "tags": ""}}, {"pk": 34, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-08 18:06:51", "author": 5, "timestamp": "2009-01-08 18:06:51", "markup": "restructuredtext", "title": "Looking At The NYTimes Congress API", "excerpt": "Earlier this week the New York Times released their `Congress API`_ second politics API (following up on the release of their `Campaign Finance API`_ late last year).  Here at Sunlight Labs we are always happy to see new APIs that wrap government data and there is definitely a lot to like here, although there are some things that will hopefully change to make the API more useful to the community at large.\r\n\r\n\r\n.. _`Congress API`: http://open.blogs.nytimes.com/2009/01/08/introducing-the-congress-api/\r\n.. _`Campaign Finance API`: http://developer.nytimes.com/docs/campaign_finance_api\r\n", "content": "Earlier this week the New York Times released their `Congress API`_ second politics API (following up on the release of their `Campaign Finance API`_ late last year).  Here at Sunlight Labs we are always happy to see new APIs that wrap government data and there is definitely a lot to like here, although there are some things that will hopefully change to make the API more useful to the community at large.\r\n\r\n\r\nThe Good\r\n========\r\n\r\nThere can never be too many ways to access this kind of government data, the more that are out there, the more they will be used, and ultimately that is one of our primary goals here at Sunlight Labs.  There are a few really nice design decisions that are worth noting.\r\n\r\n* The API includes methods not only for details on Members of Congress but is more focused on their roll call votes over time.\r\n* The data goes back as far as any readily available sources go, this is a nice touch that makes it easier on people looking to do historical analysis of the behavior of Congress. (Data goes back to at least 1992 but as early as 1947 for Senate biographical data)\r\n* The `RESTful`_ API is easy to use and experiment with from a browser.  The four methods are easy to understand and don't require jumping through hoops.  They also use standard `HTTP error codes`_.\r\n* They have chosen to use Bioguide IDs as their primary keys, which means that they haven't introduced yet another identifier for members of congress. [*]_\r\n* The data returned is reasonably minimal throughout most of the API, you get back pretty much what you ask for without a ton of irrelevant information to sort through.\r\n\r\nThe Bad\r\n=======\r\n\r\nAs nice as the new API is from a technical perspective, there are a few problems that a would-be user might have.  These problems primarily stem from the `Terms of Use`_\r\n\r\n* 5000 requests per day limit seems a bit restrictive.  (it is prominently noted that this is subject to change, but no indication whether this change would likely be in the upwards or downwards direction)\r\n* Section 1.e.vi of the ToS states that you may not use the API for any service that competes with products or services offered by NYT.  This comes across as overly broad as one can imagine that a lot of this vote information would be useful for organizations that can in some way be considered 'in competition with the NY Times.' (such as local news organizations).\r\n* Section v of the attribution restrictions prohibits archiving of data for access by users \"at any future date after you have finished using the service\" which would prohibit something like building an application that used the NY Times as an initial source for votes but had no need to hit the Congress API regularly.\r\n\r\nThe Ugly\r\n========\r\n\r\nOk, not much is really ugly about this API, as I said it is quite simple and elegant.  At the time of this post however output is XML only and let's face it, XML is ugly.  (I have hopes that this will change as the Campaign Finance API is JSON, XML, or Serialized PHP with JSON by default.)\r\n\r\nThe New York Times should be applauded for their effort in creating both this and the Campaign Finance API, hopefully the future holds more of this from them and we look forward to seeing what they release next.  I hope that some of the more troublesome provisions can be revised or clarified as to make it beneficial to a wider audience.\r\n\r\n.. _RESTful: http://en.wikipedia.org/wiki/Representational_State_Transfer\r\n.. _`Congress API`: http://open.blogs.nytimes.com/2009/01/08/introducing-the-congress-api/\r\n.. _`Campaign Finance API`: http://developer.nytimes.com/docs/campaign_finance_api\r\n.. _`HTTP error codes`: http://developer.nytimes.com/docs/reference/errors\r\n.. _`Terms of Use`: http://developer.nytimes.com/Api_terms_of_use\r\n\r\n.. [*]  As the maintainer of the `Sunlight Labs API <http://services.sunlightlabs.com/api/>`_ (primarily focused on providing ID lookups for legislators) I cannot emphasize how much I appreciate new websites and services using one of the standard ids.\r\n", "date_published": "2009-01-08 18:05:09", "comment_count": 6, "slug": "nytimes-congress-api", "tags": ""}}, {"pk": 32, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-18 13:09:07", "author": 6, "timestamp": "2008-12-18 13:09:07", "markup": "markdown", "title": "Welcome to Sunlight Labs 2.0", "excerpt": "You may have noticed, we've redesigned our site. Our awesome new designer, Ali made it and we're excited to have her as part of our team. And we're excited about the new website too. Especially those things on the right that allow us to update what we're working on via [twitter](http://twitter.com).\r\n\r\nThe website isn't the only thing we're redesigning. We're also redesigning how Sunlight Labs works. We're clearly no longer the six-month pilot project we were chartered to be [31 months ago](http://blog.sunlightfoundation.com/2006/05/10/launching-sunlight-labs/). We're now a team of great developers using technology to change the way our Congress operates and have been for quite some time. So we're long overdue for a gear-shift in the way we think about Sunlight Labs and how we work. We see three fundamental shifts in how we think about ourselves now vs. how the Labs was conceived.", "content": "You may have noticed, we've redesigned our site. Our awesome new designer, Ali Felski, made it and we're excited to have her as part of our team. And we're excited about the new website too. Especially those post-its on the right that allow us to update what we're working on via [twitter](http://twitter.com).\r\n\r\nThe website isn't the only thing we're redesigning. We're also redesigning how Sunlight Labs works. We're clearly no longer the six-month pilot project we were chartered to be [31 months ago](http://blog.sunlightfoundation.com/2006/05/10/launching-sunlight-labs/). We're now a team of great developers using technology and the social web to open up the way our government in Washington operates and have been for quite some time. So we're long overdue for a gear-shift in the way we think about Sunlight Labs and how we work. There are three fundamental shifts in how we think about ourselves now vs. how the Labs was conceived.\r\n\r\nThe first shift is thinking of ourselves as a *permanent* institution, and that we are more than a \"mash-up lab.\" We now develop long-standing projects like [Capitol Words](http://www.capitolwords.org) and people depend on many of our [APIs](http://sunlightlabs.com/projects/) to run their websites. We think that's pretty cool. And we're just beginning. We have two more very large projects in the wings: The Sunlight Data Commons, and the [Pew Subsidy Project](http://subsidyscope.com).\r\n\r\nThe second shift is understanding that much of the software we write is useful to society and should be *open* so that anyone can use it to further government accountability and transparency. So, we're now publishing all of our source under various open source licenses, and as much of our data as bandwidth allows as CC-BY. We're beginning to make that transition [now](/projects/), and all new projects we create will be turned into Open Source projects. And to the best of our ability, all of the data our software produces will also be available in that form.\r\n\r\nThird, we now realize that many of the problems we face cannot be solved through traditional business methods. Whether it is [Name Standardization](http://wiki.sunlightlabs.com/index.php?title=Name_Standardization) or just finding new ways to [take in data](http://wiki.sunlightlabs.com/index.php?title=Data_Intake) there's a lot of work to be done, and it is probably too much for a small team of eight to do. We need to, instead, begin building a vessel that will enable outside developers to also enjoy the privilege of changing how their government works (and vice versa, how people work with their government). In doing so, we are inspired by our heroes at [Apache](http://www.apache.org) and [Mozilla](http://www.mozilla.org) Foundation, who have blazed a path for open source developers worldwide.\r\n\r\n_In short: Sunlight Labs is an open source development team dedicated to making their government accountable._ And we want you to be a part of it. \r\n\r\nNow, to that end, I'd like to announce that with this redesign of the Labs, and a redesign of the Executive Branch of Government, Sunlight is launching its Apps for America contest. The award will be given to the best applications that use our community's data, tools and APIs to make their government good. [Check it out](/contest) and help us build some apps that shed light on our new Congress.\r\n\r\nWe're also opening up our first of hopefully many [email lists](http://groups.google.com/group/sunlightlabs) so that we can begin setting a course for new open source projects, and getting your help on our existing ones. And we'll need your ideas and proposals for new open source projects as well. \r\n\r\nWe hope you're as excited as we are by this new chapter of the Sunlight Labs story. \r\n\r\nOnward!\r\n\r\n--Clay\r\n", "date_published": "2008-12-19 10:10:28", "comment_count": 2, "slug": "labs2", "tags": ""}}, {"pk": 33, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-18 17:00:38", "author": 5, "timestamp": "2008-12-18 17:00:38", "markup": "restructuredtext", "title": " Feed A Need: Donate Time to Sunlight for the Holidays", "excerpt": "reddit.com_ recently unveiled FeedANeed.org_, an charity drive designed to connect volunteers with worthy nonprofits in need of some form of assistance from the community.\r\n\r\nFeedANeed allows anyone to vote for worthy nonprofits and/or sign up to volunteer some chunk of their time to top vote-getting organizations.  Volunteers that complete their pledged time by February 14th, 2009 will be entered into a drawing for prizes from various sponsors (from a wide variety of organizations Wired_, EFF_, Baconfreak_, and even `Sunlight Foundation`_).\r\n\r\n.. _reddit.com: http://reddit.com\r\n.. _FeedANeed.org: http://FeedANeed.org\r\n.. _Wired: http://wired.com\r\n.. _EFF: http://eff.org\r\n.. _Baconfreak: http://baconfreak.com\r\n.. _FeedANeed: http://wired.reddit.com/feedaneed/?s=top\r\n.. _Sunlight Foundation: http://sunlightfoundation.com", "content": "reddit.com_ recently unveiled FeedANeed.org_, an charity drive designed to connect volunteers with worthy nonprofits in need of some form of assistance from the community.\r\n\r\nFeedANeed allows anyone to vote for worthy nonprofits and/or sign up to volunteer some chunk of their time to top vote-getting organizations.  Volunteers that complete their pledged time by February 14th, 2009 will be entered into a drawing for prizes from various sponsors (from a wide variety of organizations Wired_, EFF_, Baconfreak_, and even `Sunlight Foundation`_).\r\n\r\nThe reddit community is somewhat technical so there is a place to list design or development skills, but they are certainly not a requirement.  You can indicate you are simply willing to do a couple hours of data entry or image recognition, something akin to Amazon's Mechanical Turk.  When we're talking about government data there is no shortage of material that would benefit from a team of volunteers being thrown at it.\r\n\r\n**Voting ends December 23rd @ 7pm Eastern - before the polls close please head over to FeedANeed_ and cast a vote for us (and your other favorite nonprofits).**\r\n\r\nAfter you vote for us please consider signing up to donate as little as two hours of your time to a worthy cause.  A lot of nonprofits are facing the prospect of reduced donations due to the current economic climate, and this is a great way to support your favorite causes without spending a dime.  Quite a few of us here at Sunlight will be returning the favor by volunteering a couple hours of our time elsewhere through FeedANeed as a way of saying thanks.\r\n\r\n\r\n.. _reddit.com: http://reddit.com\r\n.. _FeedANeed.org: http://FeedANeed.org\r\n.. _Wired: http://wired.com\r\n.. _EFF: http://eff.org\r\n.. _Baconfreak: http://baconfreak.com\r\n.. _FeedANeed: http://wired.reddit.com/feedaneed/?s=top\r\n.. _Sunlight Foundation: http://sunlightfoundation.com", "date_published": "2008-12-18 16:59:48", "comment_count": 0, "slug": "feed_a_need", "tags": ""}}, {"pk": 31, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 1, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "Under the hood of the DNC and RNC convention sites", "excerpt": "I took a few minutes this morning to look at the technology that powers the DNC and RNC convention web sites. It is always interesting to see what technological decisions different organizations take when they are trying to accomplish similar goals.", "content": "I took a few minutes this morning to look at the technology that powers the DNC and RNC convention web sites. It is always interesting to see what technological decisions different organizations take when they are trying to accomplish similar goals.\r\n\r\n#### Democratic National Convention\r\n\r\n<a href=\"http://demconvention.com/\">http://demconvention.com/</a>\r\n\r\nThe URL for the web site was registered with <a href=\"https://www.domaindiscover.com/\">Domain Discover</a>. The whois information lists the registrant as:\r\n\r\nDemocratic National Committee  \r\n430 S. Capitol St. S.E.  \r\nWashington, DC 20003  \r\n\r\nFor all of the domain squatters out there, the domain expires in November, so keep an eye out. My best guess is that the servers are hosted by <a href=\"http://www.verizonbusiness.com/\">Verizon Business</a>. <a href=\"http://www.servint.net/\">ServInt</a>, based in McLean, VA, provides the hosting for the name servers which are also used by dnc.org and democrats.org, among others.\r\n\r\nThe DNC convention web site is served from an <a href=\"http://httpd.apache.org/\">Apache</a> 2.0.52 web server running on <a href=\"http://www.redhat.com/\">Red Hat Linux</a>. <a href=\"http://www.silverstripe.com\">SilverStripe</a>, a PHP-based, open source content management system, is used to power the site. The main page of the site has a XHTML 1.0 Transitional doctype, but is served as text/html and is not valid XHTML. The site is laid out using divs to define logical elements within the page and CSS to position the elements.\r\n\r\nThe following are screenshots of the page with JavaScript disabled and with both JavaScript and CSS disabled.\r\n\r\n<a href=\"http://blog.sunlightlabs.com/wp-content/uploads/2008/09/dnc_noscript.png\"><img class=\"aligncenter size-thumbnail wp-image-48\" title=\"DNC, no script\" src=\"http://blog.sunlightlabs.com/wp-content/uploads/2008/09/dnc_noscript.png\" alt=\"DNC, no script\" width=\"150\" height=\"115\" /></a>\r\n\r\n<a href=\"http://blog.sunlightlabs.com/wp-content/uploads/2008/09/dnc_noscript_nostyle.png\"><img class=\"aligncenter size-thumbnail wp-image-49\" title=\"DNC, no script, no style\" src=\"http://blog.sunlightlabs.com/wp-content/uploads/2008/09/dnc_noscript_nostyle.png\" alt=\"DNC, no script, no style\" width=\"57\" height=\"150\" /></a>\r\n\r\n#### Republican National Convention\r\n\r\n<a href=\"http://gopconvention2008.com/\">http://gopconvention08.com/</a>\r\n\r\nThe RNC registered their convention's domain with <a href=\"http://www.godaddy.com\">GoDaddy</a>. The registrant is listed as:\r\n\r\nRoman Buhler  \r\n4056 41st Street North  \r\nMcClain [__sic__], Virginia 22101  \r\n\r\n<a href=\"http://www.opensecrets.org/indivs/search.php?name=Buhler&amp;state=VA&amp;zip=&amp;employ=&amp;cand=&amp;c2008=Y&amp;sort=N&amp;capcode=4425k&amp;submit=Submit\">Mr. Buhler</a> is the president of <a href=\"http://www.opensecrets.org/lobby/firmsum.php?lname=Roman+Buhler+%26+Assoc&amp;year=2008\">Roman Buhler &amp; Associates</a>, a lobbying firm based in McLean, VA. Mr. Buhler also <a href=\"http://www.legistorm.com/person/Roman_Buhler/48145.html\">served as counsel</a> of the House Administration Committee from 1989 to 2003. Hosting for the web servers and DNS is provided by <a href=\"http://www.smartechcorp.net/\">Smartech</a>, based in Chattanooga, TN.\r\n\r\nThe web site is served by <a href=\"http://www.iis.net/\">Microsoft IIS</a> 6.0 using <a href=\"http://microsoft.com/\">Microsoft's</a> <a href=\"http://www.asp.net/\">ASP.Net</a> language. The main page declares a generic XHTML namespace, but does not declare a doctype, is served as text/xml, and does not validate as XHTML. The site is laid out using HTML tables.\r\n\r\nThe following are screenshots of the page with JavaScript disabled and with both JavaScript and CSS disabled.\r\n\r\n<a href=\"http://blog.sunlightlabs.com/wp-content/uploads/2008/09/rnc_noscript.png\"><img class=\"aligncenter size-thumbnail wp-image-50\" title=\"RNC, no script\" src=\"http://blog.sunlightlabs.com/wp-content/uploads/2008/09/rnc_noscript.png\" alt=\"RNC, no script\" width=\"150\" height=\"134\" /></a>\r\n\r\n<a href=\"http://blog.sunlightlabs.com/wp-content/uploads/2008/09/rnc_noscript_nostyle.png\"><img class=\"aligncenter size-thumbnail wp-image-51\" title=\"RNC, no script, no style\" src=\"http://blog.sunlightlabs.com/wp-content/uploads/2008/09/rnc_noscript_nostyle.png\" alt=\"RNC, no script, no style\" width=\"150\" height=\"53\" /></a>", "date_published": "2008-09-02 20:20:40", "comment_count": 1, "slug": "under-the-hood-of-the-dnc-and-rnc-convention-sites", "tags": ""}}, {"pk": 30, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 6, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "Google Spreadsheet and the Sunlight Labs API", "excerpt": "James is finishing up a tweak to the Sunlight Labs API that allows for fairly sophisticated search for members of Congress, it isn't \"published\" yet but it is active so if you want to experiment you're welcome to try it out, but for now it is \"unofficial\".", "content": "James is finishing up a tweak to the Sunlight Labs API that allows for fairly sophisticated search for members of Congress, it isn't \"published\" yet but it is active so if you want to experiment you're welcome to try it out, but for now it is \"unofficial\".\r\n\r\nHere's the deal: We wanted a better way for people to search for members, as members of congress are often times referred to by different names-- think \"Ted Kennedy,\" \"Edward Kennedy,\" \"Teddy Kennedy\" etc. Whether it is nicknames or typos, it makes analyzing data difficult if names are not standardized.\r\n\r\nWe're not saying we've come up with a complete solution to name standardization or even congressional name standardization, but we've got a simple solution that might make some lives easier. To demonstrate, we'll use Google Spreadsheets. Follow along at home!\r\n\r\n__Step 1:__ Get an API key from Sunlight Labs [here](http://services.sunlightlabs.com/api/ \"Sunlight Labs API\").\r\n\r\n__Step 2:__ Create a Google Spreadsheet\r\n\r\n__Step 3:__ Name the columns of your spreadsheet \"Member\", \"Firstname\", \"Lastname\" so it looks like this:\r\n\r\n![](http://plusoneme.com/step1.jpg)\r\n\r\n__Step 4:__ Let's add some wacky mispellings and some semi-dirty data like below:\r\n\r\n![](http://plusoneme.com/step2.jpg)\r\n\r\n__Step 5:__ Here's where it gets fun. We'll use the importXML function in Google Spreadsheets to take the values of our dirty data and send them to the Sunlight Labs API, and get a firstname. Enter this code into your spreadsheet:\r\n\r\n    =importXML(\"http://services.sunlightlabs.com/api/legislators.search.xml?apikey=YOURAPIKEYHERE&amp;name=\"&amp;A2,\"//firstname\")\r\n\r\nSee below for an example:\r\n\r\n![](http://plusoneme.com/step3.jpg)\r\n\r\n__Step 6:__ Do the same for the last name column, but change your call to parse the lastname, like so:\r\n\r\n    =importXML(\"http://services.sunlightlabs.com/api/legislators.search.xml?apikey=YOURAPIKEYHERE&amp;name=\"&amp;A2,\"//lastname\")\r\n\r\n__Step 7:__ Finish it up! Select the first values of those newly processed columns and fill in the rest of the values like so:\r\n\r\n![](http://plusoneme.com/step4.jpg)\r\n\r\nNeat! Clean easy name cleanup in your spreadsheet!", "date_published": "2008-08-21 20:55:38", "comment_count": 0, "slug": "google-spreadsheet-and-the-sunlight-labs-api", "tags": ""}}, {"pk": 29, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 5, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "\"Cool project, what CMS did you guys use?\"", "excerpt": "Sunlight Labs is often asked \"What CMS do you use?\". James discusses our development philosophy and the drawbacks of CMS selection.", "content": "#### Pass223.com\r\n\r\nI recently worked on [Pass223.com](http://pass223.com \"Pass 223\"), a simple site that urges the Senate to pass a piece of legislation that requires the Senate to adhere to the same electronic financial disclosure rules in place for representatives and presidential candidates.\r\n\r\nPass223.com is similar to that of hundreds of related action sites: choose a legislator, call them, report results, repeat.\u00a0 I wrote the code, our esteemed creative director Kerry did the bulk of the design, and various others here at Sunlight helped to refine the concept and wording of the site and call script.\r\n\r\nIt was a bit surprising seeing how positive the [feedback](http://www.dailykos.com/story/2008/8/6/9501/66080/422/562998) has been for such a simple site.\u00a0 A number of people have been pointing to Pass223 as an example of how this type of thing should be done.  Most of that credit goes to the team that worked together to revise the awesomely straightforward script.\r\n\r\nThe other question that has come up is what content management system (CMS) Pass223 was done on and what legislative database it was built on top of.  This made me think about the other reason Pass223 was able to come together the way that it did, the tools used behind the scene.\r\n\r\n#### When all you have is a hammer...\r\n\r\nIt seems, especially in the nonprofit world where developers are sparse, that content management systems like [Drupal](http://drupal.org/ \"Drupal :(\") are considered the solution to every problem that arises.\u00a0 Because Content Management Systems can not possibly do everything that organizations want they are left with two options: attempt to mangle the CMS to do things it was never intended to do, or alternatively not get what they actually want.\u00a0 Because of the difficulty in dealing with the massive codebases of most CMSs, they often find themselves accepting both results.\u00a0 A great deal of time is therefore sunk into a project and in the end things still don't work quite how they were planned.\r\n\r\nThe supposed benefit of a CMS is the speed of deployment and ease of use, but as [Jeremy's recent post about LetOurCongressTweet](http://blog.sunlightlabs.com/2008/07/11/from-idea-to-production-in-six-hours/ \"LOCT post\") mentioned, we are able to rapidly create sites without the use of a CMS.\u00a0 And in reality, struggling to fit an innovative project or idea into the rigid structure of a CMS is not easy nor fast.\r\n\r\nA better use of the time and money spent maintaining and modifying complex CMS installations would be to spend that time learning and deploying sites in a framework such as [Django](http://djangoproject.com Django :)\") or [Ruby on Rails](http://www.rubyonrails.org/ \"Ruby on Rails\").\r\n\r\n#### Perfectionists with Deadlines\r\n\r\nDjango in particular was created to solve this problem.  Working with a bloated CMS forces you to make a decision between getting what you want and getting something fast, and more often than not you wind up with neither.  It is because of this that a team originally working at the [Lawrence Journal-World](http://www2.ljworld.com/) newspaper built Django to meet their needs as \"perfectionists with deadlines.\"\r\n\r\nFrameworks like Django provide all of the pieces of commonly used functionality, user registration, Object-Relational mappers to avoid dealing with the database directly, caching, and a ton more.  All of these pieces are given to you without any mandate that they must all be used, they are simply building blocks from which your particular project can choose to use or not.  A large site such as  [EarmarkWatch](http://earmarkwatch.org) may need complex user profiles, whereas it is possible to eschew all of the unneeded modules and build something as quick and simple as Pass223.com.\r\n\r\nUltimately, unless some CMS already provides exactly what you want, it is far easier to build a project from reusable components within a framework than to attempting to teach an old CMS a new trick.  One of the reasons that Pass223.com seems to impress people used to looking at the typical contact-your-legislator forms is because we had the flexibility to build what we wanted.", "date_published": "2008-08-07 18:04:44", "comment_count": 4, "slug": "cool-project-what-cms-did-you-guys-use", "tags": "django"}}, {"pk": 28, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 1, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "From Idea to Production in Six Hours", "excerpt": "We recently decided to launch a petition-like site that uses Twitter as the organizing method; using one of the very technologies that are impacted by Congressional Web use restrictions. We knew this had to be timely to have an impact, so the decision was made to have the Web site completed by the end of the day. That gave Kerry Mitchell, our fearless Creative Director, and I about six hours to get the site completed.", "content": "There has been some [commotion](http://blog.sunlightfoundation.com/2008/07/10/let-our-congress-tweet/) over the past few days regarding Congresstional Web use restrictions. The rules are inadequate for the current state of the Web and must be rewritten to reflect changes in technology. Republicans and Democrats have been going back-and-forth over proposed changes to the existing rules; one side claiming the other is trying to stifle their communication. While they keep on bickering, we wanted to raise awareness of these Web use restrictions and get people involved.\r\n\r\nWe decided to launch a petition-like site that uses Twitter as the organizing method; using one of the very technologies that are impacted by Congressional Web use restrictions. We knew this had to be timely to have an impact, so the decision was made to have the Web site completed by the end of the day. That gave Kerry Mitchell, our fearless Creative Director, and I about six hours to get the site completed.\r\n\r\nSo how did it go?\r\n\r\n#### Twitter\r\n\r\nAs LOCT is a petition-like site, it is important to get a list of the people that are following the LOCT Twitter account. Twitter has a very nice API that makes it easy to pull information from the service. To get JSON list of people following your Twitter account, just send an HTTP GET request to <code>http://twitter.com/statuses/followers.json</code> using [HTTP Basic Authentication](http://en.wikipedia.org/wiki/Basic_authentication_scheme) with your username and password. You can also get a list of people following other user accounts from <code>http://twitter.com/statuses/followers/&lt;username&gt;.json</code>; no authentication necessary. We query Twitter for this information and cache it locally in a database.\r\n\r\nUnfortunately, due to Twitter's recent performance issues, many of the nicest features have either been limited or disabled, making it almost impossible to use Twitter exclusively for LOCT. We needed to get a list of tweets that mentioned LOCT, but couldn't with the current performance restrictions in place. If only there was another service that provided this functionality.\r\n\r\n#### And there is!\r\n\r\n[Summize](http://summize.com/) rocks. Based right here in the greater Washington metropolitan area, Summize is tweet search service that has one of the few direct feeds into every tweet that is twittered. They also have an awesome API that makes it dead simple to search all tweets. <code>http://summize.com/search.json?q=%23loct08</code>. That is all you need to get search results in JSON. Just like the list of followers from Twitter, the results are cached locally for Maximum Performance<sup>TM</sup>.\r\n\r\n#### I would follow Django if it had a Twitter account\r\n\r\nAs with almost all projects created by [Sunlight Labs](http://sunlightlabs.com), [Let Our Congress Tweet](http://letourcongresstweet.org/) is writting using [Django](http://djangoproject.com), a [Python](http://python.org) Web development framework. I love Django. It simplifies development by providing [object-relational mapping](http://en.wikipedia.org/wiki/Object-relational_mapping), templating, and other features in an unobtrusive way.\r\n\r\nDeveloping in Django is already quite rapid, but by reusing existing code we can develop at an unheard-of pace. Writing a reusable Django application is quite easy as it is nothing more than a standard Python module that can be used in any project.\r\n\r\n#### Feedinating the countryside\r\n\r\nA few weeks ago we released a new version of the [Sunlight Foundation](http://sunlightfoundation.com/) Web site. The old, infuriating Drupal installation was replaced with a slick Django application that was written in-house. One of the main features of the new site is feed aggregator that pulls in the recent blog posts from across the Sunlight-influenced transparency network. To accomplish this, we wrote _Feedinator_, a Django feed aggregator application that makes it easy to pull in feeds from multiple blogs and display them in different ways on a Web site.\r\n\r\nWe use Feedinator on Let Our Congress Tweet to pull in the feed of Tweets from the [LOTC08 twitter account](http://twitter.com/loct08/) and a [del.icio.us](http://del.icio.us/) feed of Web sites that have mentioned LOCT. By writing Feedinator in a way that makes it easily reusable, we were able to start incorporating feeds into LOCT in a matter of minutes.\r\n\r\nIf you would like to use Feedinator in your own project, you are in luck. We plan on releasing the code in the near future as well as the code for a few other Django applications and Python modules.\r\n\r\n#### Designers are useful\r\n\r\nWhile I was coding, Kerry took care of the design, CSS, and HTML. A few minutes of converting the HTML into Django templates and the site was up and running.\r\n\r\nSo that's it! After throwing in a few cron entries and some Apache configuration files, the site went live. We went from an idea to a production ready Web site in about six hours. Sure, the it isn't overly complicated, but we're proud of it nonetheless.", "date_published": "2008-07-11 16:07:59", "comment_count": 1, "slug": "from-idea-to-production-in-six-hours", "tags": ""}}, {"pk": 27, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 6, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "On Baseball and Congress", "excerpt": "<p>Modern baseball&#8217;s origins are something historians don&#8217;t have a good read on. If you look at the <a href=\"http://en.wikipedia.org/wiki/Origins_of_baseball\">Origins of Baseball</a> article on Wikipedia, you&#8217;ll see that we don&#8217;t know very much about where the rules came from, but it formalized somewhere around 1845 when the Knickerbocker Club of New York City began to play baseball against the New York Nine. In 1857 16 clubs finally sent delegates to a convention to standardize the rules and standardize America&#8217;s Pastime.</p>", "content": "<p>Modern baseball&#8217;s origins are something historians don&#8217;t have a good read on. If you look at the <a href=\"http://en.wikipedia.org/wiki/Origins_of_baseball\">Origins of Baseball</a> article on Wikipedia, you&#8217;ll see that we don&#8217;t know very much about where the rules came from, but it formalized somewhere around 1845 when the Knickerbocker Club of New York City began to play baseball against the New York Nine. In 1857 16 clubs finally sent delegates to a convention to standardize the rules and standardize America&#8217;s Pastime.</p>\r\n\r\n<p><a href=\"http://en.wikipedia.org/wiki/Baseball_statistics\">Baseball statistics</a> have their own story. A fellow named <a href=\"http://en.wikipedia.org/wiki/Henry_Chadwick_(writer)\">Henry Chadwick</a> was the first to start using statistics to judge a player&#8217;s performance. It was a few years after the sport was invented and formalized that this young journalist would give himself the goal of creating &#8220;numerical evidence as that would prove what players helped or hurt a team win.&#8221;</p>\r\n\r\n<p>It took nearly 100 years for baseball statistics to make it to the common man and woman. It wasn&#8217;t until 1951 when a researcher named Hy Turkin published the Encyclopedia of Baseball that used a computer to compile statistics for the first time. It wasn&#8217;t until 1977 that decent, predictive and objective statistical methods called <a href=\"http://en.wikipedia.org/wiki/Sabermetrics\">sabermetrics</a> were invented and distributed. Sabermetrics are the analytical methods that Theo Epstein used to break a curse and build a World Series winning Boston Red Sox. He even hired Sabermetrics&#8217; inventor to work for the Red Sox.</p>\r\n\r\n<p>It took over a century to invent and distribute the objective performance measuring statistics we use today to evaluate baseball players. To tell whether or not Greg Maddux is a better pitcher than Pedro Martinez. </p>\r\n\r\n<p>But though <a href=\"http://en.wikipedia.org/wiki/First_Continental_Congress\">Congress</a> has been around for nearly 234 years we still don&#8217;t have an objective way to tell whether or not my namesake, <a href=\"http://en.wikipedia.org/wiki/Henry_Clay\">Henry Clay</a> was as effective of a speaker as <a href=\"http://en.wikipedia.org/wiki/Nancy_Pelosi\">Nancy Pelosi</a>. This isn&#8217;t to say we haven&#8217;t been making up our own statistics. We&#8217;ve been compiling subjective <a href=\"http://www.lcv.org/scorecard/\">scorecards</a> for years. But we need a process of standardizing our statistics, publishing how they&#8217;re calculated, and we need to build a system for authenticating and delivering those results.</p>\r\n\r\n<p>It took us over 100 years to get good baseball stats, but this isn&#8217;t to say that the data didn&#8217;t exist or wasn&#8217;t being recorded. The data was still there. Here&#8217;s the full stats on the <a href=\"http://www.baseball-reference.com/teams/CHC/1876.shtml\">1876 Chicago White Stockings</a>. People were watching, keeping score, logging the games and recording the data and even making their own statistics out of it. But it was the process of standardizing the statistics, publishing how they&#8217;re to be calculated and sharing the results that made these <em>subjective</em> metrics effective.</p>\r\n\r\n<p>So what metrics out there are effective in evaluating our legislators? Off the top of my head, here&#8217;s some elementary ones:</p>\r\n\r\n<ol>\r\n<li><p>Attendance Percentage: The percent of time a member attends Congress when it is in session.</p></li>\r\n<li><p>Vote percentage: The percentage of time a member has voted when they&#8217;ve had the opportunity to do so.</p></li>\r\n<li><p>Sponsored Bills Per Term: The average number of bills sponsored and co-sponsored per term</p></li>\r\n<li><p>Sponsored Bills Passed Per Term: The average number of sponsored and co-sponsored bills passed per term</p></li>\r\n<li><p>Party percentage: The percentage of time the member votes with their political party</p></li>\r\n<li><p>Vote Victory Percentage: The percentage of time the member votes with a bill that passes.</p></li>\r\n</ol>\r\n\r\n<p>These are just obvious building blocks of a much more sophisticated statistical system. All these statistics exist right now. Sunlight&#8217;s partner, <a href=\"http://opencongress.org\">Open Congress</a> and <a href=\"http://govtrack.us\">GovTrack.us</a> and many more track their congressional statistics in their own way as do many others. In order to do it right we need:</p>\r\n\r\n<ol>\r\n<li><p>Standardization: We need to be calculating these things and naming these statistics the same way everywhere.</p></li>\r\n<li><p>Comparison: Statistics are not relevant unless they&#8217;re in context. We need to be able to create a ranked 1-535 list for every statistic we standardize and create</p></li>\r\n<li><p>Adoption: They need to be adopted and as pervasive as RBIs and ERAs.</p></li>\r\n<li><p>More: We need more statistics made from the data that congress generates that provide &#8220;numerical evidence&#8221; about the effectiveness of congress. We need our own &#8220;sabermetrics&#8221; that objectively evaluate whether or not a Member of Congress is a effective at representing those that chose to elect them. The ones I've listed aren't even close to being accurate predictors.</p></li>\r\n</ol>\r\n\r\n<p>So let this be a post to start a discussion amongst the transparency community about how we can begin standardizing our own objective statistics, making them useful and centralized. Let&#8217;s start working together to invent new statistics for how our members can be evaluated. </p>\r\n", "date_published": "2008-06-30 02:49:54", "comment_count": 3, "slug": "call-to-action-learn-from-baseball", "tags": ""}}, {"pk": 26, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 6, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Fun with CapitolWords", "excerpt": "We launched <a href=\"http://www.capitolwords.org\">Capitol Words</a> just a couple weeks ago and got a <a href=\"http://www.boingboing.net/2008/06/19/daily-congressional.html\">really</a> <a href=\"http://news.oreilly.com/2008/06/requesting-a-mashup.html\">great</a> <a href=\"http://www.metafilter.com/72702/Capitol-Words-US-Congress-In-A-Word-A-Day\">reception</a> from the blogs. I\u2019m two weeks in to my new duties as Director of Sunlight Labs and while I didn\u2019t have much (really, anything) to do with the project's success, I am really excited about it. With the <a href=\"http://www.capitolwords.org/api/\">CapitolWords API</a> we can start doing some interesting analysis of overall word-usage in Congress.", "content": "We launched <a href=\"http://www.capitolwords.org\">Capitol Words</a> just a couple weeks ago and got a <a href=\"http://www.boingboing.net/2008/06/19/daily-congressional.html\">really</a> <a href=\"http://news.oreilly.com/2008/06/requesting-a-mashup.html\">great</a> <a href=\"http://www.metafilter.com/72702/Capitol-Words-US-Congress-In-A-Word-A-Day\">reception</a> from the blogs. I\u2019m two weeks in to my new duties as Director of Sunlight Labs and while I didn\u2019t have much (really, anything) to do with the project's success, I am really excited about it. With the <a href=\"http://www.capitolwords.org/api/\">CapitolWords API</a> we can start doing some interesting analysis of overall word-usage in Congress.\r\n\r\nSome of this is obvious and you can see at the surface. Check out the screenshots below:\r\n<img src=\"http://img.skitch.com/20080629-c7ysh38su9w462su2ci9pbk1mj.jpg\" alt=\"June, 2005\" />\r\n\r\n<img src=\"http://img.skitch.com/20080629-x9ngxqcys8qndyk1wh1xcfitws.jpg\" alt=\"June, 2007\" />\r\n\r\n<img src=\"http://img.skitch.com/20080629-cdfu7h3rt82sre8tqc97b8ct92.jpg\" alt=\"June, 2008\" />\r\n\r\nJune seems to be predominantly about energy and oil. Septembers of even numbered years tend to be about security and intelligence. March tends to be about budgets and amounts.\r\n\r\nNeat! Josh wrote most of the code and handled the architecture of the system. Garrett who heads our <a href=\"Louis\">http://www.louisdb.org</a> project also had a big hand in concieving and building the application. Of course Kerry, our wonderful Creative Director helped make the user interface and designed the site. It is written in Django and MySQL. Great work guys!", "date_published": "2008-06-29 19:08:25", "comment_count": 0, "slug": "fun-with-capitolwords", "tags": ""}}, {"pk": 25, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 3, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Open Government Site Govtrack Goes Open Source", "excerpt": "", "content": "<img align=\"right\" title=\"GovTrack.US logo\" id=\"image39\" alt=\"GovTrack.US logo\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2008/04/picture-45.thumbnail.jpg\" /><img align=\"right\" title=\"OpenSource.org logo\" id=\"image40\" alt=\"OpenSource.org logo\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2008/04/picture-52.jpg\" />Josh Tauberer, one of the champs of opening U.S. Government data announced today he has made all of <a title=\"Govtrack.us\" href=\"http://govtrack.us\">Govtrack.us</a> available as open source.\r\n<blockquote>This week I made <a target=\"_blank\" href=\"http://www.govtrack.us/\">www.GovTrack.us</a> officially totally open source.\r\nGovTrack is a website that tracks U.S. federal legislation and also\r\nbuilds the only comprehensive open database of congressional\r\ninformation. While the data behind GovTrack has been provided in the\r\npublic domain for a number of years now, and has been successfully\r\npowering a bunch of other sites like OpenCongress, I've been playing\r\ncatch-up in getting the source code of the website opened up.</blockquote>\r\nRun at get the code! (<a title=\"Govtrack.us source code\" href=\"http://www.govtrack.us/source.xpd\">link</a>)", "date_published": "2008-04-03 14:17:44", "comment_count": 0, "slug": "open-government-site-govtrack-goes-open-source", "tags": ""}}, {"pk": 24, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 3, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Diagram of Financial Support for Public Government Sites", "excerpt": "", "content": "<img width=\"331\" height=\"247\" align=\"right\" alt=\"Diagram of financial support for public government sites\" title=\"Diagram of financial support for public government sites\" src=\"http://visiblegovernment.ca/images/blog/MoneyFlowSmall.PNG\" /><img width=\"337\" height=\"254\" align=\"right\" alt=\"Flows of data from government to non profit government sites\" title=\"Flows of data from government to non profit government sites\" src=\"http://visiblegovernment.ca/images/blog/DataFlowSmall.PNG\" />\r\n\r\nW0ot! Thanks for the excellent diagram WavingSparks.\r\n\r\nIt's very helpful to have such a <a href=\"http://waving.deadsquid.com/?p=29\">nice graphic explaining the data flows</a>. As you discovered, Sunlight is very interested in financially supporting and being part of making transparency information available.\r\n\r\nIn interest of transparency, Sunlight is also supporting <a title=\"Taxpayer for Common Sense\" href=\"http://www.taxpayer.net/\">Taxpayers for Common Sense</a> to update their website and offer smaller transparency grants, too (<a title=\"Sunlight Foundation Grants\" href=\"http://sunlightfoundation.com/grants\">http://sunlightfoundation.com/grants</a>). <a title=\"GovTrack\" href=\"http://govtrack.us\">GovTrack</a> is a pretty efficient and essential website and is nobody's weak link! For more related websites check out:\r\n<ul>\r\n\t<li><a title=\"Insanely Useful Government Data Sites\" href=\"http://sunlightfoundation.com/resources\">http://sunlightfoundation.com/resources</a></li>\r\n\t<li><a title=\"ProgrammableWeb Government Directory\" href=\"http://sunlightfoundation.com/resources\">http://www.programmableweb.com/government</a></li>\r\n</ul>\r\n(Link: <a href=\"http://waving.deadsquid.com/?p=29\">http://waving.deadsquid.com/?p=29)</a>", "date_published": "2008-03-14 22:59:19", "comment_count": 0, "slug": "38", "tags": ""}}, {"pk": 23, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 3, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Open Source Semantics Picking Up Speed", "excerpt": "", "content": "<img align=\"right\" alt=\"Screenshot of size of Metaweb WEX download\" title=\"Screenshot of size of Metaweb WEX download\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2008/02/picture-42.jpg\" />Metaweb has announced an open source release of structured data from Wikipedia. Via the email from get.theinfo email list:\r\n<blockquote><em>\"Hello from Metaweb. We've just released a GFDL licensed extraction of\r\nWikipedia in XML + relational form. Anyone is welcome to use it for\r\nany purpose...\"</em></blockquote>\r\nThis follows Reuters recent announcement of Open Calais API to extract people, places, things, and simple relationships from unstructured text. (We are experimenting with similar techniques of entity tagging via open protocols at Sunlight.) Metaweb's WEX's is 57GB of download-able structured data from the largest peer-production encyclopedia project ever.  The Semantic Web, so long discussed, is now beginning a virtuous cycle of innovation.  We are entering the age of <em>open source semantics</em>. Like compounding interest, Moore's Law, and exercise, results from the cycle of innovation around open source semantics will multiply quickly. If you thought Google circa 2007 is impressive, buckle your seatbelt and reach for your helmet. Things are about to move even faster.\r\n\r\nAddendum: DBPedia is another project extracting data from Wikipedia in the RDF format.\r\n\r\n<strong>Links</strong>: \u2022<a href=\"http://download.freebase.com/wex/\">Metaweb's WEX</a> \u2022<a href=\"http://www.opencalais.com/\">Open Calais</a> \u2022<a href=\"http://www.readwriteweb.com/archives/reuters_calais.php\">ReadWriteWeb on Open Calais</a> \u2022<a title=\"DBPedia\" href=\"http://dbpedia.org\">DBPedia</a>", "date_published": "2008-02-19 13:51:17", "comment_count": 0, "slug": "open-source-semantics-picking-up-speed", "tags": ""}}, {"pk": 22, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 3, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Google's Social Graph API", "excerpt": "", "content": "<img width=\"300\" align=\"right\" alt=\"Screenshot Social Graph API code page\" title=\"Screenshot Social Graph API code page\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2008/02/picture-31.jpg\" />The <a title=\"Social Graph API Code page\" href=\"http://code.google.com/apis/socialgraph/\">Social Graph API page</a> on Google code exemplifies the future of multimedia learning and why open source is being so productive relative to proprietary software development efforts. A two and half minute YouTube video introduces the concept. Links are included to documentation and examples. I've got code, examples, documentation, and even a human giving me a tutorial. (This one intro video could easily be expanded to series of step-by-step instructions.) Five developers might jump on the Social Graph API or 5,000. The right five might matter more than having 5,000 developers. What matters is two points. First, beneficial multimedia is no longer expensive to produce or distribute. In fact, it is becoming a basic skill of people everywhere. Second, the distributed nature of open source mentality encourages people to provide information in economical paired-down forms\u2014even if the code is not open source but merely an API.", "date_published": "2008-02-11 17:53:36", "comment_count": 0, "slug": "34", "tags": ""}}, {"pk": 21, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 4, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "Stay-at-home Servers", "excerpt": "[Microsoft propoganda targets](http://gizmodo.com/photogallery/microserveces08/) our most vulnerable citizens - *our children*!", "content": "[Microsoft propoganda targets](http://gizmodo.com/photogallery/microserveces08/) our most vulnerable citizens - *our children*!", "date_published": "2008-01-09 19:10:26", "comment_count": 0, "slug": "stay-at-home-servers", "tags": ""}}, {"pk": 20, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 5, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "polipoly - A tool for dealing with district boundary polygons", "excerpt": "Finding out what congressional district an address falls within can be a difficult problem.  One solution is to use the polipoly library that we have open sourced to check if a point falls within a polygon boundary of a congressional district.", "content": "Finding out what congressional district an address falls within can be a difficult problem. Typically you need a 9 digit zip code, and the US Postal Service does not make looking up the zip+4 easy.  Even with a zip+4, existing solutions are either expensive or inaccurate and sometimes both.\r\n\r\nIt turns out that the US census bureau publishes <a href=\"http://www.census.gov/geo/www/cob/cd110.html\">polygon files</a> defining the boundaries of all 435 congressional districts. Through geocoding services such as google maps we can easily convert an address to a latitude and longitude and therefore it is possible to determine what district an address lies within by simply testing what polygon it falls within.\r\n\r\nWe <a title=\"recently added a new API method\" href=\"http://sunlightlabs.com/blog/?p=29\">recently added a new API method</a> but one of the major drawbacks is that because it uses Google as a geocoding service we are limited to 50,000 calls a day.  We're aware that some organizations may want to do batch processing of their membership lists, which may means thousands of lookups at once.  In addition to using up all of our geocoding requests, calling a web service for this kind of batch processing isn't efficient.\r\n\r\nTo attempt to meet these challenges we are proud to announce <a title=\"polipoly\" href=\"http://code.google.com/p/polipoly/\">polipoly</a>.  Polipoly is a small python library that uses public domain census shapefiles to allow developers to write simple python scripts that can relate addresses to congressional districts.  And because you're running it with your own Google Maps API key, you are able to use all 50,000 geocoding requests a day.\r\n\r\nFor information on installation and usage head over to the project page. Example source for a web service that works just like <a target=\"_blank\" href=\"http://sunlightlabs.com/api/places.getDistrictFromAddress.php\">places.getDistrictFromAddress</a> has been released, as well as an example of processing a CSV file similar to the kind that could be exported from a database or spreadsheet.\r\n\r\nThis is the second project we have released on Google Code (the first being the <a href=\"http://code.google.com/p/sunlightadk/\">Sunlight ADK</a>) but much more is on the way in terms of open source.", "date_published": "2007-12-10 10:00:58", "comment_count": 0, "slug": "polipoly", "tags": "python"}}, {"pk": 19, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 4, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "Crossposted from my personal blog", "excerpt": "If you like APIs and mashups, you should check out [ProgrammableWeb](http://www.programmableweb.com/) - it\u2019s a directory/advice/analysis site for all things API-ish.", "content": "If you like APIs and mashups, you should check out [ProgrammableWeb](http://www.programmableweb.com/) - it\u2019s a directory/advice/analysis site for all things API-ish.\r\n\r\nMost recently, they\u2019ve been working on building a subsection for [government](http://www.programmableweb.com/government) APIs, and have included 5 APIs that we're involved with, as well as APIs from MySociety.org and a whole lot more.\r\n\r\nAnd who knows, we may have another contest someday, involving mashups that combine these various APIs.", "date_published": "2007-11-13 20:43:46", "comment_count": 0, "slug": "crossposted-from-my-personal-blog", "tags": ""}}, {"pk": 18, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 3, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "New Address to Congressional District API Method", "excerpt": "", "content": "James Turk added a powerful address to congressional district method <a target=\"_blank\" href=\"http://sunlightlabs.com/api/places.getDistrictFromAddress.php\">places.getDistrictFromAddress</a> to the <a href=\"http://sunlightlabs.com/api/\">Sunlight Labs API</a>. The method geocodes a street address using Google's Map API and identifies the congressional district polygon in which the address's lat/long falls using <em>up-to-date</em> shape files from Census.gov.\r\n\r\nHere's an example call:\r\n<pre>http://api.sunlightlabs.com/places.getDistrictFromAddress?address=1818+N+St+NW+Washington,+DC&output=xml</pre>\r\nHere's an example result in XML (json also available):\r\n<pre>&lt;results&gt;\r\n&lt;address&gt;1818 N St NW Washington, DC&lt;/address&gt;\r\n&lt;latitude&gt;38.907231&lt;/latitude&gt;\r\n&lt;longitude&gt;-77.042149&lt;/longitude&gt;\r\n&lt;districts&gt;\r\n&lt;district state=\"DC\"&gt;98&lt;/district&gt;\r\n&lt;/districts&gt;\r\n&lt;/results&gt;</pre>\r\nWhen I started at Sunlight Labs in 2006, I heard companies paying annual fees for zipcode to congressional district translation databases and services. This should no longer be the case, at least for small doses of information. First, we are offering this API method. Second, our service uses official congressional district political boundaries now being (or by 2008 will be) updated in real-time by the Census as they learn of changes. So it should be possible for others to code such a service as well. The only cost should be related to number of addresses needing to be coded. Google reasonably imposes 15,000 calls-per-day limitation from same IP address on its service. So we might look into an alternate geocoding service so we can scale. Then again, we might also just fire up instances of EC2 to source the calls from different IP addresses, too.", "date_published": "2007-11-13 19:54:11", "comment_count": 1, "slug": "new-address-to-congressional-district-api-method", "tags": ""}}, {"pk": 17, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 1, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Sunlight API Development Kit", "excerpt": "When working on a prototype, it is often necessary to get a REST web service up and running quickly. It's easy enough to do, but the amount of code that is duplicated for each service can really increase the time to completion. To make the development of REST web services quicker and easier, we have developed the <a href=\"http://sunlightadk.googlecode.com\">Sunlight API Development Kit</a> or, as we affectionately call it, the <a href=\"http://sunlightadk.googlecode.com\">Sunlight ADK</a>. The ADK is a PHP framework that assists in the rapid development of REST web services. We've released the code under the LGPL license.", "content": "<p>When working on a prototype, it is often necessary to get a REST web service up and running quickly. It's easy enough to do, but the amount of code that is duplicated for each service can really increase the time to completion. To make the development of REST web services quicker and easier, we have developed the <a href=\"http://sunlightadk.googlecode.com\">Sunlight API Development Kit</a> or, as we affectionately call it, the <a href=\"http://sunlightadk.googlecode.com\">Sunlight ADK</a>. The ADK is a PHP framework that assists in the rapid development of REST web services. We've released the code under the LGPL license.</p>\r\n<h3>REST Framework</h3>\r\n<p>The core of the ADK is a framework for rapid web service development. You don't have to worry about receiving requests, building XML or JSON, or any other tedious code. All you need to do is write code to populate a RESTResponse object. We take it from there and generate XML or JSON from the object depending on the type of response the user requested.</p>\r\n<h3>Administration Application</h3>\r\n<p>So how do you manage all of the services you've written? We were nice enough to include a web based management application that can be used to manage users and service methods. To create a new method just fill in the method name and the name of the PHP class that implements the service. As long as your service class was dropped in the right folder, you'll be up and running with the method.</p>\r\n<h3>Key Management and Access Controls</h3>\r\n<p>The administration application can also be used to issue keys to users of the web service. You can place usage limits on each service method to limit the number of calls or amount of data that is transferred over a variable time period. You can also create methods that are accessible to a limited number of users. Of course if you prefer to have your service open to everyone then a simple configuration change will allow access to the services without a key.</p>\r\n<h3>Get it and contribute!</h3>\r\n<p>We hope you'll find the Sunlight ADK useful. <a href=\"http://sunlightadk.googlecode.com\">Get a copy now</a> and let us know what you think. If you would like to contribute to the project, which we hope you do, please contact us at <a href=\"mailto:labs@sunlightfoundation.com\">labs@sunlightfoundation.com</a>.", "date_published": "2007-10-19 18:33:45", "comment_count": 0, "slug": "sunlight-api-development-kit", "tags": ""}}, {"pk": 16, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 4, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "Mini-Grants", "excerpt": "If you're a developer-type person, and you are involved in some way in making a difference in the way people interact w/Congress, then you may want to check this out...", "content": "If you're a developer-type person, and you are involved in some way in making a difference in the way people interact w/Congress, then you may want to check this out:\r\n\r\n> The Sunlight Foundation/Network is offering grants of $1,000 to $5,000 for local groups that have creative ideas for changing the relationship between representatives and the people they represent. In addition, Sunlight also provides consulting support and some networking opportunities for its Grantees.\r\n\r\n> We encourage applications from existing small local nonprofits and websites, from offshoots of national groups, from individuals, and from informal groups of citizens. The grants will go towards funding and implementing original ideas that will create a better, more democratic relationship between government and citizens.\r\n\r\n> Projects will be judged on how closely they fit with Sunlight\u2019s mission of improving the relationship between citizens and their member of Congress through more transparency of information.  Example of people who have received grants in the past is located here: [http://www.sunlightfoundation.com/grants/] (if you scroll to half way there is the mini grant section).  The focus should be on shedding more light on what Congress does and how to improve the communication between citizens and Congress.  As a rule we do not award money for salaries but do for technology upgrades.\r\n\r\n> If you are interested in applying, please fill out the provided web form, [it is available on the Sunlight Foundation grants page](http://www.sunlightfoundation.com/grants/minigrantapplication/).  Please describe your project with a detailed description of how it fits in with Sunlight\u2019s mission and your goals for your project, an itemized budget (including the amount requested from Sunlight) and contact information. \r\n\r\n> If you have any questions feel free to contact <nisha@sunlightfoundation.com>.\r\n\r\nExamples - you are involved in communications/interacting with Congress, and... your laptop died, or you could really use another linux server for your organization, or you can't afford to go to an important conference, etc.\r\n\r\nMaybe we can help!", "date_published": "2007-10-11 15:39:26", "comment_count": 0, "slug": "mini-grants", "tags": ""}}, {"pk": 15, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 1, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Sunlight Labs API changes", "excerpt": "Recent updates to the Sunlight API data.", "content": "<p>Recent updates to the Sunlight API data:</p>\r\n<h4>Deleted Members of Congress</h4>\r\n<ul>\r\n<li>Paul Gillmor (fakeopenID148)</li>\r\n<li>Juanita Millender-McDonald (fakeopenID272)</li>\r\n<li>Charles Norwood (fakeopenID294)</li>\r\n<li>Craig Thomas (fakeopenID528)</li>\r\n</ul>\r\n<h4>Added Members of Congress</h4>\r\n<ul>\r\n<li>John Barraso (fakeopenID600)</li>\r\n<li>Madeleine Bordallo (fakeopenID601)</li>\r\n<li>Paul Broun (fakeopenID602)</li>\r\n<li>Donna Marie Christensen (fakeopenID603)</li>\r\n<li>Eni Fa'aua'a Hunkin (fakeopenID604)</li>\r\n<li>Luis Fortuno (fakeopenID605)</li>\r\n<li>Eleanor Norton (fakeopenID607)</li>\r\n<li>Laura Richardson (fakeopenID608)</li>\r\n</ul>", "date_published": "2007-09-28 20:55:02", "comment_count": 0, "slug": "sunlight-labs-api-changes", "tags": ""}}, {"pk": 14, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 4, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "European Transparency", "excerpt": "Our comrades-in-arms on the Euro side of the Atlantic have their own transparency advocates and meetups.  Here's an interesting one, called [Berlin In August](http://people.oii.ox.ac.uk/escher/2007/08/16/berlin-in-august-summary/).", "content": "Our comrades-in-arms on the Euro side of the Atlantic have their own transparency advocates and meetups.\u00a0 Here's an interesting one, called [Berlin In August](http://people.oii.ox.ac.uk/escher/2007/08/16/berlin-in-august-summary/).\r\n\r\nTwo links in particular caught my eye:\r\n\r\n* [Good Practices for Transparency Sites](http://berlininaugust.politik-digital.de/index.php/Good_Practice)\r\n* [What is Still Missing](http://berlininaugust.politik-digital.de/index.php/What_is_still_missing)\r\n\r\nEnjoy!", "date_published": "2007-09-27 14:52:30", "comment_count": 0, "slug": "european-transparency", "tags": ""}}, {"pk": 13, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 4, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "Digg EarmarkWatch", "excerpt": "If you are a digg user, please consider helping us get the word out about our latest tool:\r\n\r\n[http://digg.com/politics/Citizens_Here_is_your_chance_to_investigate_Congress_Find_The_Pork](http://digg.com/politics/Citizens_Here_is_your_chance_to_investigate_Congress_Find_The_Pork)", "content": "If you are a digg user, please consider helping us get the word out about our latest tool:\r\n\r\n[http://digg.com/politics/Citizens_Here_is_your_chance_to_investigate_Congress_Find_The_Pork](http://digg.com/politics/Citizens_Here_is_your_chance_to_investigate_Congress_Find_The_Pork)", "date_published": "2007-09-26 13:43:51", "comment_count": 0, "slug": "digg-earmarkwatch", "tags": ""}}, {"pk": 12, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 4, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "EarmarkWatch", "excerpt": "Our social app to identify and clean up the data around government earmarks - [EarmarkWatch](\"http://www.earmarkwatch.org/) is live.  Props to Kerry and James for all the hard work they did in getting it ready.", "content": "Our social app to identify and clean up the data around government earmarks - [EarmarkWatch](\"http://www.earmarkwatch.org/) is live.\u00a0 Props to Kerry and James for all the hard work they did in getting it ready.", "date_published": "2007-09-25 13:00:59", "comment_count": 0, "slug": "earmarkwatch", "tags": ""}}, {"pk": 11, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 1, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Mashup the web", "excerpt": "One of our focuses here at Sunlight Labs is to demonstrate how open data enables citizens to be engaged and informed on how Congress works. We do this by creating mashups that make information from a variety of sources easy to manipulate and understand. I recently gave a talk at a <a href=\"http://www.heritage.org/press/carr/bootcamp.cfm\">CARR Boot Camp</a> on using the web to work with data. While not directly related to Congressional transparency, the following example from the talk is a good demonstration of how an end user can work with open data without the assistance of software developers.", "content": "<p>One of our focuses here at Sunlight Labs is to demonstrate how open data enables citizens to be engaged and informed on how Congress works. We do this by creating mashups that make information from a variety of sources easy to manipulate and understand. I recently gave a talk at a <a href=\"http://www.heritage.org/press/carr/bootcamp.cfm\">CARR Boot Camp</a> on using the web to work with data. While not directly related to Congressional transparency, the following example from the talk is a good demonstration of how an end user can work with open data without the assistance of software developers.</p>\r\n<p>We'll start with a site I highly value, <a href=\"http://dcfoodies.com/\">DC Foodies</a>. The site has great reviews of restaurants in the DC area and other related blog posts. Most of the review entries contain the address of the restaurant being reviewed. By itself this is of little value. We can read the post and see the address, but that's about it. Fortunately the site has an RSS feed.</p>\r\n<p>Let's take the RSS feed and run it through Yahoo's fantastic Pipes service. Pipes lets you take RSS and other feeds and execute various operations on the data. The site provides a dead simple visual editor that is used to apply the operatons and filters to the data. For example:</p>\r\n<p><img id=\"image20\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2007/09/picture-1.png\" alt=\"DCFoodies.com in Yahoo Pipes\" /></p>\r\n<p>The pipe in the image above takes the DC Foodies RSS feed, geocodes any location data found in the posts, and filters out entries that do not contain geographic information. Pipes adds custom elements to the RSS feed that contain the latitude and longitude of the restaurants based on the addresses in the reviews. How cool is that? It gets even better.</p>\r\n<p>Among the various formats in which you can output your modified feed, <a href=\"http://code.google.com/apis/kml/documentation/\">KML</a> is specifically suited to syndication of geographic data. We can take the <a href=\"http://pipes.yahoo.com/pipes/pipe.run?_id=GnHiiblc3BGoaGgXCR2yXQ&_render=kml\">URL of the KML output</a> and paste it into the search box of <a href=\"http://maps.google.com\">Google Maps</a>. Click the Search Maps button and Google will <a href=\"http://maps.google.com/maps?f=q&hl=en&geocode=&q=http:%2F%2Fpipes.yahoo.com%2Fpipes%2Fpipe.run%3F_id%3DGnHiiblc3BGoaGgXCR2yXQ%26_render%3Dkml&ie=UTF8&ll=38.996959,-77.02748&spn=0.032352,0.056047&z=14&om=1\">plot the locations</a> from the KML feed on the map. Click on one of the map markers and you will see a summary of the original blog post.</p>\r\n<p>New tools and technologies are allowing end users to mash up content without needing the assitance of software developers. Building open platforms that work on open formats is key to allowing end user manipulation of data across disparate resources.</p>", "date_published": "2007-09-20 19:54:09", "comment_count": 0, "slug": "mashup-the-web", "tags": ""}}, {"pk": 10, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 1, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Refresh DC: Web Widgets: What, Why, and How", "excerpt": "Refresh is a community of web designers, developers, and other new media professionals working together to refresh the creative, technical, and professional aspects of their trades in greater Washington, DC.", "content": "<p style=\"background-color: #324B83; padding: 15px;\"><img src=\"http://refresh-dc.org/images/logo.png\" alt=\"Refresh DC logo\" /></p>\r\n<p>It's mid-month which means it's time for another meeting of <a href=\"http://refresh-dc.org\">Refresh DC</a>. <a href=\"http://www.willmeyer.com/\">Will Meyer</a> from <a href=\"http://www.clearspring.com/\">Clearspring</a> will lead an interactive discussion on web widgets in general, and best practices specifically, with some examples as appropriate.</p>\r\n<p>Refresh is a community of web designers, developers, and other new media professionals working together to refresh the creative, technical, and professional aspects of their trades in greater Washington, DC.</p>\r\n<p>These events are a great place to meet incredibly creative and highly intelligent web people in the DC area. Refresh has brought out an vigorous community that few people knew existed in this town known more for government contracting and politics than web technologies. It's Silicon Valley of the east...or at least getting there.</p>", "date_published": "2007-09-20 18:43:09", "comment_count": 0, "slug": "refresh-dc-web-widgets-what-why-and-how", "tags": ""}}, {"pk": 9, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 4, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "Howdy", "excerpt": "I'm John Brothers, and I'm the new CTO of the lab.", "content": "Thanks for reading the SunlightLabs Blog!\u00a0 I'm John Brothers, and I'm the new CTO of the lab.\r\n\r\n We're going to be doing a lot more with this blog over the coming months, so look for articles on events, releases of our open source projects, tutorials, advice, commentary and a whole lot more!", "date_published": "2007-09-20 15:07:41", "comment_count": 0, "slug": "howdy", "tags": ""}}, {"pk": 8, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 1, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "Using RSS for House Committee Schedule Feeds", "excerpt": "Do you want congressional committee schedule information in a standard syndication format? We sure would and this is what we propose.", "content": "Based on John Wonderlich's work on the [Open House Project](http://theopenhouseproject.com) and Joshua Ruihley's work on [Open Hearings](http://openhearings.org), we here in the [Labs](http://sunlightlabs.com) decided to write a 'dream feed' as an example of an RSS format congress could use to syndicate their committee meeting schedule.\r\n\r\n[http://sunlightlabs.com/projects/openhouse/schedule-v01.xml](http://sunlightlabs.com/projects/openhouse/schedule-v01.xml \"link to feed\")\r\n\r\n    <?xml version=\"1.0\"?>\r\n    <rss version=\"2.0\"\r\n        xmlns:xcal=\"urn:ietf:params:xml:ns:xcal\"\r\n        xmlns:enc=\"http://www.solitude.dk/syndication/enclosures/\">\r\n      <channel>\r\n        <title>House Committee Schedule</title>\r\n        <link>http://house.gov/schedule/</link>\r\n        <description>Master schedule for house committees.</description>\r\n        <language>en-us</language>\r\n        <lastBuildDate>Fri, 06 Jul 2007 09:39:21 EDT</lastBuildDate>\r\n        <docs>http://www.rssboard.org/rss-2-0-9</docs>\r\n        <generator>House Schedule Generator 1.0</generator>\r\n        <webMaster>schedule@house.gov</webMaster>\r\n        <ttl>3600</ttl>\r\n        <item>\r\n          <title>Subcommittee on General Farm Commodities and Risk Management</title>\r\n          <link>http://agriculture.house.gov/hearings/schedule.html</link>\r\n          <description>To review trading of energy-based derivatives.</description>\r\n          <pubDate>Fri, 06 Jul 2007 09:39:21 EDT</pubDate>\r\n          <guid>http://agriculture.house.gov/hearings#200707121000</guid>\r\n          <category domain=\"http://house.gov/schedule/type\">hearing</category>\r\n          <category domain=\"http://house.gov/schedule/visibility\">public</category>\r\n          <xcal:organizer cn=\"House Committee on Agriculture\">http://agriculture.house.gov</xcal:organizer>\r\n          <xcal:location>1300 Longworth House Office Building, Washington, DC</xcal:location>\r\n          <xcal:dtstart>2007-07-12T10:00:00Z</xcal:dtstart>\r\n          <xcal:dtend>2007-07-12T11:00:00Z</xcal:dtend>\r\n          <xcal:attendee role=\"CHAIRMAN\" cn=\"Rep. John Doe\">mailto:john.doe@house.gov</xcal:attendee>\r\n          <xcal:attendee role=\"OPT-PARTICIPANT\" cn=\"Rep. Joe Smith\">mailto:joe.smith@house.gov</xcal:attendee>\r\n          <xcal:attendee role=\"X-WITNESS\" cn=\"Dan Johnson\">mailto:djohnson@company.com</xcal:attendee>\r\n          <enc:enclosure title=\"Live Media\">\r\n            <enc:link type=\"video/quicktime\" length=\"11534336\"\r\n              url=\"http://agriculture.house.gov/hearings/200707121000.mov\" />\r\n            <enc:link type=\"audio/mpeg\" length=\"11534336\"\r\n              url=\"http://agriculture.house.gov/hearings/200707121000.mp3\" />\r\n          </enc:enclosure>\r\n          <enc:enclosure title=\"Archived Media\">\r\n            <enc:link type=\"video/quicktime\" length=\"11534336\"\r\n              url=\"http://agriculture.house.gov/hearings/archive/200707121000.mov\" />\r\n            <enc:link type=\"audio/mpeg\" length=\"11534336\"\r\n              url=\"http://agriculture.house.gov/hearings/archive/200707121000.mp3\" />\r\n          </enc:enclosure>\r\n          <enc:enclosure title=\"Agenda\">\r\n            <enc:link type=\"application/pdf\" length=\"11534\"\r\n              url=\"http://agriculture.house.gov/hearings/200707121000_agenda.pdf\" />\r\n          </enc:enclosure>\r\n        </item>\r\n      </channel>\r\n    </rss>\r\n\r\n\r\nThe feed was written to the [RSS 2.0.9 spec](http://www.rssboard.org/rss-2-0-9 \"specification\"). Since RSS is meant for syndication, two additional modules, elements belonging to a namespace, were used to add the information that was needed. The [xCal XML specification](http://tools.ietf.org/html/draft-royer-calsch-xcal-03 \"iCalendar in XML\") was used to add event data to the feed. xCal is an XML representation of the widely used iCalendar format. The format is fairly straighforward except for the organizer and attendee elements. The specification requires the value of these elements to be a URI rather than a string naming the individual. The URI can be any identifier, but is typically the email address of the individual. To include the name of the individual, a cn attribute must be added to the tag. The attendee element has a role attribute that describes the role in which the attendee will be serving. We've added a custom X-WITNESS role to identify witnesses that are testifying before a committee.\r\n\r\nAdditionally, we have a need to add multiple enclosures to the feed to represent different types of media: audio and video of events and documents related to the event. Since the RSS 2.0.9 spec is a bit unclear as to whether multiple enclosures are supported we decided to use Andreas Pedersen's [multiple enclosure extension](http://www.solitude.dk/archives/20050208-0045/ \"multiple enclosures in RSS\"). It's fairly straightforward and allows us to link to multiple types of related documents and media.\r\n\r\nYour feedback is welcome and desired! We'd like to make this sample feed as good as it can be to serve as an example of the Right Way to syndicate congressional schedules so any fixes or additions are quite appreciated. We'll post all changes to the blog. ", "date_published": "2007-07-10 20:32:01", "comment_count": 3, "slug": "using-rss-for-house-committee-schedule-feeds", "tags": ""}}, {"pk": 7, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 3, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "CiviCRM integrating SunlightLabs API", "excerpt": "", "content": "David  announced <a href=\"http://civicrm.org/node/205\">CiviCRM integration with our SunlightLabs API</a> today. This is more exciting stuff. In an email to us, David noted the business implications:\r\n<blockquote>\r\n<ol>\r\n\t<li><em>People could automatically email all the people in the database within a particular district.</em></li>\r\n\t<li><em>You can have a tab in a contact record automatically populated with political information.</em></li>\r\n</ol>\r\n</blockquote>\r\nIn his blog post, David complimented the documentation of <a href=\"http://sunlightlabs.com/api\">the SunlightLabs API</a> (to which we have to thank <a href=\"http://taggel.com/\">Labs alumni Dr. Carl Anderson</a> who also created the first version of the API modeling it and the documentation from Flickr's excellent API). He also politely pointed out our need to blog a bit more (this is a start!) and suggested some more improvements.\r\n\r\nThe <a href=\"http://sunlighfoundation.com\">SunlightFoundation.com website</a> runs on Drupal and CiviCRM, so we ourselves will be a beneficiary of this integration. W0ot!", "date_published": "2007-07-05 16:44:13", "comment_count": 0, "slug": "civicrm-integrating-sunlightlabs-api", "tags": ""}}, {"pk": 6, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 2, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Ajaj: Ajax2.0", "excerpt": "Is Ajaj the new Ajax?", "content": "Is Ajaj the new Ajax?\r\n\r\n<em><strong>Ajax: Asynchronous Javascript and XML</strong></em>. I wonder though how many people actually use XML in web2.0 applications. After all, an XMLHttpRequest can return both responseXML and responseText. My bet is that text, including <strong>*J*</strong>SON, is used <em>far</em> more frequently than XML.\r\n\r\nIf one can do all the data processing server-side, send it back as HTML text to the browser, and the browser needs only to set some innerHTML, why bother with XML? Well, you respond, \"what if I have structured data and different pieces of data need to go to different parts of the page: a title here, an image URL over there, some content here. Then, innerHTML won't work. You need to send over data plus descriptive metadata so that the browser code knows what it is.\" While, true, I am still not convinced that XML is used that much. First, one can send different chunks of data in simple responseText. For instance, one might send back a comma- or pipe-delimited string. The javascript does a split on that character and so long as the correct ordering of those data pieces is preserved, the data can be separated and sent where it needs to go in the page. For more complex data, or for a more robust data transfer one can use JSON in the responseText.\r\n\r\nI've been developing <a href=\"http://sunlightlabs.com/api/\">an API for the labs</a> recently and each method can produce JSON and XML. It was really striking to me just how easy JSON is to consume in the browser. One gets all the benefits of metadata markup, but it is both very compact and extraordinarily simple to  reference those data items in Javascript. For instance, I send\r\n<div style=\"padding: 10px; background: #ffcc66 none repeat scroll 0% 50%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial\">{ name : \"Joe Doe\", title : \"VP, Engineering\", phone : \"13-456-7890\"}</div>\r\n<em>[<strong>Update</strong> : as the comment below points out, it should have read: { \"name\" : \"Joe Doe\", \"title\" : \"VP, Engineering\", \"phone\" : \"13-456-7890\"}] </em>\r\n\r\nin my responseText. In Javascript I need only do the following:\r\n<div style=\"padding: 10px; background: #ffcc66 none repeat scroll 0% 50%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial\">var obj = eval( '(' + XMHttpRequest.responseText + ')' );</div>\r\nand now I reference those individual data items as obj.name, obj.title and obj.phone. Could it be any simpler?\r\n\r\nWell, I thought that until I really started playing around with JSON and found that it has frustrating limits, ones that JSON.org does not highlight <strike>(dare I say lie?)</strike> in their documentation.\r\n\r\nTake the following code:\r\n<div style=\"padding: 10px; background: #ffcc66 none repeat scroll 0% 50%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial\">var j1 = \"{ data: \\\"ab\\\"}\";\r\nvar obj1 = eval( \"(\" + j1 + \")\" );\r\nalert(obj1.data);</div>\r\nwhat is alerted? \"ab\". Correct.\r\n\r\nHow about this:\r\n<div style=\"padding: 10px; background: #ffcc66 none repeat scroll 0% 50%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial\">var j1 = \"{ data: \\\"a\\nb\\\"}\";\r\nvar obj1 = eval( \"(\" + j1 + \")\" );\r\nalert(obj1.data);</div>\r\nWhat is alerted? \"a\\nb\". Correct? Wrong. It bails.\r\n\r\n<strong><em>JSON, or rather the Javascript parser, cannot handle \\n.\r\n</em></strong>\r\nThat's strange I swear that \\n is shown on JSON's wonderfully easy to understand documentation. Let's take a look:\r\n\r\n<img width=\"468\" height=\"357\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2007/03/string.gif\" />\r\n\r\nYes, that's clear. JSON.org says that '\\n' is allowed in JSON. <a href=\"http://framework.zend.com/issues/browse/ZF-504\">I was not the only one to find this \\n limitation</a>:\r\n<div style=\"padding: 10px; background: #ffcc66 none repeat scroll 0% 50%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial\">\"As for the second, regarding newline characters, http://www.json.org/ says they are valid JSON notation\"</div>\r\n<div style=\"padding: 10px; background: #ffcc66 none repeat scroll 0% 50%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial\">Sort of, but it also states that \"It is based on a subset of the JavaScript Programming Language, Standard ECMA-262 3rd Edition - December 1999\", which clearly forbids them (par. 7.3)</div>\r\n<strike>So it is a lie then. JSON does not support \\n.</strike>\r\n\r\n<strike>Not only is this annoying but, in my mind, it does limit JSON usage.  I cannot  use it to transport arbitrary chunks of text.</strike>\r\n\r\n<hr /><strong>UPDATE</strong>: so at here Ajaxworld in NYC, I just asked Douglas Crockford, inventor of JSON, about this. So, it turns out that \\n needs to be escaped properly. Thus, the newline character should have been \\\\n\r\nThat is,\r\n<div style=\"padding: 10px; background: #ffcc66 none repeat scroll 0% 50%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial\">var j1 = \"{ data: \\\"a\\\\nb\\\"}\";\r\nvar obj1 = eval( \"(\" + j1 + \")\" );\r\nalert(obj1.data);</div>\r\ndoes work. JSON can handle arbitrary chunks of text so long as unicode characters are properly escaped. Whoops sorry about that. Off to eat some humble pie...\r\n<hr />", "date_published": "2007-03-11 16:26:33", "comment_count": 1, "slug": "ajaj-ajax20", "tags": ""}}, {"pk": 5, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 2, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Instant APIs: no code necessary, just add mouseclicks", "excerpt": "APIs and mashups are the bread and butter of the Sunlight Labs. In fact, the official name of the the Labs is actually The Sunlight <em>Mashup</em> Labs, so it is perhaps not surprising that we headed off to the <a href=\"http://wiki.mashupcamp.com/index.php/WhosComingToMashupCamp3\">MashupCamp 3</a> at MIT, Cambridge a couple of weeks ago to geek out and to see the latest happenings, trends and players in the world of mashups and technology.", "content": "<img id=\"image10\" alt=\"dapper logo\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2007/02/dappit-flower-small-multicolor.thumbnail.gif\" />\r\n\r\n<img width=\"125\" height=\"23\" id=\"image11\" alt=\"openkapow\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2007/02/kapowlogo.thumbnail.jpg\" />\r\n\r\nAPIs and mashups are the bread and butter of the Sunlight Labs. In fact, the official name of the the Labs is actually The Sunlight <em>Mashup</em> Labs, so it is perhaps not surprising that we headed off to the <a href=\"http://wiki.mashupcamp.com/index.php/WhosComingToMashupCamp3\">MashupCamp 3</a> at MIT, Cambridge a couple of weeks ago to geek out and to see the latest happenings, trends and players in the world of mashups and technology.\r\n\r\nThere was a lot of cool stuff demoed, debated and hacked at the camp but one theme that really stood out for me was tools that made scraping easier and more effective than ever before. In short, tools to create instant APIs, without programming \u2014 yes, really.\r\n\r\nTo be more concrete: suppose there is a website that you visit every day and you read the headlines. However, you prefer to read these stories in your favorite RSS reader but this website does not provide an RSS feed. You can use these tools to create such a feed yourself.\r\n\r\nOr, suppose you want to buy a second-hand iBook from Craigslist and are not in a rush; you would rather wait for the right deal. Well, you could manually check the listings everyday but you would rather be alerted if an iBook comes up for sale within a certain distance from home and under a certain price. To do this programmatically, you need to grab the listings for computer ads and then you can parse them to see in any match your criteria. You can use these tools to create an API for Craigslist computer ads, under a search query of \"iBook\", in a matter of minutes.\r\n\r\nAnything that is presented on a webpage, certainly a static webpage, is something that could be scraped. The problem is that scraping is not easy. Quality of HTML structure varies considerably. While some sites always use valid W3C XHTML, use CSS with lots of descriptive class names and sensible divs that divide different sections of the page, many do not. Do a view source of a random <a href=\"http://profile.myspace.com/index.cfm?fuseaction=user.viewprofile&friendid=154222679&MyToken=a044c8f0-98ad-403d-bbb9-08528fe02798\">myspace page</a> and you will see what I mean: it is a huge mess.\r\n\r\nSo, to scrape a page when it does not have a clear or consistent structure, or to scrape a page that requires a login (say, your AOL buddy list) is not at all easy. It can be done but it takes time and, moreover, because one is scraping who knows how long that scraper will last. The content provider makes a change in page structure and your scraper has to be reworked. Finally, one has to be a coder: you need to use tools such as curl or php to get the page contents, you may need tools such as html parsers to navigate the content, and you may need competency in regex to pull out precisely what you need from the page. Finally, you may actually need a server to create a page to display the final, desired content; and not everyone has such resources. (We will see below how one  can dispense with this latter requirement.) Any tools that provide instant APIs from arbitrary webpages, especially for non-progammers, has to be good news.\r\n\r\nTwo organizations in this space who demoed at Mashup Camp are <a href=\"http://www.dappit.com/\">Dapper</a> and <a href=\"http://openkapow.com/\">OpenKapow</a>. Now that I've had a chance to play around with these technologies, I want to take this opportunity here to review them, giving an unbiased critique.\r\n\r\n<strong>Dapper</strong>\r\n\r\nDapper is a US startup based in Israel founded by Eran Shir and Jon Aizen and is currently in open beta. While all services are free at the moment, large projects or commercial uses of Dapper may be charged in the future. However, if Dapper works as well as it is claimed then I can see many organizations making good use of Dapper and saving in development costs even in a fee-based structure. Why do I say this? What precisely is Dapper?\r\n\r\nDapper is a web-based tool and service for scraping, creating APIs from potentially any webpage and then generating output of that API in a variety of formats including HTML, XML, RSS and even Google Maps.\r\n\r\nOne starts by entering a URL and the webpage is displayed within Dapper. One can click through different pages and add each page to a \"basket\". Thus, one could add say pages 1, 2, 3, 4 of a blog. Dapper then analyzes these pages to work out the structure, say what is a static header and footer and what is dynamic content. One then gets to \"play\". That is, clicking on a story title (should) highlights all other story titles in the page. As such, one is confident that Dapper will grab the correct content from the page when you specify precisely what you want your API, or \"Dapp\", should do.\r\n\r\nI have to say that I had mixed success with this. While Dapper correctly identified the story titles for techcrunch.com and http://www.followthemoney.org/Newsroom/index.phtml, it did not do so on sunlightfoundation.com and it could not work out my intention of grabbing rows or columns on http://opensecrets.org/orgs/list.asp?order=A (but it could get the org titles). However, this is still a beta and Dapper certainly does work: there are hundreds of user-created Dapps that one can browse and use.\r\n\r\nAfter playing, one can then progress to creating the API proper: one clicks on a desired element, gives it a name and one can then define a group, e.g. specify that this story title, this author name and this number of diggs are all related as one unit. After that, one can preview the API, i.e. check what is pulled out by the API. If all is well, one is done. Then the real fun begins...\r\n\r\nDapper provides the API in a impressive variety of formats: XML, HTML, RSS, Alerts, iCalendar (transforms the output of a Dapp into an iCalendar which can be used in Google Calendar, Sunbird, iCal, and other programs), Google maps (places locations directly onto a map), Google gadgets, Netvibes, image loop, email, link to another Dapp, CSV, JSON, YAML, XSL, and fork it as another Dapp. This APIs are published and hosted at Dapper (hence one doesn't need their own server to provide an API). Dapper also allows one to define parameters for the API such as a {query} or {page}.\r\n\r\nSome of the resultant URLs are mess though. So, Dapp allows one to define a service that provides a nice clean URL. That is, instead of say http://www.dappit.com/RunDapp?dappName=sunlightlabs&v=1&thisparam=y&thatparam=z... one can instead provide users with say http://www.dappit.com/services/sunlightlabs.\r\n\r\nAnother nice feature is their AggregatorAid that allows one to pool Dapps into one single service. Thus, if one has a suite of different individual search Dapps (Google, digg, reddit, etc) one can provide a single query that will aggregate the results as a single API. The only restriction is that each that individual Dapp must have the same query parameter name (I hope that this is relaxed in later versions).\r\n\r\nFinally, Dapper provides secure, authenticated login into sites. Thus, if you want to scrape your AOL buddies or comments on your Facebook page, Dapper allows you to create an API without exposing your username or password.\r\n\r\nOverall, Dapper is a clean, slick instant API generator that has some very nice features: it is web-based, has a clean interface, and provides API output in the majority of formats that anyone would want. It is beta, it is not perfect, but then it is free. One really can get a basic API scraped from a page, published, and up and running in literally 5 minutes. This is why I say that I can see a valid business model here. Organizations, especially non-profits such as ourselves, can try Dapper first to create a given API, and only if it doesn't work then create the API from scratch or outsource which will certainly be more time consuming and expensive.\r\n\r\n<strong>OpenKapow</strong>\r\n\r\nOpen Kapow, also in beta, founded by  \t      Stefan Andreasen in Denmark, is part of the larger Kapow Techologies which appears to have a large range of corporate clients and a number offices on both sides of \"the pond\".\r\n\r\nOpenKapow has the same goal of instant APIs as Dapper but takes a different approach. For a start, while Dapper is web-based, OpenKapow requires a large \"RoboMaker\" download (100+ Mb) for windows or linux. [So, as a mac guy that cuts me out. If one takes a look around at any hacker or mashup conference you will find a sea of macs, and these are the guys who are most likely to do such mashups. While macs are UNIX underneath the linux version does not install on macs.]\r\nInstalling the RoboMaker on windows, one is presented with a sophisticated Java-based tool for mashups. As in Dapper, a webpage is loaded and displayed within the tool and one can select various elements of the page. RoboMaker has a nice DOM JTreeView that one can explore the structure of the page and one can click various elements and they will be highlighted and at the same time, the selected item is also shown in an HTML viewer. Together, the three panels provide a better understanding of the page structure than Dapper.\r\nOne the right are a series of controls for selecting tags based on name, type, conditions etc. There really are a very large number of features and controls that (I would image) provide a very granular approach to scraping. Therein, however, lies the problem.\r\n\r\nThere were so many controls, so many right click context menus, that it was not at all obvious for the newbie where to begin. I failed to select all the h1 tags of the page despite trying various combinations of select *.h1 etc. I saw OpenKapow in action at mashupcamp and I did see Stefan demo this creating an API for Google search. It looked very easy...if you know what you are doing. I really do believe that OpenKapow is a more sophisticated tool that probably can deal with edge cases that Dapper cannot. It is a tool for hackers/programmers who must invest some time into understanding the tool (and getting the thing downloaded and installed) but it is certainly not a quick and easy hacker tool. While both will generate APIs without true coding, I feel that Dapper is an quick-and-dirty, intuitive 90% solution while OpenKapow is a 90%+ tool for more serious projects.\r\n\r\nPerhaps I have a short attention span but I have to admit that I gave up trying to get something basic working on OpenKapow. Besides, even if i did get it working, the output formats for OpenKapow are far more limited: (X)HTML, XML, REST, and RSS.\r\n\r\n<strong>Conclusions </strong>\r\n\r\nDapper especially allows one to try a quick hack. If that doesn't work then one could resort to writing it oneself or using a tool such as OpenKapow. OpenKapow looks as though it would be worth the effort to invest in learning all of the features if one was going to be doing lots of scraping and mashups, or in a corporate environment where one needed a very granular mashup that needed to work all the time. I think that <em>both</em> of these represent an exciting new era for mashups. They both represent programming-free tools for instant, shareable APIs. They both involve a reasonable GUI to select elements rather than needing to poke around the underlying HTML itself. As such, the bar for mashing up is significantly lowered.", "date_published": "2007-02-01 19:37:36", "comment_count": 0, "slug": "instant-apis-no-code-necessary-just-add-mouseclicks", "tags": ""}}, {"pk": 4, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 2, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Why offline is the new online", "excerpt": "The history of computing has some interesting trends. First it was all about mainframes. To get anything computed you had to logon to some remote machine so large that if filled a room. Then came along the desktop; desktop apps were (and arguably still are) it. In the 1990's, however, things got pushed onto the web, web <em>sites</em> were the thing, and now with the rise of Ajax, web <em>apps</em> are becoming the new it.", "content": "<img width=\"113\" height=\"79\" alt=\"apollo logo\" title=\"apollo logo\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2007/01/apollologo.jpg\" />The history of computing has some interesting trends. First it was all about mainframes. To get anything computed you had to logon to some remote machine so large that if filled a room. Then came along the desktop; desktop apps were (and arguably still are) it. In the 1990's, however, things got pushed onto the web, web <em>sites</em> were the thing, and now with the rise of Ajax, web <em>apps</em> are becoming the new it. Applications such as <a href=\"http://www.google.com/support/writely/\">writely</a> (now \"google docs\") or <a href=\"http://www.google.com/googlespreadsheets/tour1.html\">google spreadsheets</a> that have the look, feel and responsiveness of traditional desktop applications.  I want to predict the next segment of this zig-zag computing trend: offline web apps on your desktop.\r\n\r\nIn the last couple of months there have been some very interesting developments. Adobe have staked their claim in this area with the announcement of <a href=\"http://labs.adobe.com/wiki/index.php/Apollo\">Apollo</a>, Mozilla is already in this arena with <a href=\"http://developer.mozilla.org/en/docs/XULRunner\">xulrunner</a>, and recently <a href=\"http://www.sitepen.com/blog/2007/01/02/the-dojo-offline-toolkit/\">dojo</a> announced a forthcoming offline library. Before we get into the specifics though, I want to dig a little deeper into some background and discuss the influence of widgets.\r\n\r\nWidgets are really hot now, small focussed applications that serve one or two useful purposes: show the local weather, a simple RSS reader, show what is playing on iTunes etc. The problem is that they are tied to particular browser\u2014<a href=\"http://widgets.opera.com/\">opera widgets</a> only work for the opera browser\u2014or tied to a particular operating system\u2014apple's <a href=\"http://www.apple.com/macosx/features/dashboard/\">dashboard widgets</a> only work on a mac\u2014or they must be served through a third party host (e.g. <a href=\"http://www.widgetbox.com/\">widgetbox</a>). This sucks for all involved: developers always want to write once, run anywhere but instead they have to write and maintain slightly different versions for lots of different platforms. Users don't want to search through some complicated table to work out which one they need to download. What we all want is a universal \"click to download and install\" widget, that does what it says, and that does what it says in the same way on all platforms with the same code. We want a simple app that will float on the window, do its thing.\r\n\r\nDesktop app are tied to native libraries for good reason: performance. Fast, efficient Cocoa apps for mac, .net for for windows etc. While Java claims to be universal, it is very slow to launch, there are still UI inconsistencies across platforms, and it can never reach the same degree of performance as native apps for compute-intense apps such as rendering and video CODECs. Widgets are different however. Their functions are typically so simple that it makes no difference. To grab and display the latest blog post entry's title from your favorite blog is hardly going to stress the CPU. So, they can be written in universal language such as HTML+CSS+JS.\r\nThe stupid thing is that widgets are all written in the same language: HTML+CSS+JS. Opera widgets and Apple's dashboard widgets are essentially the same under the hood but they just cannot run on each other platforms. Within 6 months we are, however, going to see more universal widgets and desktop apps written in web technology.\r\nAdobe is starting to promote its forthcoming Apollo platform. Apollo will provide libraries for offline storage, will have the ability to read and write to the desktop's file system (which is difficult to achieve in true web apps because of the built in security model), and will have libraries for flash, HTML and PDFs. So to make sure we are clear here: you will be able to write a web app in HTML and have it run universally on all platforms: windows, mac and linux, and further, the HTML can be rendered through the flash viewer so that it is virtually guaranteed to look and operate exactly the same on all of those platforms. As a developer, one can write once, make it generally available for download as an .air file, have it work on all platforms. Not only that but Apollo apps can continuously check for connectivity: you shut down your computer because you are getting onto a plane, no problem. After take off, you open it up and continue as before. Moreover, all of these offline activities or transactions are stacked up locally and processed online as soon as connectivity is restored. You can continue to work offline and it will seamlessly sync to online when it can. Oh, and did I say all of this will be free? [They will make their money through selling more Adobe Flex licenses to developers.] Apollo will be available later this year but the main feature they need to fix is distribution. Currently, one has to download an Apollo runtime and an apollo installer. This is not likely to engender uptake by the mainstream Joe Doe end user. However, unsurprisingly, they do have plans to distribute through the flash player so the back end installation will be seamless.\r\n\r\nMozilla already in this space through xulrunner. So, \"xulrunner --install -app gmail\" is all that is needed to create a desktop app for gmail. This will provide a local executable that will launch a window to gmail. This can be running independently of any browser. This is from Mozilla so of courser there are <a href=\"http://www.mozilla.com/en-US/firefox/\">Firefox</a> libraries running behind the scenes but the point is that I can have a persistent desktop application running up in say my top right corner of my screen, always there, always keeping me informed of whatever I want to keep informed about, without me having to have firefox or any other browser running and have that window or tab showing. This too will allow developers to write desktop applications in HTML+CSS+JS that run offline. Given the number of developers who know these technologies, this could be a huge boon for desktop app innovation. Again, this allows a write once run anywhere [but with less guarantees about consistent look and feel] and has local SQLlite database for local storage.\r\nDojo too is likely to be another important player. They are working on an <a href=\"http://www.sitepen.com/blog/2007/01/02/the-dojo-offline-toolkit/\">offline toolkit</a>, a \"small, cross-platform, generic       download that enables web applications to work offline.\" It will be cross platform, cross-browser, connectivity detecting, secure, with local storage. Sound familiar?\r\n\r\nLet me stress that one is not going to be able to take an existing complex, server-side PHP+MySQL web application, type a single command and have it instantly available as a desktop application. (Besides, would really trust your code, even if encrypted, out there on everyone's desktop?) However, this is an exciting area for web-innovation. Watch this space in the next six months. More and more of the web will be seeping onto your desktop. This is why I say offline is the new online.", "date_published": "2007-01-21 15:57:21", "comment_count": 0, "slug": "why-offline-is-the-new-online", "tags": ""}}, {"pk": 3, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 2, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "The convergence of Ajax and Flash", "excerpt": "While reading a couple of posts in Ajaxian about  <a href=\"http://ajaxian.com/archives/jsflickrslideshow-sliding-through-flickr\">hacking the canvas tag with a flickr stream</a> and a <a href=\"http://ajaxian.com/archives/smooth-gallery-10-released\">mootools-based image gallery stream</a> it struck me just how sophisticated some Ajax-based UIs have become, so much so that it has become harder to tell what is and is not flash.", "content": "<img width=\"184\" height=\"134\" align=\"top\" alt=\"smooth gallery\" title=\"smooth gallery\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2007/01/smoothgallery10.jpg\" />\r\n\r\nWhile reading a couple of posts in Ajaxian about  <a href=\"http://ajaxian.com/archives/jsflickrslideshow-sliding-through-flickr\">hacking the canvas tag with a flickr stream</a> and a <a href=\"http://ajaxian.com/archives/smooth-gallery-10-released\">mootools-based image gallery stream</a> it struck me just how sophisticated some Ajax-based UIs have become, so much so that it has become harder to tell what is and is not flash.\r\n\r\nThat is, not so long ago, flash was the only viable option for a variety of interactivity including drag and drop and sophisticated animation. However, over the last year a slew of Ajax libraries have been released such as <a href=\"http://dojotoolkit.org/\">dojo</a>, <a href=\"http://developer.yahoo.com/yui/\">Yahoo! UI</a>, <a href=\"http://en.wikipedia.org/wiki/Canvas_(HTML_element)\">canvas tag</a> has spread, especially with the release of <a href=\"http://excanvas.sourceforge.net/\">excanvas</a> that hacks this SVG type functionality for internet explorer. Now one can draw and animate curves in DHTML.\r\n\r\nNow don't get me wrong, flash is a great innovative, product (sometimes abused where the whole website is flash and you cannot link to anything) has a very thin client, and most importantly looks and feels the same on different browsers (without all those frustrating conditional CSS IE hacks). For this reason it will hold a strong position for a while to come yet, but I do feel that we will continue to see increasing innovation in Ajax libraries and a convergence of DHTML websites to have a flash-like look and feel.", "date_published": "2007-01-08 17:54:12", "comment_count": 0, "slug": "the-convergence-of-ajax-and-flash", "tags": ""}}, {"pk": 1, "model": "blogdor.post", "fields": {"status": "draft", "last_updated": "2008-12-04 17:56:22", "author": 2, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Cool. Sunlight Now in Second Life (Thanks to an API...)", "excerpt": "", "content": "<!-- start content_top block if on homepage -->\r\n<!-- end content_top -->\r\n<!-- start main content -->\r\n<!-- begin content -->\r\n<div class=\"node\">\r\n<div class=\"blogcontent\"><object width=\"425\" height=\"350\">\r\n<param value=\"http://www.youtube.com/v/8FxlGNA5Q-I\" name=\"movie\" />\r\n<param value=\"transparent\" name=\"wmode\" /></object>Here is a cool development. Steve Nelson is displaying information on members of Congress inside Second Life (<a href=\"http://slurl.com/secondlife/Capitol%20Hill%201/173/17/30/\">SLurl location</a>) using the Sunlight Labs's still-in-beta API (Application Programming Interface).\r\nSteve is entering his \"U.S. House of Representatives Info Center\" in <a href=\"http://sunlightfoundation.com/mashup\">Sunlight's first Web 2.0 Mash-Up contest</a> (deadline April 15). I'm blogging it  because it so wonderfully illustrates the phrase Sun Microsystems, Inc.'s CEO Jonathan Schwartz used to explain why Sun, a publically traded Fortune 500 company, decided to embraced open source: \"Openness is an accelerant.\"  <img src=\"/files/slch-new-med.jpg\" />  <em>Openness is an accelerant.</em> None of us in the Labs know the first thing about programming Second Life. Yet, because we have an open web service API for certain data on Congressional Representatives at <a href=\"http://sunlightlabs.com/api/\">SunlightlLabs.com/API</a>, someone else could.  As the screen shot shows, Second Life members approach Steve's Info Center and then type their zip code into chat. Steve  programmed his Info Center to go over the web to SunlightLabs's API and fetch back a photo of the appropriate representative and links to related pages on different accountability web sites.\r\nOur openness accelerated Steve's bringing data we compiled to new users in a new context with new tools. Regardless of your opinion of 3D worlds, <em>how cool is that?</em>  To get a bit more geeky, two types of openness enable Steve's Info Center Mash-Up: 1) the openness of web service APIs and 2) the openness Second Life provides to its \"citizens\" to create virtual objects.\r\nSunlight Labs rockstar Dr. Carl Anderson created a web service API to a database that cross references zip codes with congressional districts with the IDs different web sites use to use publish dynamic content on members of Congress from their particular databases. Having an API means other developers -- like Steve Nelson -- can access the cross-referencing we've already done directly from their computer programs thereby accelerating their ability to make new applications with this data. Our API open goodness is then paired with the openness of Second Life's platform that allows members to create and populate its virtual world with structures and objects much the same way AOL's members created and populated AOL with community bulletin boards and chat rooms.\r\nSteve created his interactive congressional display within SL's Capitol Hill, a place for political information he co-created with others. SL's openness accelerates its development by its members.  (To experience a different but equally cool Web 2.0 mash-up of congressional data in your standard browser, see <a href=\"http://www.tetonpost.com/citycon\">www.tetonpost.com/citycon</a>.)  If you are citizen of Second Life, <a href=\"http://slurl.com/secondlife/Capitol%20Hill%201/173/17/30/\">visit Steve's Info Center</a>. If you are a developer -- Second Life or Web 2.0 -- use openness as an accelerant to your own ideas for mashing-up congressional information and enter our <a href=\"http://sunlightfoundation.com/mashup\">Mash Up Contest</a> before the April 15, 2007 deadline for chance to win $2,000. We are eager to see what's next!</div>\r\n</div>", "date_published": null, "comment_count": 0, "slug": "", "tags": ""}}, {"pk": 2, "model": "blogdor.post", "fields": {"status": "draft", "last_updated": "2008-12-04 17:56:22", "author": 3, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Machine Found Data vs Human Entered Data", "excerpt": "", "content": "Friends, developers, NGOs, lend me your eyes;\r\nI come to explain data visualizations, not to praise them.\r\nThe evil that men do lives after them;\r\nThe data can oft  be inferred from their documents.\r\n-- Mark Anthony, if he did data visualizations\r\n\r\nThis post considers if the current state of technology makes \"machine found data\" more useful and economical over \"human entered data\" and if machine found data is a better bet for developers and non-profits to place their efforts for the immediate future. (Yes, it probably is, for those not wanting to read the entire post.)\r\n\r\nMost of the data visualizations I have seen lately have the following in common:\r\n* uses machine found data or data received from another party\r\n* large sample set\r\n* frequently includes time\r\n* data extracted from other occurring activities\r\n* trends rather than search\r\n\r\nWhen I think of the datasets which non-profits in the government oversight and money and politics arena, I see the following in common:\r\n* human entered data or data requiring human standardization\r\n* small to medium sample sets\r\n* linear aggregations\r\n* duration aggregation but little trend analysis\r\n* data created specifically for searching\r\n\r\nMy hypothesis is the most significant determinant of creating data visualizations is the choice of whether to work with machine found data or to work with human entered data. I believe the affordances and mindset of working with machine found data tends to lead toward meaningful data visualizations while working with human entered data tends lead away from data visualization and toward anecdotal ___\r\n\r\n\r\n\r\n\r\nI've been thinking about this for a while, but two visualizations on the Dow Jones Insight blog (via O'Reilly Blog) set me to writing.  Both visualizations count words in documents to make pretty pictures. Or to say it more technically: Both visualizations use machine-based textual analysis to extract information for statistical comparisons. \r\n\r\nthat use textual analysis of media coverage to generate a statistical analysis that can be visualized. In other words, Dow Jones counts words to make pretty pictures. \r\n\r\nCounting words is a great way to\r\n\r\n\r\n\r\n\r\n. The first visualizes candidate speeches. The second visualizes press sentiment toward candidates. Both are examples of what I would describe as machine found data. Machine found data is structured data generated by a computer from existing digital content. Read this little tidbit from the Down Jones post:\r\n\r\n\"The system considered 65,374 press documents and found 26,435 of them to contain either favorable or unfavorable language dominating in reference to a candidate.\"\r\n\r\n\r\n\r\n\r\n\r\n", "date_published": null, "comment_count": 0, "slug": "", "tags": ""}}, {"pk": 35, "model": "blogdor.post", "fields": {"status": "draft", "last_updated": "2009-01-09 18:02:19", "author": 3, "timestamp": "2009-01-09 18:02:19", "markup": "none", "title": "Weekly Lab Report Wrap-Up", "excerpt": "Test test", "content": "<u>GENERAL NEWS</u>\r\n\r\n<b>Faster Memcache! Kill! Kill!</b> Another nail in the traditional relational databases coffin...the powerful, scalable tool  MemcacheDB is now available. W0ot!\r\n\r\n<b>We Are All Mario.</b> Clay Johnson discovered World9's Mario-sound effect and happily bounced around the office. \r\n\r\n<b>NYT API 4 MoC.</b> NYTimes releases a ... wait for it ... API for Members of Congress. Very cool. Terms of Service with a low-five figure call limit, less cool. Sunlight's Labs API ninja James Turk reviews.\r\n\r\n<b>New Congress, New Data. No Problem.</b> Sunlight Labs gets updated in a single day with the new members of Congress thanks to James' work tying the Labs API to authoritative sources.\r\n\r\n<b>Tweet-of-the-Week</b>\r\n\r\n<u>Maven Tim's Restaurant</u>\r\nChina Lee, Silver Spring Maryland. As promised, great Dim Sum served in the afternoon. Waited an hour to be seated last Sunday.\r\n\r\n\r\n\r\n", "date_published": null, "comment_count": 0, "slug": "weekly-lab-report-01-09-09", "tags": ""}}, {"pk": 36, "model": "blogdor.post", "fields": {"status": "draft", "last_updated": "2009-01-10 15:21:25", "author": 6, "timestamp": "2009-01-10 15:21:25", "markup": "markdown", "title": "Fixing Your Seat at the Table", "excerpt": "", "content": "Early on in the Presidential Transition, [http://change.gov](Change.gov) announced an incredibly compelling, never-before-done process: [http://change.gov/open_government/yourseatatthetable](Your Seat at the Table). They announced that every document that the transition team received in a meeting where there was three or more attendees would be posted online.\r\n\r\nLet me be clear: that was an *awesome* step and the change.gov team should be commended for taking it.\r\n\r\nThat being said it looks like the change.gov team is learning as they go.  I respect that: being the first people to try this stuff means you've got to. The process has been less than perfect, and I hope what they're learning as a community is that being transparent requires a lot of work, and often opens the door to a lot more requests. I cannot imagine the time and expense they've occurred building such a project for the American people. Here's some things I've noticed.\r\n\r\nAs of today:\r\n1.  Weeks of documents are missing\r\n2.  When you click on \"page 2\" of the [http://change.gov/open_government/yourseatatthetable](main page) you receive an error; [http://change.gov/S=5ca7c563bed3878feb30bbec035a51e83fd3a690/open_government/yourseatatthetable/P10/](The page you requested is not available right now.)\r\n3. Several of the documents on the front page have dates in the future. \r\n4. You can only search once every 15 seconds. \r\n\r\nI'm *certain* that there's a strong amount of effort being put on this by the administration and they're taking it very seriously, but I suspect that it will never be good enough. Transparency's a big job!\r\n\r\nWhat's interesting though, is that there's probably a much easier solution for them that would require a lot less work: \r\n\r\n1. Standardize the names of the documents, and put them on an FTP server or an index-free http page somewhere \r\n2. update that FTP server when documents come in and are sanitized. \r\n3. write a blog post telling us it is there. In the blog post, say \"people are encouraged to build their own search interfaces and browsing interfaces to the data as they see fit\" \r\n\r\nThe result? Instead of having a functionally broken [http://change.gov/open_government/yourseatatthetable](page) you'll likely have the [http://sunlightlabs.com](Sunlight Labs) scrambling to build an interface, [http://watchdog.net](Watchdog.net) parsing the data and incorporating it, and media organizations like the New York Times working on their own too. Plus, you'd earn praise from the tech community for being participatory and more open.\r\n\r\nThe point being: This transparency business gets really tough if you make it tough, but if you're looking for the easy way out, there's likely a much simpler solution. And often the simpler ones end up being more sophisticated and impressive anyhow.\r\n\r\n", "date_published": null, "comment_count": 0, "slug": "seat_at_the_table", "tags": ""}}, {"pk": 51, "model": "blogdor.post", "fields": {"status": "draft", "last_updated": "2009-02-02 10:36:34", "author": 6, "timestamp": "2009-02-02 10:36:34", "markup": "markdown", "title": "Legislative Transparency", "excerpt": "", "content": "We work hard to make congressional data more transparent and available to the world, and often times we forget how good we've got it. Because if you look towards many other legislative bodies, they aren't doing nearly as good of a job as the U.S. Congress.\r\n\r\nState legislatures have become quite a passion of mine lately. A lot of the legislation passed that truly effects our lives is passed not at the federal level but at the state and local level, and wouldn't it be great to be able to see that?\r\n\r\nI've started building the [http://wiki.sunlightlabs.com/index.php/State_Legislation_Page](State Legislation Page) and so far I've added about 8 states. Others have added a few more taking us to 15. ", "date_published": null, "comment_count": 0, "slug": "legislative-transparency", "tags": ""}}, {"pk": 87, "model": "blogdor.post", "fields": {"status": "draft", "last_updated": "2009-04-21 16:40:03", "author": 6, "timestamp": "2009-04-21 16:40:03", "markup": "none", "title": "The Scores from Apps for America", "excerpt": "", "content": "<br/><div><small><a href=\"http://app.blist.com/?exp1=v1#/blist/cjoh/Base-Apps-for-America-Scores\" style=\"font-size:12pt;font-weight:bold;text-decoration:none;color:#000000;\">Base Apps for America Scores</a></small><br/><iframe width=\"425px\" height=\"344px\" src=\"http://app.blist.com/widgets/270818?width=425px&height=344px&exp1=v1\" frameborder=\"0\" scrolling=\"no\"><a href=\"http://app.blist.com/?exp1=v1#/blist/cjoh/Base-Apps-for-America-Scores\" title=\"Base Apps for America Scores\">Base Apps for America Scores</a></iframe><br/><small><a href=\"http://www.blist.com/?exp1=v1\">Get your own blist widget</a></small></div><br/>", "date_published": null, "comment_count": 0, "slug": "scores-apps-america", "tags": ""}}, {"pk": 107, "model": "blogdor.post", "fields": {"status": "draft", "last_updated": "2009-06-02 12:19:55", "author": 8, "timestamp": "2009-06-02 12:19:55", "markup": "markdown", "title": "Redesigning the Government: U.S. Supreme Court", "excerpt": "<img id=\"court_introPic\" alt=\"Introduction Image\" src=\"http://assets.sunlightlabs.com/site/images/supremeCourt_intro.jpg\"/>\r\n\r\nPresident Obama's nomination of Judge Sotomayor has brought increased attention to the U.S. Supreme Court. It also has led us to reexamine the Court's <a href=\"http://supremecourtus.gov/\">web site</a>, which is long overdue for an overhaul. In its current form, its web design is suggestive of the 1990s, and its functionality is similarly dated.\r\n\r\nThe Justices appear to <a href=\"http://appropriations.house.gov/witness_testimony/FS/SupremeCourt_Testimony_04_23_09.pdf\">agree</a>. They've recently ask Congress for money to move control of the site in-house, taking over responsibility from the GPO. This move would allow them, in their words, to \"better control and manage the web site and to be able to expand the data and services provided by the site more efficiently.\"", "content": "<img id=\"court_introPic\" alt=\"Introduction Image\" src=\"http://assets.sunlightlabs.com/site/images/supremeCourt_intro.jpg\"/>\r\n\r\nPresident Obama's nomination of Judge Sotomayor has brought increased attention to the U.S. Supreme Court. It also has led us to reexamine the Court's <a href=\"http://supremecourtus.gov/\">web site</a>, which is long overdue for an overhaul. In its current form, its web design is suggestive of the 1990s, and its functionality is similarly dated.\r\n\r\nThe Justices appear to <a href=\"http://appropriations.house.gov/witness_testimony/FS/SupremeCourt_Testimony_04_23_09.pdf\">agree</a>. They've recently ask Congress for money to move control of the site in-house, taking over responsibility from the GPO. This move would allow them, in their words, to \"better control and manage the web site and to be able to expand the data and services provided by the site more efficiently.\"\r\n\r\nThe current web site has many shortcomings. It doesn't contain briefs by the parties and omits all but a few relatively recent Court opinions. Its navigation is a nightmare and its design fails to incorporate modern techniques such as RSS feeds and XML. Much information is unnecessarily locked in PDFs. And yet, in January 2009 the nine-year-old site received 18 million hits.\r\n\r\nTo help the Court update its web presence, the Sunlight Foundation has put together the following mock-up.\r\n\r\nThe most important aspect of the mock-up is that it takes into account the web site's diverse users. It accommodates the general public and students, legal researchers, court researchers, and litigants. Accordingly, we believe the redesigned web site must be simple, straightforward, and robust. It must strive to make the Court's proceedings transparent, incorporate modern design principles, and meet the higher expectations of today's web user.\r\n\r\nThis post is the next in a series of government web site mock-ups that suggests how parts of the government should transform their online presence. Previous iterations have included: <a href=\"http://www.sunlightlabs.com/blog/2009/01/23/rethinking_usagov/\">USA.gov</a>, <a href=\"http://www.sunlightlabs.com/blog/2009/02/04/redesigning-government-fec/\">FEC.gov</a>, <a href=\"http://www.sunlightlabs.com/blog/2009/03/23/redesigning-government-epa/\">EPA.gov</a>, and <a href=\"http://www.sunlightlabs.com/blog/2009/04/16/redesigning-government-datagov/\">Data.gov</a>.\r\n\r\nUnder the fold, we have the mock-up and detailed descriptions of how the Supreme Court web site should be redesigned.\r\n\r\n<strong>Home Page</strong>\r\n\r\nThe home page must be accessible to everyone. It should allow for quick and easy searches, identify the Justices, and include content that quickly and readily engages someone's interest. The current site, by contrast, is simply a picture and a list of links.\r\n\r\nWe've added search box at the top of the page to serve as the main source of navigation. Also at the top of the page is recent news, which allows users to get a quick glimpse of anything new.\r\n\r\nIn addition to the search box, we've included a small graphic and link to the Supreme Court's calendar. This way, viewers can get a quick glance at the schedule of upcoming argument while being able to click on links for more information.\r\n\r\nAnother feature is the recent decision section. It contains links to the full decision, the details of the case, and any time another law is referred to. This allows users to dig as deep as they'd like so as to get the full picture of what happened.\r\n\r\nOne small but important detail include adding an RSS feed to the recent decisions. The RSS feed would permit users to access the information that they want through an RSS reader without having to come back to the site everyday.\r\n\r\nAt the bottom of the page is basic information about the Justices, which also serves as a gateway for more information about the Court.\r\n\r\nClick on the images for a larger view.\r\n\r\n<a href=\"http://supremecourtus.gov/\"><img  class=\"court_pic\" alt=\"Current Supreme Court Home Page Picture\" src=\"http://assets.sunlightlabs.com/site/images/supremeCourt_homeOldSmlabs.jpg\"/></a>\r\n\r\n<a href=\"http://sunlightlabs.com/images/supremeCourt_home.jpg\"><img class=\"court_pic2\" alt=\"Sunlight Mock-up Home Page Picture\" src=\"http://assets.sunlightlabs.com/site/images/supremeCourt_homeSmlabs.jpg\"/></a>\r\n\r\n\r\n<strong>About the Court</strong>\r\n\r\nThe \"About the Court\" page should provide a good overview for people who aren't that familiar with the Supreme Court. Much of this information is already available, but is very difficult to find or view. Often, the data can only be found in a PDF.\r\n\r\nBy adding visuals to a page, users immediately grasp what's happening even if they're simply scanning the page. We also are again showing snippets of content to allow users to be able to scan the various content and choose to click through if they want more information.\r\n\r\nThis is exemplified by brief snippets of information about each of the Justices (including all former Justices), and a graphic representation of where cases originate by circuit.\r\n\r\n<a href=\"http://supremecourtus.gov/about/about.html\"><img class=\"court_pic\" alt=\"Current Supreme Court About The Court Page Picture\" src=\"http://assets.sunlightlabs.com/site/images/supremeCourt_aboutOldSmlabs.jpg\"/></a>\r\n\r\n<a href=\"http://sunlightlabs.com/images/supremeCourt_about.jpg\"><img class=\"court_pic2\" alt=\"Sunlight Mock-up About The Court Page Picture\" src=\"http://assets.sunlightlabs.com/site/images/supremeCourt_aboutSmlabs.jpg\"/></a>\r\n\r\n<strong>Proceedings: Landing Page</strong>\r\n\r\nThe mock-up gives users the ability to search through all proceedings by term, by name, by justice, by topic, and so on, all of which is unavailable on the current web site. By giving users this tool, they can both browse through cases and immediately identify the specific case they need.\r\n\r\nWe also break out the proceedings so that users have all the holdings front and center, with links to the full decision and the details of the case. There is an RSS feed so that users can be notified the moment a new decision is released.\r\n\r\nWe've also added a database that contains how the Justices voted in each case. The underlying voting pattern data are exportable for scholarly analysis.\r\n\r\n<a href=\"http://supremecourtus.gov/opinions/08slipopinion.html\"><img class=\"court_pic\" alt=\"Current Supreme Court Proceedings Page Picture\" src=\"http://assets.sunlightlabs.com/site/images/supremeCourt_proceedingsOldSmlabs.jpg\"/></a>\r\n\r\n<a href=\"http://sunlightlabs.com/images/supremeCourt_procedings.jpg\"><img class=\"court_pic2\" alt=\"Sunlight Mock-up Proceedings Page Picture\" src=\"http://assets.sunlightlabs.com/site/images/supremeCourt_proceedingsSmlabs.jpg\"/></a>\r\n\r\n<strong>Proceedings: Case by Case</strong>\r\n\r\nThe top of the page contains basic information about the case -- its docket number, the question presented, the parties, and its subject matter. In addition, there's a handy status box containing information on how the case was decided, when it is scheduled for argument (and a link to a transcript and audio/video), its citation, and the ability to check to see whether the decision is still good law.\r\n\r\nThe bulk of the page is occupied by the case's proceedings and orders. Organized chronologically -- from petition for a writ of <em>certiorari</em> to final decision -- each action is cataloged and summarized. We suggest that the Court adopt the practice of uploading the briefs from each party, and add links to each filing or order as they occur. In addition, we've added an RSS feed, so that users can monitor the case without having to view the page.\r\n\r\nThe bottom of the page contains the case's procedural posture. It includes links to documents containing lower court filings and a description of the disposition of the case. This information is omitted from the current Supreme Court web site, but is very useful in helping understand the background underlying the Court's decision.\r\n\r\nWe suggest that there be several URLs for each case page. Both the docket number (e.g. http://www.supremecourtus.gov/case/2008-108) and Supreme Court Reporter citation (http://www.supremecourtus.gov/case/556US103) should link to the same page. This way, people can build hyperlinks to cases using readily available public information.\r\n\r\n<a href=\"http://origin.www.supremecourtus.gov/docket/08-108.htm\"><img class=\"court_pic\" alt=\"Current Supreme Court Case Page Picture\" src=\"http://assets.sunlightlabs.com/site/images/supremeCourt_caseOldSmlabs.jpg\"/></a>\r\n\r\n<a href=\"http://sunlightlabs.com/images/supremeCourt_case.jpg\"><img class=\"court_pic2\" alt=\"Sunlight Mock-up Case Page Picture\" src=\"http://assets.sunlightlabs.com/site/images/supremeCourt_caseSmlabs.jpg\"/></a>\r\n\r\n<strong>Court Procedure</strong>\r\n\r\nWe did not mock-up the court procedure web page (or the following pages), but we envision the court procedure page's contents as focusing on parties before the Court. It should contain the Courts' Rules of Procedure, a guide for counsel arguing before the Court, how to deliver documents to the Clerk (including a way to e-file), bar admissions applications, and a list of all counsel who have argued before the Court.\r\n\r\n<strong>Calendar</strong>\r\n\r\nEach year the Supreme Court makes available a calendar of its argument and conference schedule. We suggest translating this calendar from PDF to html/XML, and placing it online. In other words, users will be able to see on the web page the Court's argument calendar, and by hovering their mouse over (or click on) a particular date, to see what cases are scheduled for that day. All previous calendars should be available here as well.\r\n\r\n<strong>Visiting the Court</strong>\r\n\r\nThis page contains basic information about visiting the court. It should include a map, hours of operation, tour information, argument schedules, etc. It could also include a virtual tour of the building as well.\r\n\r\n<strong>Blog</strong>\r\n\r\nThe Supreme Court is the only branch of government that does not have a blog, which is ironic since so much of what the Court does lends itself to written form. Indeed, because the Court communicates in writing, the best way for the public to understand what the Court is doing is to read its orders and opinions. The blog can serve as a gateway to this information.\r\n\r\nAt a minimum, the Court's blog should include contemporaneous summaries of and links to decisions, highlight and link to speeches given by the Justices, promote press releases and media advisories, and identify other major reports (such as the year-end report on the federal judiciary).\r\n\r\nA more ambitious goal would be to hire a reporter to cover the Court from the inside out. For example, the reporter could provide information about the history of the Court, the people who make it function, coverage of oral arguments (or even audio or video), etc. Having a reporter on the inside would help demystify what the Court does and provide additional context for its actions.\r\n\r\n<strong>Contact</strong>\r\n\r\nThis page would contain basic information on how to contact the Court, and would link to information about visiting the Court.\r\n\r\n<strong>Conclusion</strong>\r\n\r\nOf course, this is just our view on how the Supreme Court web site should look. Tell us yours in the comments. Ultimately, we will present these finding to the Supreme Court for its use.\r\n\r\nIf you have any questions, don't hesitate to contact us directly.\r\n\r\n<a href=\"http://www.sunlightfoundation.com/people/afelski/\">Ali Felski</a> (Sunlight Senior Labs Designer)\r\n<a href=\"http://www.sunlightfoundation.com/people/dschuman/\">Daniel Schuman</a> (Policy Counsel)\r\n\r\nOne final note: this pages are intended as mock-ups, and don't necessarily reflect how the Court has ruled in any particular case.", "date_published": null, "comment_count": 0, "slug": "redesigning-government-us-supreme-court", "tags": ""}}]
