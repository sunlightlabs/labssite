[{"pk": 58, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-02-09 12:49:59", "author": 6, "timestamp": "2009-02-09 12:49:59", "markup": "markdown", "title": "I heart Bit.ly", "excerpt": "We've been using [Bit.ly](http://bit.ly) a lot here at Sunlight Labs-- I've become somewhat of an evangelist of using it over many similar shortcut services. What gets me excited about [bit.ly](http://bit.ly) is the advanced tracking capabilities it has. Did you know....\r\n\r\n1. Bit.ly tracks [friendfeed](http://friendfeed.com) and [twitter](http://www.twitter.com) conversations?\r\n2. It tracks each individual click and tells you where they're coming from? \r\n3. It actually scrapes the page you link to with it and provides you with semantic data on the page?!\r\n4. All of this is completely \"transparent?\" You can view the stats of any bit.ly link\r\n\r\nAll you have to do is put /info/ in between bit.ly and the random characters it assigns to your link. So for instance, the bit.ly link to our recent \"Redesigning the Government\" post was [http://bit.ly/LONF](http://bit.ly/LONF) But if you just put that magic /info/ in it, you can see the stats: [http://bit.ly/info/LONF](http://bit.ly/info/LONF)\r\n\r\nWe've been using it to track Twitter's effectiveness at driving traffic to the Sunlight Labs website we're quite excited to see it using [OpenCalais](http://opencalais.com) to scrape semantic data off the page as well. It is quite a unique blend of interesting information about something as simple as a link.\r\n\r\nAlso: I highly recommend the Firefox extension. It allows you to see some basic information about a bit.ly or other shortcutted link when you mouse-over it like how many clicks it has received and more importantly, what the full URL is.\r\n\r\nThanks Bit.ly!", "content": "We've been using [Bit.ly](http://bit.ly) a lot here at Sunlight Labs-- I've become somewhat of an evangelist of using it over many similar shortcut services. What gets me excited about [bit.ly](http://bit.ly) is the advanced tracking capabilities it has. Did you know....\r\n\r\n1. Bit.ly tracks [friendfeed](http://friendfeed.com) and [twitter](http://www.twitter.com) conversations?\r\n2. It tracks each individual click and tells you where they're coming from? \r\n3. It actually scrapes the page you link to with it and provides you with semantic data on the page?!\r\n4. All of this is completely \"transparent?\" You can view the stats of any bit.ly link\r\n\r\nAll you have to do is put /info/ in between bit.ly and the random characters it assigns to your link. So for instance, the bit.ly link to our recent \"Redesigning the Government\" post was [http://bit.ly/LONF](http://bit.ly/LONF) But if you just put that magic /info/ in it, you can see the stats: [http://bit.ly/info/LONF](http://bit.ly/info/LONF)\r\n\r\nWe've been using it to track Twitter's effectiveness at driving traffic to the Sunlight Labs website we're quite excited to see it using [OpenCalais](http://opencalais.com) to scrape semantic data off the page as well. It is quite a unique blend of interesting information about something as simple as a link.\r\n\r\nAlso: I highly recommend the Firefox extension. It allows you to see some basic information about a bit.ly or other shortcutted link when you mouse-over it like how many clicks it has received and more importantly, what the full URL is.\r\n\r\nThanks Bit.ly!", "date_published": "2009-02-09 12:52:08", "comment_count": 0, "slug": "clay-hearts-bitly"}}, {"pk": 57, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-02-09 09:55:01", "author": 3, "timestamp": "2009-02-09 09:55:01", "markup": "none", "title": "Weekly Lab Report 2009.04", "excerpt": "More evidence of The Sunlight Foundation's pursuing its catalyst mission\u2014using the power of the Internet to catalyze greater government openness and transparency\u2014at the Sunlight Labs. Catalysts initiate reactions that precipitate the desired solution. Here's what happened this past week at Sunlight Labs...\r\n<br /><br />", "content": "More evidence of The Sunlight Foundation's pursuing its catalyst mission\u2014using the power of the Internet to catalyze greater government openness and transparency\u2014at the Sunlight Labs. Catalysts initiate reactions that precipitate the desired solution. Here's what happened this past week at Sunlight Labs...\r\n<br /><br />\r\n<p>\r\n<a href=\"http://transparencyjobs.com/jobs/67/\"><b>Sunlight Labs Application Developer sought.</b></a> Our tiny contribution to stimulate the economy. If you are interested in a large data warehouse project and live or want to live in DC, we may be interested in you for this full time position. Tag as #job.\r\n\r\n<p>\r\n<a href=\"http://wiki.sunlightlabs.com/index.php/Project_Ideas.\"><b>Please Hack a Project.</b></a> You can be a Lab's contributor to government transparency without leaving your job or moving to DC. Check out the growing list of project ideas like blog plugins, ping the president, and recaptcha for federal data.  Tag as #ideas.\r\n\r\n<p>\r\n<a href=\"http://wiki.sunlightlabs.com/index.php/Stimulus_and_bailout_web_projects#Stimulus\"><b>Here a Stimulus. There a Stimulus. Everywhere a Stimulus.</b></a>#microtask: Please add any projects tracking stimulus-related data to this wiki page.\r\n<p>\r\n\r\n<a href=\"http://sunlightlabs.com/blog/2009/02/04/redesigning-government-fec/\"><b>Ali's Take on Opportunities to Improve FEC.gov.</b></a> Designer Ali Felski presented another alternate home page and site vision for a government site, FEC.gov.\r\n<p>\r\n\r\n<a href=\"http://sunlightlabs.com/blog/2009/02/02/gop-anywhere-api/\"><b>GOP goes API. James Turk reviews.</b></a> In addition to reviewing GOP's first big API effort, James has contributed a <a href=\"http://gist.github.com/58177\">a python code snippet of using the API</a>.\r\n<p>\r\n\r\n<a href=\"http://www.shiftspace.org/spaces/yeas-and-nays/\"><b>Sunlight Labs API + Asterix + FF Plugin</b></a> ShiftSpace.org is using data from Sunlight Labs API to enabling calling of Representatives from your browser. This is a great example of the economies of scale that APIs and data sharing should exist. Tag as #api, #successstory.\r\n<p>\r\n\r\n<b>Labs Tweet of the Week:</b> @EllnMllr tweeted: <i>\"Everyone thinks alike about how government data should be presented so we can use it.  <a href=\"http://bit.ly/tsm4\">http://bit.ly/tsm4</a>\"\r\n\r\n", "date_published": "2009-02-09 08:05:24", "comment_count": 0, "slug": "weekly-lab-report-200904"}}, {"pk": 56, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-02-07 13:06:43", "author": 3, "timestamp": "2009-02-07 13:06:43", "markup": "none", "title": "Get Your Open Source On", "excerpt": "If you know any Government folks, you might want to share with them <a href=\"http://news.cnet.com/8301-13505_3-10157924-16.html?part=rss&subj=news&tag=2547-1_3-0-5\">CNET's Matt Assay report that open source mandates are coming to enterprises</a>. Shouldn't Government be looking even harder into open source? We are. \r\n", "content": "<p>If you know any Government folks, you might want to share with them <a href=\"http://news.cnet.com/8301-13505_3-10157924-16.html?part=rss&subj=news&tag=2547-1_3-0-5\">CNET's Matt Assay report that open source mandates are coming to enterprises</a>. Shouldn't Government also be looking harder into open source? We are. \r\n</p>\r\n<p>\r\nAssay explains the mandates are coming because companies are tired of paying and re-paying for code written long ago. Physical goods have per-unit material and labor costs. Digital goods do not. Exposure to quality open source software process has a tendency to reveal the hidden inefficiency of much of the proprietary software business model. \r\n</p>\r\n<p>\r\nWe've always embraced open source at <a href=\"http://sunlightfoundation.com\">The Sunlight Foundation</a>. We are also upping our stake and <a href=\"http://sunlightlabs.com/blog/2008/12/19/labs2/\">making our open source effort larger and more formalized</a>. There's also the <a href=\"http://sunlightlabs.com/appsforamerica/\">Apps for America Contest</a> to write open source software\u2014with the requirement of using at least one Sunlight Foundation sponsored open APIs.  You can see the open source digital goods we are putting online at <a href=\"http://github.com/sunlightlabs\">http://github.com/sunlightlabs</a>.\r\n</p>\r\n<p>\r\nThe take away for Government is pretty obvious: \"If the private sector is <i>mandating</i> open source, then government may not be far behind.\" At the very least, Government needs its procurement practices to be friendly to open source.\r\n", "date_published": "2009-02-07 14:10:54", "comment_count": 0, "slug": "get-your-open-source-on"}}, {"pk": 55, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-02-05 10:58:00", "author": 6, "timestamp": "2009-02-05 10:58:00", "markup": "markdown", "title": "What are you doing with the Stimulus?", "excerpt": "Stimulus stimulus stimulus, all we're hearing about is the stimulus packages these days. Everyone in our field is figuring out different ways to parse bills, reports and other such things while we wait on [Recovery.gov](http://www.recovery.gov/) to launch. Whether it is [Stimulus Watch](http://stimuluswatch.org), [Read the Stimulus](http://readthestimulus.org), or even Pew's [Subsidy Scope](http://SubsidyScope.org) (for which we are providing technical assistance), it seems like everybody is trying to parse and find data related to either bailouts or the stimulus packages. \r\n\r\nWe've started a [wiki page](http://wiki.sunlightlabs.com/index.php/Stimulus_and_bailout_web_projects) to track what everybody is up to. Please add your own project here so people can check it out!\r\n\r\nA note: to limit spam on our wiki, we require you to register before you can edit pages. ", "content": "Stimulus stimulus stimulus, all we're hearing about are the stimulus packages these days. Everyone in our field is figuring out different ways to parse bills, reports and other such things while we wait on [Recovery.gov](http://www.recovery.gov/) to launch. Whether it is [Stimulus Watch](http://stimuluswatch.org), [Read the Stimulus](http://readthestimulus.org), or even Pew's [Subsidy Scope](http://SubsidyScope.org) (for which we are providing technical assistance), it seems like everybody is trying to parse and find data related to either bailouts or the stimulus packages. \r\n\r\nWe've started a [wiki page](http://wiki.sunlightlabs.com/index.php/Stimulus_and_bailout_web_projects) to track what everybody is up to. Please add your own project here so people can check it out!\r\n\r\nA note: to limit spam on our wiki, we require you to register before you can edit pages. ", "date_published": "2009-02-05 12:16:00", "comment_count": 0, "slug": "what-are-you-doing-stimulus"}}, {"pk": 54, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-02-04 11:07:41", "author": 8, "timestamp": "2009-02-04 11:07:41", "markup": "none", "title": "Redesigning the Government: FEC", "excerpt": "<img src=\"http://assets.sunlightlabs.com/site/images/fec_logo.jpg\" alt=\"new FEC logo\"/>\r\n\r\n<p>We got a lot of great comments and discussions happening from our redesign of USA.gov, so we thought we\u2019d continue the series and call it \u201cRedesigning the Government.\u201d At Sunlight, we deal a lot with FEC information, both internally and through one of our grantees, OpenSecrets.org so we thought we\u2019d take a look at prototyping some ways that the FEC could better disclose campaign finance information through the Web.</p>", "content": "<img src=\"http://assets.sunlightlabs.com/site/images/fec_logo.jpg\" alt=\"new FEC logo\"/>\r\n\r\n<p>We got a lot of great comments and discussions happening from <a href=\"http://www.sunlightlabs.com/blog/2009/01/23/rethinking_usagov/\">our redesign of USA.gov</a>, so we thought we\u2019d continue the series and call it \u201cRedesigning the Government.\u201d At Sunlight, we deal a lot with FEC information, both internally and through one of our grantees, <a href=\"http://www.opensecrets.org/\">OpenSecrets.org</a> so we thought we\u2019d take a look at prototyping some ways that the FEC could better disclose campaign finance information through the Web.</p>\r\n\r\n<h3>The Old</h3>\r\n\r\n<p>So lets visit <a href=\"http://www.fec.gov\">fec.gov</a> as a starting point to see what needs to be improved on the site. The first thing users will notice is that there is a very large headline that states: \u201cAdministering and Enforcing Federal Campaign Finance Laws\u201d, which is fine for lawyers who come to visit the site but I think it scares away a lot of the regular public. In fact, the FEC\u2019s primary goal according to its charter is disclosure, not administration or enforcement. We think the design and functionality of the FEC website should reflect that primary mandate. Looking at this current site I don\u2019t see a lot that would suggest that right away.</p>\r\n\r\n<img src=\"http://assets.sunlightlabs.com/site/images/fec_homepage.jpg\" alt=\"FEC title on home page\"/>\r\n\r\n<p>The second thing most users will notice, if they happen to come to the the site on a day when there is an alert, is that there is scrolling text on the bottom of the webpage. It certainly does grab your attention, but because it\u2019s scrolling across the screen, it\u2019s more difficult to read and very distracting when you\u2019re trying to read something else on the page.</p>\r\n\r\n<p>Next, we move on to the navigation. There are too many menus! There are menus on 3 sides of the site and when you rollover buttons on 2 of the sides, drop down menus appear. This creates too much confusion for the user when they are trying to find information and navigate the site. Then there is a skip navigation button that appears above the top menu that shouldn\u2019t even be visible to the regular user. This is simply used for screen readers so that when a person with a disability is using the site the screen reader won\u2019t list off every single button in menus (thanks to Jeremy for this explanation). Finally, when you resize the browser window, the layout breaks and the search box disappears behind the menu. Considering that search is a primary means of navigation for people these days, this deserves fixing.</p>\r\n\r\n<img src=\"http://assets.sunlightlabs.com/site/images/fec_fullHome.jpg\" alt=\"FEC current home page\"/>\r\n\r\n<h3>The New</h3>\r\n\r\n<p>So I started attacking the site by pay attention to the needs of the normal user. I surfaced and focused on what they would generally be interested in: campaign funds and candidate profiles. I placed some of this information on the home page but most importantly I placed radio buttons with these options below the search box to provide a faceted search allowing users to more quickly access the content they\u2019re seeking. The rest of the home page feels more accessible and open to the public by adding the seminar schedule (if a user wanted more information and wanted to attend a conference), and the commission calendar.</p>\r\n\r\n<p>Another important part of the home page is the feature section. By adding this, there is now a place for the FEC to place current information without distracting from all other content on the page and a great space to feature new and interesting visuals.</p>\r\n\r\n<p>As for the navigation, I did a quick inventory of the information that is currently on fec.gov. In doing this I found that there were a lot of things that could be paired down. For example, instead of having a search for each individual database why not just have one search but give the user more options up front so that they can narrow their search. I also placed the search box in a more prominent place so that users would use this as the main way to navigate the site.</p>\r\n\r\n<p>On the interior pages I just organized and structured the data by adding striping to the charts and adding graphics for more interest where they made sense.</p>\r\n\r\n<p>Finally, we also spent some time considering the backend. I\u2019m no developer, but I\u2019m told that the current system doesn\u2019t allow for data to be exported particularly well. We\u2019re recommending that the data be provided in open and exportable formats like XML, JSON, and CSV, which you would be able to get from any page with data on it. The developer-types have more details about this, so they might chime in with comments.</p>\r\n\r\n<h3>The Reveal</h3>\r\n\r\n<p>Click on the images below to see the full comps.</p>\r\n<a href=\"http://sunlightlabs.com/images/fec_comp1.jpg\"><img src=\"http://assets.sunlightlabs.com/site/images/fec_smcomp1.jpg\" alt=\"FEC comp 1\"/></a>\r\n<a href=\"http://sunlightlabs.com/images/fec_comp2.jpg\"><img src=\"http://assets.sunlightlabs.com/site/images/fec_smcomp2.jpg\" alt=\"FEC comp 2\"/></a>\r\n<a href=\"http://sunlightlabs.com/images/fec_comp3.jpg\"><img src=\"http://assets.sunlightlabs.com/site/images/fec_smcomp3.jpg\" alt=\"FEC comp 3\"/></a>\r\n\r\n<h3>Conclusion</h3>\r\n\r\n<p>The FEC obviously has a lot of data to think through on their site to see what is relevant and what isn\u2019t before getting to this point. But when they do, I think some of these elements could work for them or hopefully at the very least get them thinking about structure and design.</p>\r\n", "date_published": "2009-02-04 12:52:24", "comment_count": 5, "slug": "redesigning-government-fec"}}, {"pk": 53, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-02-03 13:05:08", "author": 3, "timestamp": "2009-02-03 13:05:08", "markup": "none", "title": "Call Congress from Your Browser", "excerpt": "<p>\"Using Yeas and Nays, a citizen can connect via phone to speak with her representatives, and the resulting shift keeps a record of the call located on the website that informed it.\" Uses Sunlight Labs API. <a href=\"http://www.shiftspace.org/spaces/yeas-and-nays/\">Link</a></p>\r\n\r\n<p><embed src=\"http://blip.tv/play/Aen9BJOWPQ\" type=\"application/x-shockwave-flash\" width=\"400\" height=\"292\" allowscriptaccess=\"always\" allowfullscreen=\"true\"></embed></p> ", "content": "<p>\"Using Yeas and Nays, a citizen can connect via phone to speak with her representatives, and the resulting shift keeps a record of the call located on the website that informed it.\" Uses Sunlight Labs API. <a href=\"http://www.shiftspace.org/spaces/yeas-and-nays/\">Link</a></p>\r\n\r\n<p><embed src=\"http://blip.tv/play/Aen9BJOWPQ\" type=\"application/x-shockwave-flash\" width=\"400\" height=\"292\" allowscriptaccess=\"always\" allowfullscreen=\"true\"></embed></p>", "date_published": "2009-02-04 09:44:26", "comment_count": 0, "slug": "call-congress-your-browser"}}, {"pk": 52, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-02-02 16:33:01", "author": 5, "timestamp": "2009-02-02 16:33:01", "markup": "restructuredtext", "title": "Taking a look at the GOP Anywhere API", "excerpt": "The Republicans in Congress have launched a new GOP.gov which includes an API.  While a commendable effort it leaves a lot to be desired but could serve to pave a path towards an official congressional API.", "content": "The Republicans in Congress have launched a new GOP.gov_ which on the surface looks pretty good.  The site has a nice design and places for all sorts of useful content and could has potential to serve as a portal for all sorts of useful information on current legislation (albeit from an admittedly partisan point of view).\r\n\r\nThe thing that caught my eye however was the prominent inclusion of a section on the site titled RSS & API.  This section shows RSS feeds for various sections on the site, House committees, and GOP 'Solutions' to pressing issues including the Economy, Energy, and Healthcare.  Unfortunately as of the time of writing none of these links work.\r\n\r\nHoping I would have more luck with the API I clicked over and was at first quite impressed.  The `GOP Anywhere API documentation`_  includes methods to get details on members of congress, committees, bills, votes, and various other documents.  In other words, the same thing that APIs like the `NY Times Congress API`_, `Project Vote Smart API`_, and many others aim to include -- but potentially from a more official and accurate source.\r\n\r\nI immediately signed up for an API key, signup was relatively painless, asking for the regular information (name, zipcode, email) and the examples looked good as well so I figured I'd be using the API almost immediately.  Upon firing up an interactive python session to play around I found that the simple registration process had created a false sense of security as instead I was greeted with a host of problems:\r\n\r\n* It is fairly confusing where to get your API key (both a coworker and I stumbled here at first), the URL referenced in the examples does not exist, but the key is shown to logged in users on the front page of the API documentation.\r\n* All of the API methods have to be accessed via `HTTP POST` when in fact all of the methods are simply retrieving (or GET-ting) data.  This should be relatively simple to fix, and would do wonders for allowing users to experiment with the API without a client library.\r\n* The documentation implied that you had to use an https URL (although this doesn't appear to be true and hopefully never is enforced) \r\n* There are four examples of how to make a call to the API, but written in what seems like a peculiar choice of languages: perl, VBScript, ASP.NET, and JSP.  None of these languages are exactly the languages thriving in the kind of mashup communities that an API presumably serves.  I would have expected to see at least one example in PHP, Ruby, or Python.\r\n* members.get for a Democratic district returns \"ERROR: Member Does Not Exist\" and for districts in which a Republican was ousted in the 2008 elections seems to return the defeated Republican with no indication that the representative in question is no longer in office. (an example of this is NY-29th's Randy Kuhl)\r\n* Unfortunately it is difficult to judge the rest of the API as most of the documented methods do not appear to work. see [1]_ \r\n* Finally, are Senators included?  The site seems to make no mention of this but I don't see why they shouldn't be.\r\n\r\nMany of these questions would best be discussed for most APIs on a mailing list or with some sort of API contact, but no such official list exists and the only feedback mechanism is an email address listed with a disclaimer stating that the API is not supported.  This is hardly the way to foster a community of developers building upon and improving your tools.\r\n\r\nI'm sure that some of these problems can be addressed (most likely the API simply isn't entirely online yet and the missing documentation and methods will be up shortly) but some of these are more serious.  As far as examples of access in other languages, the community can provide those (if and when the API becomes fully functional, I'll offer to do the python ones)  Although this is GOP.gov, it seems worthwhile for Republicans and Democrats alike to be returned in methods like member.get.  Allow filtering by party of course, but to simply return no data or incorrect data seems to do a disservice to all would-be users of this potentially valuable service.\r\n\r\nThe GOP is clearly trying to make an effort to offer data to the public in new and interesting ways and for that they should be commended.  I would hope that they see fit to include Democratic legislators as well (even if the GOP.gov site exists to serve primarily partisan interests, tracking the votes of Democrats seems valuable enough).  \r\n\r\nPerhaps with some improvements and a little bit of work from both sides of the aisle the GOP.gov API can become the house.gov API.\r\n\r\n.. [1] **update:** just shortly after posting I received a reply to an email I had sent to the email contact (same-day which is encouraging) letting me know that due to a rush to launch only five high-priority methods are available at the moment (bill.get, bill.getall, member.get, vote.get and vote.member)\r\n\r\n\r\n.. _`GOP.gov`: http://gop.gov\r\n.. _`GOP Anywhere API documentation`: http://www.gop.gov/api\r\n.. _`NY Times Congress API`: http://sunlightlabs.com/blog/2009/01/08/nytimes-congress-api/\r\n.. _`Project Vote Smart API`: http://api.votesmart.org/docs/index.html\r\n\r\n", "date_published": "2009-02-02 16:31:35", "comment_count": 4, "slug": "gop-anywhere-api"}}, {"pk": 50, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-02-02 08:36:17", "author": 3, "timestamp": "2009-02-02 08:36:17", "markup": "none", "title": "Weekly Lab Report 2009.03", "excerpt": "It was a quiet (code) week in Sunlight Labs, more a week of conversations and information sharing than production. The fact practitioners can so easily move between \"producing\" and \"knowledge sharing\" is one of the things I like about the Web and its mix of blogs and wikis and email lists and twitter, etc. Here's what happened this past week at Sunlight Labs... ", "content": "It was a quiet (code) week in Sunlight Labs, more a week of conversations and information sharing than production. The fact practitioners can so easily move between \"producing\" and \"knowledge sharing\" is one of the things I like about the Web and its mix of blogs and wikis and email lists and twitter, etc. Here's what happened this past week at Sunlight Labs... \r\n<br /><br />\r\n<p>\r\n<a href=\"http://wiki.sunlightlabs.com/index.php/How_you_can_help\"><b>How You Can Help Wiki Page.</b></a> There's a now a very rough wiki page outlining different tasks developers, designers, and evangelist-inclined can dive into to make government more transparent. \r\n\r\n<p>\r\n<a href=\"http://sunlightlabs.com/blog/2009/01/27/section-508-compliance-easier-you-think/\"><b>Accessibility is Easier via Web Standards.</b></a> Who knew Senior Developer Jeremy Carbaugh use was once did Section 508 compliance testing? Who knew that 80% of compliance can be achieved via today's web standards? Tag as #reference.\r\n\r\n<p>\r\n<a href=\"http://sunlightlabs.com/blog/2009/01/28/govt-hackers-know-these-challenges/\"><b>Required reading for Government Hackers.</b></a> Some great documents online describe the challenges Government faces in implementing technologies the larger Web takes for granted.  Greg Elin put together this handy set of links to those documents.  Tag as #reference.\r\n\r\n<p>\r\n<a href=\"http://wiki.sunlightlabs.com/index.php/Sunlight_Labs_Wordlist_API\"><b>Stop! In the Name of Words.</b></a> Before stopwords distort your frequency counts... Removing stopwords\u2014frequently occurring words that are filtered when algorithmically analyzing content\u2014is a little wheel everyone keeps re-inventing. So, Sunlight Labs is experimenting with a stopwords removal API service. \r\n<p>\r\n\r\n<a href=\"http://wiki.sunlightlabs.com/index.php/State_Legislation_Page\"><b>#microtask Add Your State's Legislation Website.</b></a> I love lists. We're creating one of State Legislation web pages. \r\n<p>\r\n\r\n<p>\r\n<b>Maven Tim's Restaurant Pick:</b> <a href=\"http://www.gwhospital.com\">George Washington University Hospital</a> So eager was Tim to eat here, he broke his collarbone to do it. Our wishes for a speedy recovery.\r\n<p>\r\n<b>Labs Tweet of the Week:</b> @jroo tweeted: <i>\"@EllnMllr sent this out to the Sunlight staff list. I think everyone should see it: <a href=\"http://bit.ly/FeXk\">http://bit.ly/FeXk</a>\"\r\n\r\n", "date_published": "2009-02-01 12:00:00", "comment_count": 0, "slug": "weekly-lab-report-200903"}}, {"pk": 43, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-19 10:40:31", "author": 3, "timestamp": "2009-01-19 10:40:31", "markup": "none", "title": "Government Hackers - Know These Challenges", "excerpt": "The value proposition of Web 2.0 changing government is no longer debated. But your govt hacker street cred depends on understanding the real challenge is interoperability with policies and rules created for a federal government whose founders embraced the disruptive technology of their time, the printing press.\r\n<p>\r\nThe following links are your briefing book...", "content": "<a href=\"http://www.wired.com/politics/onlinerights/magazine/17-02/ff_obama?currentPage=4\"><img src=\"http://blog.sunlightfoundation.com/media//2009/01/wired-govt-obstacles-2.png\" width=\"410\" title=\"Screenshot excerpt of the obstacles for a wired presidency from January 2009 Wired article on the topic.\r\n\"/></a>\r\n<p>\r\nThe value proposition of Web 2.0 changing government is no longer debated.  But if you want to be a player  improving transparency and democracy, your street cred depends on understanding the real challenge is interoperability with policies and rules created for a federal government whose founders embraced the disruptive technology of their time, the printing press.\r\n<p>\r\nThe following links are your briefing book...\r\n<ul>\r\n<li><a href=\"http://www.wired.com/politics/onlinerights/magazine/17-02/ff_obama?currentPage=1\">Evan Ratliff Wired Magazine article summarizing on what Obama faces to create a Wired Presidency</a>. Read it all.</li>\r\n<li><a href=\"http://www.usa.gov/webcontent/technology/other_tech.shtml\">WebContent.gov's Social Media and Web 2.0 in Government reference page</a>. Be sure and read Dec 2008's <a href=\"http://www.usa.gov/webcontent/documents/SocialMediaFed%20Govt_BarriersPotentialSolutions.pdf\">Barriers and Solutions to Implementing Social Media and Web 2.0 in Government</a></li>\r\n<li>Federal Web Managers Council's White Paper for Transition Team <a href=\"http://www.usa.gov/webcontent/documents/Federal_Web_Managers_WhitePaper.pdf\">Putting Citizens First: Transforming Online Government</a></li>\r\n<li><a href=\"http://blog.cdt.org/2009/01/08/a-new-cookie-policy-for-egov-20-%E2%80%93-part-i/\">CDT's Ari Swartz on an E-Govt Cookie Policy Part I</a> and <a href=\"http://blog.cdt.org/2009/01/09/a-new-cookie-policy-for-e-gov-20-part-2/\">Part 2 which the Labs helped inform</a></li>\r\n<li><a href=\"http://resource.org/8_principles.html\">8 Principles of Open Data</a></li>\r\n<li><a href=\"http://www.mysociety.org/2009/01/07/top-5-internet-priorities-for-the-next-government-any-next-government/\">Tom Steinberg's Top 5 Internet Priorities for the Next Government (any next Government)</a></li>\r\n<li><a href=\"http://blog.sunlightfoundation.com/2009/01/06/federal-cto-wishlist/\">Sunlight's Federal CTO Wishlist (John Wonderlich)</li>\r\n<li><a href=\"http://blog.sunlightfoundation.com/2008/12/03/yes-we-canuse-comments-web-services-on-government-web-sites/\">Sunlight Foundation post: Yes we can use comments, web services on Government web sites (Greg Elin)</a></li>\r\n<li><a href=\"http://www.ombwatch.org/article/archive/551\">Right to Know Recommendations</a> - Long document, includes more general policy issues beyond technology</li>", "date_published": "2009-01-28 09:55:47", "comment_count": 2, "slug": "govt-hackers-know-these-challenges"}}, {"pk": 49, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-27 09:57:33", "author": 1, "timestamp": "2009-01-27 09:57:33", "markup": "markdown", "title": "Section 508 compliance is easier than you think", "excerpt": "Worried about having to comply with Section 508 accessiblity standards? Don't! 80% of the work is done for you if you follow web standards when developing a site. Part one of a two-part series.", "content": "_This is part one of a two-part post. Part one covers the basics of web standards and progressive enhancement and Section 508 standards \u00a71194.22 (a)-(f)._\r\n\r\nLabs designer Ali Felski [recently posted](http://sunlightlabs.com/blog/2009/01/23/rethinking_usagov/) about an experiment in redesigning the web site of [USA.gov](http://usa.gov). One of the commenters on the post felt that the design would not be feasible as it is not compliant with [Section 508 accessibility standards](http://www.section508.gov/index.cfm?FuseAction=Content&ID=12). I disagree. It is nearly impossible to infer a site's compliance with Section 508 from the design.\r\n\r\nSeveral years ago I worked (contracted) as a Section 508 tester for the Treasury Department on their [Governmentwide Accounting and Reporting Modernization](http://www.fms.treas.gov/gwa/) project. Since then I have developed applications for the IRS and FBI that have had to implement Section 508 standards. There seems to be quite a bit of confusion over the implications of Section 508 and the effort involved in implementing the standards... even amongst those dealing with it each day. In this two-part post I will dispel some of the misconceptions and show how easy it is to accomplish when sites are developed with web standards and progressive enhancement.\r\n\r\n### Developing with web standards\r\n\r\nWhen developing with web standards, there are three important things to keep in mind:\r\n\r\n*   HTML is content\r\n*   CSS is presentation\r\n*   JavaScript is behavior\r\n\r\nThink about it for a minute and let it sink in. Most people think of HTML as that markup language that is used to layout web pages. This, however, is a violation of the separation of layers. HTML is meant to give semantic meaning to your content. __p__ tags denote paragraphs. __div__ tags indicate logic divisions within content. __table__ tags are used for tabular data and _not for layout_... as tempting as it is.\r\n\r\n__img__ tags are another good example. You would probably think that the Sunlight Labs logo at the top of this page is rendered with an __img__ tag in the HTML. By doing this we would be including presentation elements in our content. The proper way to implement this is by using an __h1__ tag containing 'Sunlight Labs' and using CSS [image replacement techniques](http://css-discuss.incutio.com/?page=ImageReplacement) to display the logo image on the page instead of the text. One of the benefits of this method is that our organization name is included in the content of the page rather than locked away in an image.\r\n\r\n### Progressive enhancement\r\n\r\nSo far we've seen an example of maintaining a separation of content and style. [Progressive enhancement](http://www.alistapart.com/articles/understandingprogressiveenhancement) is a method of separating content and behavior.\r\n\r\nWhat happens if we don't maintain of separation of content and behavior and the user has JavaScript disabled? In most cases the page will either be missing content or not function correctly. By creating fully functional web sites without JavaScript and then layering on behavioral features, it is possible to create functional and rich user experiences.\r\n\r\n### Section 508 Guidelines\r\n\r\n*   #### (a) A text equivalent for every non-text element shall be provided (e.g., via \"alt\", \"longdesc\", or in element content).\r\n\r\n    If we properly separate our three layers, we remove most of the situations in which we would have to provide text equivalents. Markup should only include __img__ tags when the image is actually part of the content of the page (i.e. [Flickr](http://flickr.com) or [Boston.com's The Big Picture](http://www.boston.com/bigpicture/)).\r\n\r\n    _The lesson:_ Logos, navigation, buttons and other content elements are not proper uses of __img__ tags. When non-text content is necessary, use title and alt attributes.\r\n\r\n*   #### (b) Equivalent alternatives for any multimedia presentation shall be synchronized with the presentation.\r\n\r\n    That fancy Java slideshow applet that adds ripple effects to the slides probably isn't necessary. Simplify your life and make basic HTML pages styled with CSS instead. \r\n\r\n    _The lesson:_ If you do need multimedia capabilities, use technologies that have accessibility options. \r\n\r\n*   #### (c) Web pages shall be designed so that all information conveyed with color is also available without color, for example from context or markup.\r\n\r\n    Users with colorblindness or screen readers can't tell a red icon from a green icon. If all you have in your HTML is an __img__ tag, you are hiding content from people that are unable to see the page. Be sure to have meaningful text content in your HTML whenever you are conveying information with color.\r\n\r\n\t_The lesson:_ Color is presentation, not content. Convey all information as text in HTML and use styles heets to make it pretty and colorful.\r\n\r\n*   #### (d) Documents shall be organized so they are readable without requiring an associated style sheet.\r\n\r\n    Remember our three layers of separation?\r\n\r\n    _The lesson:_ If you have created a page that needs CSS or JavaScript to be readable, you have violated our \"HTML is content\" principle. Semantic markup without styles creates HTML that is meaningful even if it is boring to look at.\r\n\r\n*   #### (e) Redundant text links shall be provided for each active region of a server-side image map.\r\n\r\n    Don't use image maps as they are almost always unnecesary. Simple as that.\r\n\r\n    _The lesson:_ Provide basic text interactions with plain old HTML. If rich interactions are necessary, enable them by using progressive enhancement techniques.\r\n\r\n*   #### (f) Client-side image maps shall be provided instead of server-side image maps except where the regions cannot be defined with an available geometric shape.\r\n\r\n    _The lesson:_ Didn't we agree to not use image maps?\r\n\r\nStay tuned for part two where we explore standards (g)-(p)!", "date_published": "2009-01-27 23:33:20", "comment_count": 1, "slug": "section-508-compliance-easier-you-think"}}, {"pk": 48, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-26 01:32:31", "author": 3, "timestamp": "2009-01-26 01:32:31", "markup": "none", "title": "Weekly Lab Report 2009.02", "excerpt": "You probably heard a new President was inaugurated this past week and that his first memo from the White House was about TRANSPARENCY. (Yhaaa.)  We were back in our Lab coats nevertheless. Here's what happened last week at Sunlight Labs. ", "content": "<p>\r\n<b>Around here, we use APIs.</b> <a href=\"http://capitolwords.org/api/\">CapitolWords.org has an API</a> linked from the home page, but our Systems Guru Tim Ball noticed someone scraping the site dictionary word by dictionary word.  I know, it's hard to let go of one's dysfunctions, but you we <i>want</i> you to use our data. We have an API.  \r\n<p>\r\n<a href=\"http://thisweekindjango.com/links/2009/jan/24/django-secretballot/\"><b>Our Django Voting App gets some love.</b></a> ThisWeekinDjango.com threw more whuffie our way by highlighting our open source module for allowing users to vote without having to login. \r\n<p>\r\n<a href=\"http://en.wikipedia.org/wiki/Brian_Behlendorf\"><b>Touched by Brian Behlendorf.</b></a> Brian was a key developer behind Apache, Apache Foundation, and Subversion. <a href=\"http://oreilly.com/catalog/opensources/book/brian.html\">Brian also wrote this great essay in 1999 about open source and platforms</a> and visited our offices this week in 2009 to discuss open source in government. Also visiting was Deb Bryant from Oregon State University's <a href=\"http://osuosl.org/hosting/clients\">Open Source Lab who's infrastructure hosts the BIG open source projects</a>. \r\n<p>\r\n\r\n<p><a href=\"http://sunlightlabs.com/blog/2009/01/23/rethinking_usagov/\"><b>See Ali blog. Blog. Blog. Blog.</b></a> Senior Designer Ali Felski, who joined Sunlight in the fall and makes these beautiful pages, blogged here for the first time this week, writing about her mockup of a redesigned USA.gov site.\r\n\r\n<p><b>Apps for America already Rocking!</b>\r\n It's a big deal when the author <a href=\"http://obiefernandez.com/\">The Rails Way</a> Obie Fernandez <a href=\"http://blog.obiefernandez.com/content/2009/01/getting-our-national-service-groove-on-with-apps-for-america.html\">spends his Day of Service working on a transparency app</a>. It's also pretty amazing when guy who's you only know by his weird blog name hears about our <a href=\"http://sunlightlabs.com/appsforamerica\">Apps for America Contest</a> and creates a hack to use our API and Google to automagically discover the RSS feed every Member of Congress! \r\n\r\n<p><a href=\"http://spreadsheets.google.com/pub?key=p-kX0k_KXgtwwzHnvEn4eCA\">Stimulus package spreadsheet</a> Clay and Jeremy put together a public Google spreadsheet of spending in the proposed stimulus package. Read the post <a href=\"http://sunlightlabs.com/blog/2009/01/21/what-were-doing-stimulus-bill/\">here</a>.\r\n\r\n<p>\r\n<b>Maven Tim's Restaurant Pick:</b> <a href=\"http://www.yelp.com/biz/nava-thai-noodle-and-grill-wheaton\">Nava Thai</a> Because it is \"friendly\" and \"cheap\" after you spent all your money on inauguration celebrations.\r\n<p>\r\n<b>Labs Tweet of the Week:</b> @jroo tweeted: <i>\"'Transparency and the rule of law will be the touchstones of this presidency': <a href=\"http://tinyurl.com/aul7os\">http://tinyurl.com/aul7os</a>\"\r\n", "date_published": "2009-01-25 17:00:00", "comment_count": 0, "slug": "weekly-lab-report-200902"}}, {"pk": 47, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-23 13:12:13", "author": 8, "timestamp": "2009-01-23 13:12:13", "markup": "none", "title": "Rethinking usa.gov", "excerpt": "<img class=\"sideImage\" src=\"http://assets.sunlightlabs.com/site/images/usaGov_newLogo.jpg\" alt=\"header on usa.gov\"/>\r\n\r\n<p>With Obama in the President\u2019s seat now, and many new people coming into the vast executive branch, they now have an opportunity to revisit their presence on the web and explore the possibilities of getting the American people more interested and more informed about what their government is doing. The hub for all this information is a site known as <a href=\"http://www.usa.gov\">usa.gov</a>.</p>", "content": "<img class=\"sideImage\" src=\"http://assets.sunlightlabs.com/site/images/usaGov_newLogo.jpg\" alt=\"header on usa.gov\"/>\r\n\r\n<p>With Obama in the President\u2019s seat now, and many new people coming into the vast executive branch, they now have an opportunity to revisit their presence on the web and explore the possibilities of getting the American people more interested and more informed about what their government is doing. The hub for all this information is a site known as <a href=\"http://www.usa.gov\">usa.gov</a>.</p>\r\n\r\n<p>So what would it be like if the new administration were to rethink their presence on the web and turn usa.gov into a more open facing site\u2013a site where people could pick and choose the content that was relevant to them and display it all in a way that was organized and appealing. At the <a href=\"http://www.sunlightfoundation.com\">Sunlight Foundation</a> we decided to spend a little time thinking about these challenges and came up with a short design exercise that will hopefully get people thinking in that direction.</p>\r\n\r\n<h3>The Old</h3>\r\n\r\n<p>First of all, let\u2019s visit the current usa.gov site to see what\u2019s been done well and what misses the mark completely. Starting at the top of the page, the header is too cluttered. The logo doesn\u2019t have enough space around it which makes it hard for a users eyes to focus there first, and the design itself is dated. I do agree with having the search box as the main source of navigation at the top of the page because there is so much content on usa.gov. However, I think the search options you are given\u2013government web, images, news, maps, and usa.gov\u2013could be eliminated by having a better search results page and a more prominent news section on the home page.</p>\r\n\r\n<img src=\"http://assets.sunlightlabs.com/site/images/usaGov_header.jpg\" alt=\"header on usa.gov\"/>\r\n\r\n<p>Next is the navigation below the header. It\u2019s kind of nice to be able to navigate based on audience, but at the same time who really thinks of himself as an audience? With people changing all the time wouldn\u2019t you want your government site to change with you instead of pigeon-holing you?</p>\r\n\r\n<img src=\"http://assets.sunlightlabs.com/site/images/usaGov_navBar.jpg\" alt=\"navigation on usa.gov\"/>\r\n\r\n<p>Finally, the main content of the site just seems generally random. To get to the content you\u2019re after, it involves many clicks and there is little to draw the user\u2019s eye to the most important content on a page.</p>\r\n\r\n<img src=\"http://assets.sunlightlabs.com/site/images/usaGov_content.jpg\" alt=\"content on usa.gov\"/>\r\n\r\n<h3>The New</h3>\r\n\r\n<p>In thinking about a new structure and new design for usa.gov we had a few goals that we wanted to accomplish. They basically were: letting the user customize and personalize the content that was displayed on the site, having better structure and navigation, and just having a cleaner, more powerful overall look to the site.</p>\r\n\r\n<p>I think a great way to make the government more accessible to the public is to ensure content is relevant to each person by allowing them to customize usa.gov. With a simple login, users could save information that was relevant to them instead of painfully sifting though links and then having to do it all over again when they might want the same content down the road.</p>\r\n\r\n<p>Taking that idea a step further would be pulling content from other government websites. For example, if you have student loans and what to see your balance, or if you just need to know where you\u2019re currently registered to vote, wouldn\u2019t it be nice to see all of that content in one place? There is lots of data out there like this\u2013medicare, social security, where your local post office is or even what kind of expenditures have been made by the federal government\u2013that when opened up, would allow the Government to serve the public better and give the user a better experience.</p>\r\n\r\n<p>For navigation, I again placed the search box front and center, and then decided on three small tabs: my government (which would have your personal government information), my community (just want it sounds like: federal community events, school data and where federal money is being spent in your area), and my elected officials (who your federal elected officials are, how to contact them, where they get their money, etc.). I think breaking things out this way makes the user feel a sense of ownership (their cognitive ownership bias coming into play) which will encourage them to come back more frequently and invest time customizing their window into the government.</p>\r\n\r\n<p>Finally, I needed to organize and declutter the interface. I went with a clean look using neutral colors and pulled cues from change.gov for some continuity between government sites. This was accomplished by adding more white space, using the more primary colors (blue and red) when I needed to call attention to the content, and visualizing data so that users would be able to better understand it.</p>\r\n\r\n<h3>The Reveal</h3>\r\n\r\n<p>Click on the smaller images below to view the full comps.</p>\r\n\r\n<a href=\"http://sunlightlabs.com/images/usa_gov_mygovFinal.jpg\"><img src=\"http://assets.sunlightlabs.com/site/images/usaGov_comp1.jpg\" alt=\"content on usa.gov\"/></a>\r\n\r\n<a href=\"http://sunlightlabs.com/images/usa_communityFinal.jpg\"><img src=\"http://assets.sunlightlabs.com/site/images/usaGov_comp2.jpg\" alt=\"content on usa.gov\"/></a>\r\n\r\n<a href=\"http://sunlightlabs.com/images/usa_gov_myrepsFinal.jpg\"><img src=\"http://assets.sunlightlabs.com/site/images/usaGov_comp3.jpg\" alt=\"content on usa.gov\"/></a>\r\n\r\n<h3>Conclusion</h3>\r\n\r\n<p>Of course there were many challenges in doing this. For example, any personalization of a site would clearly have security and privacy concerns, especially with the government law concerning persistent cookies. But would leaving vital information like account numbers, social security numbers, addresses and full names help mitigate this concern? And technically all this data could be pulled instantly, so the combination of this data would only exist inside of a session. So with that in mind this isn\u2019t necessarily something that the government should immediately take and implement, but will hopefully be a conversation starter about the possibilities that are out there for an overall better face to the government.</p>\r\n", "date_published": "2009-01-23 14:10:12", "comment_count": 18, "slug": "rethinking_usagov"}}, {"pk": 46, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-23 09:39:42", "author": 6, "timestamp": "2009-01-23 09:39:42", "markup": "markdown", "title": "Does your Rep have an RSS Feed?", "excerpt": "Sunlight Labs contributor \"wubbahead\" comes up with an ingenious and automated way to find out whether or not your representative has an RSS feed by using the [Sunlight Labs API](http://services.sunlightlabs.com/api) the [Google AJAX Feed API](http://code.google.com/apis/ajaxfeeds/documentation/reference.html#lookupFeed) and some JavaScript. [Make sure to check it out](http://wubbahed.com/2009/01/22/does-your-us-representative-have-an-rss-feed/)", "content": "Sunlight Labs contributor \"wubbahead\" comes up with an ingenious and automated way to find out whether or not your representative has an RSS feed by using the [Sunlight Labs API](http://services.sunlightlabs.com/api) the [Google AJAX Feed API](http://code.google.com/apis/ajaxfeeds/documentation/reference.html#lookupFeed) and some JavaScript. [Make sure to check it out](http://wubbahed.com/2009/01/22/does-your-us-representative-have-an-rss-feed/)", "date_published": "2009-01-23 09:39:35", "comment_count": 0, "slug": "does-your-rep-have-rss-feed"}}, {"pk": 45, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-21 18:08:34", "author": 6, "timestamp": "2009-01-21 18:08:34", "markup": "markdown", "title": "What we're doing with the stimulus bill", "excerpt": "Short answer: we're trying to do some interesting things with it and we may need your help. Originally, we thought \"hey, let's put this into [Public Markup](http://publicmarkup.org)\" but unfortunately the bill's complexity actually was incompatible with Public Markup's data model. At the end of the day, the relevant parts of the bill wouldn't have fit into the commenting/displaying architecture we've used for bills in the past. \r\n", "content": "Short answer: we're trying to do some interesting things with it and we may need your help. Originally, we thought \"hey, let's put this into [Public Markup](http://publicmarkup.org)\" but unfortunately the bill's complexity actually was incompatible with Public Markup's data model. At the end of the day, the relevant parts of the bill wouldn't have fit into the commenting/displaying architecture we've used for bills in the past. \r\n\r\nSo we started on a different route, of getting the bill into a different data model so that people could comment on that. We then experimented with services like [disqus](http://disqus.com) for commenting so that we could get it out the door quickly. But during the design phase, we really struggled to find a good, usable way to comment. Then the question came up: why are we doing this? How does this change things? Allowing people to comment on this bill, section by section didn't seem up our alley or particularly revolutionary in terms of resource expenditures. So we got thinking: what if we just came up with a way to easily visualize the spending in the bill. That would be useful.\r\n\r\nSo Jeremy and I started parsing through the bill both with digital parsers and manually and managed to build a spreadsheet of all of the expenditures we could find. But the thing is-- we only were able to come up with $356,421,500,000 worth of stimulus. This is a far reach from 60% of \"875 billion dollars\" (the other 40% are tax cuts). We started getting worried that maybe our non-lawyery minds weren't able to find all the expenditures. This led to a chain of events where we said \"hey, you know what, if we can't be accurate, we'd better not go any further,\" at least officially as the Sunlight Foundation.\r\n\r\n\r\nIt seems Congress should not only embrace XML to show bills to the public but it should also create new ways for Congress to express that information so that machines (and people) can provide context to them. \r\n\r\nSo we're sort of at a loss for what to do next. We got the data into a [spreadsheet](http://spreadsheets.google.com/pub?key=p-kX0k_KXgtwwzHnvEn4eCA) and verified it against the bill text three times. With two sets of eyes. We're not saying we are perfect, but that's been checked over a few times. But where's the rest of the money? Can you help us find it? What have our non-lawyerly minds missed? [Here's the bill](http://appropriations.house.gov/pdf/RecoveryBill01-15-09.pdf)\r\n\r\n\r\n\r\n\r\n", "date_published": "2009-01-21 18:08:24", "comment_count": 3, "slug": "what-were-doing-stimulus-bill"}}, {"pk": 44, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-19 14:25:54", "author": 6, "timestamp": "2009-01-19 14:25:54", "markup": "markdown", "title": "Visualizing the Citizen's Briefing Book", "excerpt": "The Citizen's Briefing Book is an interesting little participatory function on change.gov, too. You can get a good read for what people are concerned about by looking at the number of ideas per category.\r\n\r\nJust a simple little ditty thanks to IBM's [ManyEyes](http://manyeyes.alphaworks.ibm.com/manyeyes/visualizations/idea-submissions-on-changegov).\r\n\r\n<img src=\"http://dl.getdropbox.com/u/36193/ManyEyes%20Change.gov%20Briefing%20Book.jpg\">", "content": "The Citizen's Briefing Book is an interesting little participatory function on change.gov, too. You can get a good read for what people are concerned about by looking at the number of ideas per category.\r\n\r\nJust a simple little ditty thanks to IBM's [ManyEyes](http://manyeyes.alphaworks.ibm.com/manyeyes/visualizations/idea-submissions-on-changegov).\r\n\r\n<img src=\"http://dl.getdropbox.com/u/36193/ManyEyes%20Change.gov%20Briefing%20Book.jpg\">", "date_published": "2009-01-19 14:27:16", "comment_count": 0, "slug": "visualizing-citizens-briefing-book"}}, {"pk": 42, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-19 00:42:01", "author": 3, "timestamp": "2009-01-19 00:42:01", "markup": "none", "title": "Weekly Lab Report 2009.01", "excerpt": "So much happens with Transparency Technology these days, it's a good time to start a Weekly Lab Report. Here's what happened last week at Sunlight Labs.\r\n<p><br />\r\n<a href=\"http://sunlightlabs.com/appsforamerica/\"><b>Labs launches Application Programmers <i>Incentive</i>.</b></a> WIN $15,000! WIN $15,000!  Make something useful\u2014or at least interesting\u2014with APIs from Sunlight for our Apps for America contest we officially announced this week.\r\n<p>\r\n<a href=\"http://fec.gov/law/policy/enforcement/2009/comments/comments.shtml\"><b>Clay comments old school at FEC (aka, testifies).</b></a> Who said developers are anti-social? Head Labs geek Clay Johnson testifies before FEC  commissioners. <a href=\"http://www.fec.gov/law/policy/enforcement/2009/comments/comm28.pdf\">Read Sunlight's filed comments.</a>\r\n\r\n", "content": "So much happens with Transparency Technology these days, it's a good time to start a Weekly Lab Report. Here's what happened last week at Sunlight Labs.\r\n<p><br />\r\n<a href=\"http://sunlightlabs.com/appsforamerica/\"><b>Labs launches Application Programmers <i>Incentive</i>.</b></a> WIN $15,000! WIN $15,000!  Make something useful\u2014or at least interesting\u2014with APIs from Sunlight for our Apps for America contest we officially announced this week.\r\n<p>\r\n<a href=\"http://fec.gov/law/policy/enforcement/2009/comments/comments.shtml\"><b>Clay comments old school at FEC (aka, testifies).</b></a> Who said developers are anti-social? Head Labs geek Clay Johnson testifies before FEC  commissioners. <a href=\"http://www.fec.gov/law/policy/enforcement/2009/comments/comm28.pdf\">Read Sunlight's filed comments.</a>\r\n</p>\r\n<p>\r\n<a href=\"http://www.onthemedia.org/transcripts/2009/01/16/07\"><b>On the Media puts Greg on the air.</b></a> Evangelist Greg Elin interviewed by NPR's <i>On The Media</i>.  Explains in  six minutes the significance of <a href=\"http://www.usaspending.gov/apidoc.php\">USASpending.gov's API to federal contracts</a>. \r\n</p>\r\n<p>\r\n<a href=\"http://infosthetics.com/archives/2009/01/capitol_words_word_frequency_from_the_congressional_record.html\"><b>A Picture <i>from</i> a Thousand Words.</b></a> Eye candy blog Infosthetics.com gave some love to the Lab's <a href=\"http://capitalwords.com/\">CapitalWords</a> project. \r\n</p>\r\n<p>\r\n<b>Pito researching an API cheat-sheet.</b> New Labs contributor and <a href=\"http://en.wikipedia.org/wiki/Pito_Salas\">Pivot Table creator Pito Salas</a> discussed creating a guide/cheat-sheet to linking the various API's in the transparency space with Labs API-wranger James Turk and Greg.\r\n</p\r\n</p>\r\n<p><b>Sunlightlabs.com web traffic briefly eclipsed SunlightFoundation.com.</b> It may only be temporary, but we took some pride in outshining the mothership.\r\n</p>\r\n<p>\r\n<p><b>Great Threads:</b> <a href=\"http://groups.google.com/group/open-government/browse_thread/thread/e0b47441d5cc110b\">Open Government thread on opening govt science data</a> &bull; <a href=\"http://groups.google.com/group/openhouseproject/browse_thread/thread/5a6ca78f7ef4b48a\">Congressional YouTube Channel risks and opportunities</a> \r\n</p>\r\n<p>\r\n<b>Labs Tweet of the Week:</b> @jamesturk tweeted:  <i>@bitb ain't no portable lavatory like a North Carolina portable lavatory\"</i>\r\n ", "date_published": "2009-01-18 18:40:20", "comment_count": 0, "slug": "weekly-lab-report-200901"}}, {"pk": 39, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-14 13:06:16", "author": 3, "timestamp": "2009-01-14 13:06:16", "markup": "none", "title": "Mapping Government Information Flows", "excerpt": "We could take some inspiration from this video of scientists pouring very liquid concrete into an ant hill in order to preserve its structure for study. What could we \"pour\" into the government in order to create a representation of the structure of bureaucracy and information flows inside?", "content": "<p>We could take some inspiration from this video of scientists pouring liquid concrete into an ant hill in order to preserve its structure for study. What could we \"pour\" into the government in order to create a representation of the structure of bureaucracy and information flows inside? If doctors use radioactive dye to trace blood flow in the body, shouldn't we be thinking of some simple ways to tag data going into the system and learn about how data (and dollars) flow? Shouldn't this be possible for the TARP, Stimulus Package, and other government spending?</p>\r\n\r\n<object width=\"318\" height=\"258\"><param name=\"movie\" value=\"http://www.youtube.com/v/ozkBd2p2piU&hl=en&fs=1\"></param><param name=\"allowFullScreen\" value=\"true\"></param><param name=\"allowscriptaccess\" value=\"always\"></param><embed src=\"http://www.youtube.com/v/ozkBd2p2piU&hl=en&fs=1\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\"318\" height=\"258\"></embed></object>", "date_published": "2009-01-16 17:46:17", "comment_count": 0, "slug": "mapping-government-information-flows"}}, {"pk": 41, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-15 15:34:27", "author": 6, "timestamp": "2009-01-15 15:34:27", "markup": "markdown", "title": "Change.gov Popular Words", "excerpt": "Following up on yesterday's post where [Opened up Change.gov](http://sunlightlabs.com/blog/2009/01/14/opening-your-seat-table/) I just took the titles of all the documents and ran them through [Wordle](http://wordle.net), removing some of the blatantly noisy words (Recommendation, Transition, Policy). Here's what we got, which may be a pretty nice way of seeing what people \"at the table\" are talking about.\r\n\r\n![change.gov popular words](http://dl-client.getdropbox.com/u/36193/Wordle%20-%20Change.gov%20Recommendation%20Titles-small.jpg \"Change.gov popular words\") \r\n\r\n[Here's the big version](http://www.wordle.net/gallery/wrdl/441539/Change.gov_Recommendation_Titles)", "content": "Following up on yesterday's post where we [Opened up Change.gov](http://sunlightlabs.com/blog/2009/01/14/opening-your-seat-table/) I just took the titles of all the documents and ran them through [Wordle](http://wordle.net), removing some of the blatantly noisy words (Recommendation, Transition, Policy). Here's what we got, which may be a pretty nice way of seeing what people \"at the table\" are talking about.\r\n\r\n![change.gov popular words](http://dl-client.getdropbox.com/u/36193/Wordle%20-%20Change.gov%20Recommendation%20Titles-small.jpg \"Change.gov popular words\")\u00a0\r\n\r\n[Here's the big version](http://www.wordle.net/gallery/wrdl/441539/Change.gov_Recommendation_Titles)", "date_published": "2009-01-15 15:34:23", "comment_count": 0, "slug": "changegov-popular-words"}}, {"pk": 40, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-14 14:53:10", "author": 6, "timestamp": "2009-01-14 14:53:10", "markup": "markdown", "title": "Opening Your Seat at the Table", "excerpt": "After my [post](http://sunlightlabs.com/blog/2009/01/12/learning_lessons_from_changegov/) yesterday about [Change.gov](http://change.gov)'s [Your Seat at the Table](http://change.gov/open_government/yourseatatthetable) feature, it got us thinking: what if this website disappears on January 21st? What if all this data goes away?\r\n\r\nI posted (half seriously) on our Yammer account about 3 hours ago \"Big gold star to anyone who can scrape and capture every 'your seat at the table' document in a Sunlight repository. I'm getting nervous that change.gov is going to disappear in a week.\"\r\n\r\nJames and Jeremy independently took up the challenge. And now, three hours later we have our repository. We thought we'd share the [code](http://github.com/sunlightlabs/sunlight-scraps/) for you to do it too if you'd like, and also this [handy csv file](http://dl-client.getdropbox.com/u/36193/yourseat.csv) of all of the documents.", "content": "After my [post](http://sunlightlabs.com/blog/2009/01/12/learning_lessons_from_changegov/) yesterday about [Change.gov](http://change.gov)'s [Your Seat at the Table](http://change.gov/open_government/yourseatatthetable) feature, it got us thinking: what if that website disappears on January 21st? What if all that information goes away?\r\n\r\nI posted (half seriously) on our Yammer account about 3 hours ago \"Big gold star to anyone who can scrape and capture every 'your seat at the table' document in a Sunlight repository. I'm getting nervous that change.gov is going to disappear in a week.\"\r\n\r\nJames and Jeremy independently took up the challenge. And now, three hours later we have our repository. We thought we'd share the [code](http://github.com/sunlightlabs/sunlight-scraps/) for you to do it too if you'd like, and also this [handy csv file](http://dl-client.getdropbox.com/u/36193/yourseat.csv) of all of the documents. Looks like there's some interesting stuff in here. One can probably take this CSV file, put it in a database and start getting some context for the transition. It could answer \"What are common themes the transition team is meeting about\" and \"What are the most popular words in documents.\" Or even simply \"which organization has provided the most documentation to the Transition Team.\"\r\n\r\nAnyhow, please enjoy and use wisely. You can use this to create your own archive of the \"Your Seat at the Table,\" as we have. This isn't an official data-dump and won't be considered in the [contest](/appsforamerica) and we won't support the source as it is fairly specific to change.gov. But feel free to play as you see fit!\r\n\r\n\r\n\r\n", "date_published": "2009-01-14 15:04:22", "comment_count": 2, "slug": "opening-your-seat-table"}}, {"pk": 38, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-12 13:36:27", "author": 6, "timestamp": "2009-01-12 13:36:27", "markup": "markdown", "title": "Learning Lessons from Change.gov", "excerpt": "Early on in the Presidential Transition, [Change.gov](http://change.gov announced an incredibly compelling), never-before-done process: [Your Seat at the Table](http://change.gov/open_government/yourseatatthetable). They announced that every document that the transition team received in a meeting where there was three or more attendees would be posted online. By anybody's standards -- much less a presidential transition this was an *awesome* step and the Change.gov team should be commended for taking it.\r\n\r\nThat said Change.gov team is learning as they go and looking at the implementations on Change.gov is an interesting opportunity to get some new transparency technology learning opportunities for the new administration. ", "content": "Early on in the Presidential Transition, [Change.gov](http://change.gov) announced an incredibly compelling, never-before-done process: [Your Seat at the Table](http://change.gov/open_government/yourseatatthetable). They announced that every document that the transition team received in a meeting where there was three or more attendees would be posted online. By anybody's standards -- much less a presidential transition this was an *awesome* step and the Change.gov team should be commended for taking it.\r\n\r\nThat said Change.gov team is learning as they go and looking at the implementations on Change.gov is an interesting opportunity to get some new transparency technology learning opportunities for the new administration. \r\n\r\nMy two takeaways:\r\n\r\n1. Government should be focusing *first* on providing access to data in as unrestricted of a way as possible and inviting developers to build interfaces on top of it, and\r\n2. Often times it is possible in this field to try too hard and to do too much work. A simple solution is sometimes a superior solution.\r\n\r\nHere's why-- as of today on the \"Your Seat at the Table\" page: \r\n\r\n1. Several of the documents on the front page have dates in the future. \r\n2. You can only search once every 15 seconds. \r\n3. It looks like it just got caught up, but for several weeks (from Dec. 22 to about January 9th) the site was nearly dormant and had weeks of missing data.\r\n4. There's not a clear message of what's going to happen to the data after January 20th.\r\n\r\nI'm *certain* that there's a strong amount of effort being put on this by the administration and they're taking it very seriously, but I wonder if they can really keep up with the documents they're getting. Maybe there's another way to do it.\r\n\r\nHere's the simpler approach. Maybe they should start by standardizing the names of the documents, and put them on an FTP server or an index-free http page; updating that FTP server when documents come in and are sanitized; writing one blog post pointing to the repository. In the blog post, say \"people are encouraged to build their own search interfaces and browsing interfaces to the data as they see fit\" \r\n\r\nThe result? You'll likely have the Sunlight Labs [community](http://sunlightlabs.com) scrambling to build an interface, [Watchdog.net](http://watchdog.net) parsing the data and incorporating it, and media organizations like the New York Times working on their own too. More eyeballs would be on the data, and they'd earn praise from the media and technology community for being participatory and more open.\r\n\r\nLet me add one big caveat here: This effort by the Transition team is a special case. We know they have been moving at the government equivalent of light speed, and in a situation where they had no infrastructure to work with. What I suggest above would have the effect of deputizing outside help for an urgent and atypical problem. I'm not sure approach whether what I suggest above should be a general model for such work (though I am inclined that way), but it would have been away to do it better in this instance. \r\n\r\nThat being said, if you take a look at the  [data](http://siteanalytics.compete.com/opensecrets.org+fec.gov/?metric=uv) you'll see a trend-- when the data can be taken from the Government, people do more interesting things with it and it gets in front of more eyeballs. The Government should always be keeping a keen eye towards *first* making the data available to citizens, and *then* building tools and interfaces on top of it.\r\n\r\nThe point is this: This transparency business gets really tough if you make it tough, but if you're looking for the easy way out, there's likely a much simpler solution. And often the simpler ones end up being more sophisticated and impressive anyhow. Often times, the hardest part of being transparent and open is doing too much, rather than not enough.", "date_published": "2009-01-12 13:35:27", "comment_count": 1, "slug": "learning_lessons_from_changegov"}}, {"pk": 37, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-10 18:15:30", "author": 6, "timestamp": "2009-01-10 18:15:30", "markup": "markdown", "title": "How You Can Help", "excerpt": "We've added a significant page to the wiki: [\"How You Can Help\"](http://wiki.sunlightlabs.com/index.php?title=How_you_can_help)\r\n\r\nIf you are wondering how you can use your skills to help make our Government more transparent, this web page is for you. It talks through how developers, designers and activists can be a part of the Sunlight Labs community and lend a hand to our efforts. Make sure to check it out.\r\n\r\n", "content": "We've added a significant page to the wiki: [\"How You Can Help\"](http://wiki.sunlightlabs.com/index.php?title=How_you_can_help)\r\n\r\nIf you are wondering how you can use your skills to help make our Government more transparent, this web page is for you. It talks through how developers, designers and activists can be a part of the Sunlight Labs community and lend a hand to our efforts. Make sure to check it out.\r\n\r\n", "date_published": "2009-01-10 21:33:21", "comment_count": 0, "slug": "how_you_can_help"}}, {"pk": 34, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2009-01-08 18:06:51", "author": 5, "timestamp": "2009-01-08 18:06:51", "markup": "restructuredtext", "title": "Looking At The NYTimes Congress API", "excerpt": "Earlier this week the New York Times released their `Congress API`_ second politics API (following up on the release of their `Campaign Finance API`_ late last year).  Here at Sunlight Labs we are always happy to see new APIs that wrap government data and there is definitely a lot to like here, although there are some things that will hopefully change to make the API more useful to the community at large.\r\n\r\n\r\n.. _`Congress API`: http://open.blogs.nytimes.com/2009/01/08/introducing-the-congress-api/\r\n.. _`Campaign Finance API`: http://developer.nytimes.com/docs/campaign_finance_api\r\n", "content": "Earlier this week the New York Times released their `Congress API`_ second politics API (following up on the release of their `Campaign Finance API`_ late last year).  Here at Sunlight Labs we are always happy to see new APIs that wrap government data and there is definitely a lot to like here, although there are some things that will hopefully change to make the API more useful to the community at large.\r\n\r\n\r\nThe Good\r\n========\r\n\r\nThere can never be too many ways to access this kind of government data, the more that are out there, the more they will be used, and ultimately that is one of our primary goals here at Sunlight Labs.  There are a few really nice design decisions that are worth noting.\r\n\r\n* The API includes methods not only for details on Members of Congress but is more focused on their roll call votes over time.\r\n* The data goes back as far as any readily available sources go, this is a nice touch that makes it easier on people looking to do historical analysis of the behavior of Congress. (Data goes back to at least 1992 but as early as 1947 for Senate biographical data)\r\n* The `RESTful`_ API is easy to use and experiment with from a browser.  The four methods are easy to understand and don't require jumping through hoops.  They also use standard `HTTP error codes`_.\r\n* They have chosen to use Bioguide IDs as their primary keys, which means that they haven't introduced yet another identifier for members of congress. [*]_\r\n* The data returned is reasonably minimal throughout most of the API, you get back pretty much what you ask for without a ton of irrelevant information to sort through.\r\n\r\nThe Bad\r\n=======\r\n\r\nAs nice as the new API is from a technical perspective, there are a few problems that a would-be user might have.  These problems primarily stem from the `Terms of Use`_\r\n\r\n* 5000 requests per day limit seems a bit restrictive.  (it is prominently noted that this is subject to change, but no indication whether this change would likely be in the upwards or downwards direction)\r\n* Section 1.e.vi of the ToS states that you may not use the API for any service that competes with products or services offered by NYT.  This comes across as overly broad as one can imagine that a lot of this vote information would be useful for organizations that can in some way be considered 'in competition with the NY Times.' (such as local news organizations).\r\n* Section v of the attribution restrictions prohibits archiving of data for access by users \"at any future date after you have finished using the service\" which would prohibit something like building an application that used the NY Times as an initial source for votes but had no need to hit the Congress API regularly.\r\n\r\nThe Ugly\r\n========\r\n\r\nOk, not much is really ugly about this API, as I said it is quite simple and elegant.  At the time of this post however output is XML only and let's face it, XML is ugly.  (I have hopes that this will change as the Campaign Finance API is JSON, XML, or Serialized PHP with JSON by default.)\r\n\r\nThe New York Times should be applauded for their effort in creating both this and the Campaign Finance API, hopefully the future holds more of this from them and we look forward to seeing what they release next.  I hope that some of the more troublesome provisions can be revised or clarified as to make it beneficial to a wider audience.\r\n\r\n.. _RESTful: http://en.wikipedia.org/wiki/Representational_State_Transfer\r\n.. _`Congress API`: http://open.blogs.nytimes.com/2009/01/08/introducing-the-congress-api/\r\n.. _`Campaign Finance API`: http://developer.nytimes.com/docs/campaign_finance_api\r\n.. _`HTTP error codes`: http://developer.nytimes.com/docs/reference/errors\r\n.. _`Terms of Use`: http://developer.nytimes.com/Api_terms_of_use\r\n\r\n.. [*]  As the maintainer of the `Sunlight Labs API <http://services.sunlightlabs.com/api/>`_ (primarily focused on providing ID lookups for legislators) I cannot emphasize how much I appreciate new websites and services using one of the standard ids.\r\n", "date_published": "2009-01-08 18:05:09", "comment_count": 6, "slug": "nytimes-congress-api"}}, {"pk": 32, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-18 13:09:07", "author": 6, "timestamp": "2008-12-18 13:09:07", "markup": "markdown", "title": "Welcome to Sunlight Labs 2.0", "excerpt": "You may have noticed, we've redesigned our site. Our awesome new designer, Ali made it and we're excited to have her as part of our team. And we're excited about the new website too. Especially those things on the right that allow us to update what we're working on via [twitter](http://twitter.com).\r\n\r\nThe website isn't the only thing we're redesigning. We're also redesigning how Sunlight Labs works. We're clearly no longer the six-month pilot project we were chartered to be [31 months ago](http://blog.sunlightfoundation.com/2006/05/10/launching-sunlight-labs/). We're now a team of great developers using technology to change the way our Congress operates and have been for quite some time. So we're long overdue for a gear-shift in the way we think about Sunlight Labs and how we work. We see three fundamental shifts in how we think about ourselves now vs. how the Labs was conceived.", "content": "You may have noticed, we've redesigned our site. Our awesome new designer, Ali Felski, made it and we're excited to have her as part of our team. And we're excited about the new website too. Especially those post-its on the right that allow us to update what we're working on via [twitter](http://twitter.com).\r\n\r\nThe website isn't the only thing we're redesigning. We're also redesigning how Sunlight Labs works. We're clearly no longer the six-month pilot project we were chartered to be [31 months ago](http://blog.sunlightfoundation.com/2006/05/10/launching-sunlight-labs/). We're now a team of great developers using technology and the social web to open up the way our government in Washington operates and have been for quite some time. So we're long overdue for a gear-shift in the way we think about Sunlight Labs and how we work. There are three fundamental shifts in how we think about ourselves now vs. how the Labs was conceived.\r\n\r\nThe first shift is thinking of ourselves as a *permanent* institution, and that we are more than a \"mash-up lab.\" We now develop long-standing projects like [Capitol Words](http://www.capitolwords.org) and people depend on many of our [APIs](http://sunlightlabs.com/projects/) to run their websites. We think that's pretty cool. And we're just beginning. We have two more very large projects in the wings: The Sunlight Data Commons, and the [Pew Subsidy Project](http://subsidyscope.com).\r\n\r\nThe second shift is understanding that much of the software we write is useful to society and should be *open* so that anyone can use it to further government accountability and transparency. So, we're now publishing all of our source under various open source licenses, and as much of our data as bandwidth allows as CC-BY. We're beginning to make that transition [now](/projects/), and all new projects we create will be turned into Open Source projects. And to the best of our ability, all of the data our software produces will also be available in that form.\r\n\r\nThird, we now realize that many of the problems we face cannot be solved through traditional business methods. Whether it is [Name Standardization](http://wiki.sunlightlabs.com/index.php?title=Name_Standardization) or just finding new ways to [take in data](http://wiki.sunlightlabs.com/index.php?title=Data_Intake) there's a lot of work to be done, and it is probably too much for a small team of eight to do. We need to, instead, begin building a vessel that will enable outside developers to also enjoy the privilege of changing how their government works (and vice versa, how people work with their government). In doing so, we are inspired by our heroes at [Apache](http://www.apache.org) and [Mozilla](http://www.mozilla.org) Foundation, who have blazed a path for open source developers worldwide.\r\n\r\n_In short: Sunlight Labs is an open source development team dedicated to making their government accountable._ And we want you to be a part of it. \r\n\r\nNow, to that end, I'd like to announce that with this redesign of the Labs, and a redesign of the Executive Branch of Government, Sunlight is launching its Apps for America contest. The award will be given to the best applications that use our community's data, tools and APIs to make their government good. [Check it out](/contest) and help us build some apps that shed light on our new Congress.\r\n\r\nWe're also opening up our first of hopefully many [email lists](http://groups.google.com/group/sunlightlabs) so that we can begin setting a course for new open source projects, and getting your help on our existing ones. And we'll need your ideas and proposals for new open source projects as well. \r\n\r\nWe hope you're as excited as we are by this new chapter of the Sunlight Labs story. \r\n\r\nOnward!\r\n\r\n--Clay\r\n", "date_published": "2008-12-19 10:10:28", "comment_count": 2, "slug": "labs2"}}, {"pk": 33, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-18 17:00:38", "author": 5, "timestamp": "2008-12-18 17:00:38", "markup": "restructuredtext", "title": " Feed A Need: Donate Time to Sunlight for the Holidays", "excerpt": "reddit.com_ recently unveiled FeedANeed.org_, an charity drive designed to connect volunteers with worthy nonprofits in need of some form of assistance from the community.\r\n\r\nFeedANeed allows anyone to vote for worthy nonprofits and/or sign up to volunteer some chunk of their time to top vote-getting organizations.  Volunteers that complete their pledged time by February 14th, 2009 will be entered into a drawing for prizes from various sponsors (from a wide variety of organizations Wired_, EFF_, Baconfreak_, and even `Sunlight Foundation`_).\r\n\r\n.. _reddit.com: http://reddit.com\r\n.. _FeedANeed.org: http://FeedANeed.org\r\n.. _Wired: http://wired.com\r\n.. _EFF: http://eff.org\r\n.. _Baconfreak: http://baconfreak.com\r\n.. _FeedANeed: http://wired.reddit.com/feedaneed/?s=top\r\n.. _Sunlight Foundation: http://sunlightfoundation.com", "content": "reddit.com_ recently unveiled FeedANeed.org_, an charity drive designed to connect volunteers with worthy nonprofits in need of some form of assistance from the community.\r\n\r\nFeedANeed allows anyone to vote for worthy nonprofits and/or sign up to volunteer some chunk of their time to top vote-getting organizations.  Volunteers that complete their pledged time by February 14th, 2009 will be entered into a drawing for prizes from various sponsors (from a wide variety of organizations Wired_, EFF_, Baconfreak_, and even `Sunlight Foundation`_).\r\n\r\nThe reddit community is somewhat technical so there is a place to list design or development skills, but they are certainly not a requirement.  You can indicate you are simply willing to do a couple hours of data entry or image recognition, something akin to Amazon's Mechanical Turk.  When we're talking about government data there is no shortage of material that would benefit from a team of volunteers being thrown at it.\r\n\r\n**Voting ends December 23rd @ 7pm Eastern - before the polls close please head over to FeedANeed_ and cast a vote for us (and your other favorite nonprofits).**\r\n\r\nAfter you vote for us please consider signing up to donate as little as two hours of your time to a worthy cause.  A lot of nonprofits are facing the prospect of reduced donations due to the current economic climate, and this is a great way to support your favorite causes without spending a dime.  Quite a few of us here at Sunlight will be returning the favor by volunteering a couple hours of our time elsewhere through FeedANeed as a way of saying thanks.\r\n\r\n\r\n.. _reddit.com: http://reddit.com\r\n.. _FeedANeed.org: http://FeedANeed.org\r\n.. _Wired: http://wired.com\r\n.. _EFF: http://eff.org\r\n.. _Baconfreak: http://baconfreak.com\r\n.. _FeedANeed: http://wired.reddit.com/feedaneed/?s=top\r\n.. _Sunlight Foundation: http://sunlightfoundation.com", "date_published": "2008-12-18 16:59:48", "comment_count": 0, "slug": "feed_a_need"}}, {"pk": 31, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 1, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "Under the hood of the DNC and RNC convention sites", "excerpt": "I took a few minutes this morning to look at the technology that powers the DNC and RNC convention web sites. It is always interesting to see what technological decisions different organizations take when they are trying to accomplish similar goals.", "content": "I took a few minutes this morning to look at the technology that powers the DNC and RNC convention web sites. It is always interesting to see what technological decisions different organizations take when they are trying to accomplish similar goals.\r\n\r\n#### Democratic National Convention\r\n\r\n<a href=\"http://demconvention.com/\">http://demconvention.com/</a>\r\n\r\nThe URL for the web site was registered with <a href=\"https://www.domaindiscover.com/\">Domain Discover</a>. The whois information lists the registrant as:\r\n\r\nDemocratic National Committee  \r\n430 S. Capitol St. S.E.  \r\nWashington, DC 20003  \r\n\r\nFor all of the domain squatters out there, the domain expires in November, so keep an eye out. My best guess is that the servers are hosted by <a href=\"http://www.verizonbusiness.com/\">Verizon Business</a>. <a href=\"http://www.servint.net/\">ServInt</a>, based in McLean, VA, provides the hosting for the name servers which are also used by dnc.org and democrats.org, among others.\r\n\r\nThe DNC convention web site is served from an <a href=\"http://httpd.apache.org/\">Apache</a> 2.0.52 web server running on <a href=\"http://www.redhat.com/\">Red Hat Linux</a>. <a href=\"http://www.silverstripe.com\">SilverStripe</a>, a PHP-based, open source content management system, is used to power the site. The main page of the site has a XHTML 1.0 Transitional doctype, but is served as text/html and is not valid XHTML. The site is laid out using divs to define logical elements within the page and CSS to position the elements.\r\n\r\nThe following are screenshots of the page with JavaScript disabled and with both JavaScript and CSS disabled.\r\n\r\n<a href=\"http://blog.sunlightlabs.com/wp-content/uploads/2008/09/dnc_noscript.png\"><img class=\"aligncenter size-thumbnail wp-image-48\" title=\"DNC, no script\" src=\"http://blog.sunlightlabs.com/wp-content/uploads/2008/09/dnc_noscript.png\" alt=\"DNC, no script\" width=\"150\" height=\"115\" /></a>\r\n\r\n<a href=\"http://blog.sunlightlabs.com/wp-content/uploads/2008/09/dnc_noscript_nostyle.png\"><img class=\"aligncenter size-thumbnail wp-image-49\" title=\"DNC, no script, no style\" src=\"http://blog.sunlightlabs.com/wp-content/uploads/2008/09/dnc_noscript_nostyle.png\" alt=\"DNC, no script, no style\" width=\"57\" height=\"150\" /></a>\r\n\r\n#### Republican National Convention\r\n\r\n<a href=\"http://gopconvention2008.com/\">http://gopconvention08.com/</a>\r\n\r\nThe RNC registered their convention's domain with <a href=\"http://www.godaddy.com\">GoDaddy</a>. The registrant is listed as:\r\n\r\nRoman Buhler  \r\n4056 41st Street North  \r\nMcClain [__sic__], Virginia 22101  \r\n\r\n<a href=\"http://www.opensecrets.org/indivs/search.php?name=Buhler&amp;state=VA&amp;zip=&amp;employ=&amp;cand=&amp;c2008=Y&amp;sort=N&amp;capcode=4425k&amp;submit=Submit\">Mr. Buhler</a> is the president of <a href=\"http://www.opensecrets.org/lobby/firmsum.php?lname=Roman+Buhler+%26+Assoc&amp;year=2008\">Roman Buhler &amp; Associates</a>, a lobbying firm based in McLean, VA. Mr. Buhler also <a href=\"http://www.legistorm.com/person/Roman_Buhler/48145.html\">served as counsel</a> of the House Administration Committee from 1989 to 2003. Hosting for the web servers and DNS is provided by <a href=\"http://www.smartechcorp.net/\">Smartech</a>, based in Chattanooga, TN.\r\n\r\nThe web site is served by <a href=\"http://www.iis.net/\">Microsoft IIS</a> 6.0 using <a href=\"http://microsoft.com/\">Microsoft's</a> <a href=\"http://www.asp.net/\">ASP.Net</a> language. The main page declares a generic XHTML namespace, but does not declare a doctype, is served as text/xml, and does not validate as XHTML. The site is laid out using HTML tables.\r\n\r\nThe following are screenshots of the page with JavaScript disabled and with both JavaScript and CSS disabled.\r\n\r\n<a href=\"http://blog.sunlightlabs.com/wp-content/uploads/2008/09/rnc_noscript.png\"><img class=\"aligncenter size-thumbnail wp-image-50\" title=\"RNC, no script\" src=\"http://blog.sunlightlabs.com/wp-content/uploads/2008/09/rnc_noscript.png\" alt=\"RNC, no script\" width=\"150\" height=\"134\" /></a>\r\n\r\n<a href=\"http://blog.sunlightlabs.com/wp-content/uploads/2008/09/rnc_noscript_nostyle.png\"><img class=\"aligncenter size-thumbnail wp-image-51\" title=\"RNC, no script, no style\" src=\"http://blog.sunlightlabs.com/wp-content/uploads/2008/09/rnc_noscript_nostyle.png\" alt=\"RNC, no script, no style\" width=\"150\" height=\"53\" /></a>", "date_published": "2008-09-02 20:20:40", "comment_count": 1, "slug": "under-the-hood-of-the-dnc-and-rnc-convention-sites"}}, {"pk": 30, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 6, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "Google Spreadsheet and the Sunlight Labs API", "excerpt": "James is finishing up a tweak to the Sunlight Labs API that allows for fairly sophisticated search for members of Congress, it isn't \"published\" yet but it is active so if you want to experiment you're welcome to try it out, but for now it is \"unofficial\".", "content": "James is finishing up a tweak to the Sunlight Labs API that allows for fairly sophisticated search for members of Congress, it isn't \"published\" yet but it is active so if you want to experiment you're welcome to try it out, but for now it is \"unofficial\".\r\n\r\nHere's the deal: We wanted a better way for people to search for members, as members of congress are often times referred to by different names-- think \"Ted Kennedy,\" \"Edward Kennedy,\" \"Teddy Kennedy\" etc. Whether it is nicknames or typos, it makes analyzing data difficult if names are not standardized.\r\n\r\nWe're not saying we've come up with a complete solution to name standardization or even congressional name standardization, but we've got a simple solution that might make some lives easier. To demonstrate, we'll use Google Spreadsheets. Follow along at home!\r\n\r\n__Step 1:__ Get an API key from Sunlight Labs [here](http://services.sunlightlabs.com/api/ \"Sunlight Labs API\").\r\n\r\n__Step 2:__ Create a Google Spreadsheet\r\n\r\n__Step 3:__ Name the columns of your spreadsheet \"Member\", \"Firstname\", \"Lastname\" so it looks like this:\r\n\r\n![](http://plusoneme.com/step1.jpg)\r\n\r\n__Step 4:__ Let's add some wacky mispellings and some semi-dirty data like below:\r\n\r\n![](http://plusoneme.com/step2.jpg)\r\n\r\n__Step 5:__ Here's where it gets fun. We'll use the importXML function in Google Spreadsheets to take the values of our dirty data and send them to the Sunlight Labs API, and get a firstname. Enter this code into your spreadsheet:\r\n\r\n    =importXML(\"http://services.sunlightlabs.com/api/legislators.search.xml?apikey=YOURAPIKEYHERE&amp;name=\"&amp;A2,\"//firstname\")\r\n\r\nSee below for an example:\r\n\r\n![](http://plusoneme.com/step3.jpg)\r\n\r\n__Step 6:__ Do the same for the last name column, but change your call to parse the lastname, like so:\r\n\r\n    =importXML(\"http://services.sunlightlabs.com/api/legislators.search.xml?apikey=YOURAPIKEYHERE&amp;name=\"&amp;A2,\"//lastname\")\r\n\r\n__Step 7:__ Finish it up! Select the first values of those newly processed columns and fill in the rest of the values like so:\r\n\r\n![](http://plusoneme.com/step4.jpg)\r\n\r\nNeat! Clean easy name cleanup in your spreadsheet!", "date_published": "2008-08-21 20:55:38", "comment_count": 0, "slug": "google-spreadsheet-and-the-sunlight-labs-api"}}, {"pk": 29, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 5, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "\"Cool project, what CMS did you guys use?\"", "excerpt": "Sunlight Labs is often asked \"What CMS do you use?\". James discusses our development philosophy and the drawbacks of CMS selection.", "content": "#### Pass223.com\r\n\r\nI recently worked on [Pass223.com](http://pass223.com \"Pass 223\"), a simple site that urges the Senate to pass a piece of legislation that requires the Senate to adhere to the same electronic financial disclosure rules in place for representatives and presidential candidates.\r\n\r\nPass223.com is similar to that of hundreds of related action sites: choose a legislator, call them, report results, repeat.\u00a0 I wrote the code, our esteemed creative director Kerry did the bulk of the design, and various others here at Sunlight helped to refine the concept and wording of the site and call script.\r\n\r\nIt was a bit surprising seeing how positive the [feedback](http://www.dailykos.com/story/2008/8/6/9501/66080/422/562998) has been for such a simple site.\u00a0 A number of people have been pointing to Pass223 as an example of how this type of thing should be done.  Most of that credit goes to the team that worked together to revise the awesomely straightforward script.\r\n\r\nThe other question that has come up is what content management system (CMS) Pass223 was done on and what legislative database it was built on top of.  This made me think about the other reason Pass223 was able to come together the way that it did, the tools used behind the scene.\r\n\r\n#### When all you have is a hammer...\r\n\r\nIt seems, especially in the nonprofit world where developers are sparse, that content management systems like [Drupal](http://drupal.org/ \"Drupal :(\") are considered the solution to every problem that arises.\u00a0 Because Content Management Systems can not possibly do everything that organizations want they are left with two options: attempt to mangle the CMS to do things it was never intended to do, or alternatively not get what they actually want.\u00a0 Because of the difficulty in dealing with the massive codebases of most CMSs, they often find themselves accepting both results.\u00a0 A great deal of time is therefore sunk into a project and in the end things still don't work quite how they were planned.\r\n\r\nThe supposed benefit of a CMS is the speed of deployment and ease of use, but as [Jeremy's recent post about LetOurCongressTweet](http://blog.sunlightlabs.com/2008/07/11/from-idea-to-production-in-six-hours/ \"LOCT post\") mentioned, we are able to rapidly create sites without the use of a CMS.\u00a0 And in reality, struggling to fit an innovative project or idea into the rigid structure of a CMS is not easy nor fast.\r\n\r\nA better use of the time and money spent maintaining and modifying complex CMS installations would be to spend that time learning and deploying sites in a framework such as [Django](http://djangoproject.com Django :)\") or [Ruby on Rails](http://www.rubyonrails.org/ \"Ruby on Rails\").\r\n\r\n#### Perfectionists with Deadlines\r\n\r\nDjango in particular was created to solve this problem.  Working with a bloated CMS forces you to make a decision between getting what you want and getting something fast, and more often than not you wind up with neither.  It is because of this that a team originally working at the [Lawrence Journal-World](http://www2.ljworld.com/) newspaper built Django to meet their needs as \"perfectionists with deadlines.\"\r\n\r\nFrameworks like Django provide all of the pieces of commonly used functionality, user registration, Object-Relational mappers to avoid dealing with the database directly, caching, and a ton more.  All of these pieces are given to you without any mandate that they must all be used, they are simply building blocks from which your particular project can choose to use or not.  A large site such as  [EarmarkWatch](http://earmarkwatch.org) may need complex user profiles, whereas it is possible to eschew all of the unneeded modules and build something as quick and simple as Pass223.com.\r\n\r\nUltimately, unless some CMS already provides exactly what you want, it is far easier to build a project from reusable components within a framework than to attempting to teach an old CMS a new trick.  One of the reasons that Pass223.com seems to impress people used to looking at the typical contact-your-legislator forms is because we had the flexibility to build what we wanted.", "date_published": "2008-08-07 18:04:44", "comment_count": 4, "slug": "cool-project-what-cms-did-you-guys-use"}}, {"pk": 28, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 1, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "From Idea to Production in Six Hours", "excerpt": "We recently decided to launch a petition-like site that uses Twitter as the organizing method; using one of the very technologies that are impacted by Congressional Web use restrictions. We knew this had to be timely to have an impact, so the decision was made to have the Web site completed by the end of the day. That gave Kerry Mitchell, our fearless Creative Director, and I about six hours to get the site completed.", "content": "There has been some [commotion](http://blog.sunlightfoundation.com/2008/07/10/let-our-congress-tweet/) over the past few days regarding Congresstional Web use restrictions. The rules are inadequate for the current state of the Web and must be rewritten to reflect changes in technology. Republicans and Democrats have been going back-and-forth over proposed changes to the existing rules; one side claiming the other is trying to stifle their communication. While they keep on bickering, we wanted to raise awareness of these Web use restrictions and get people involved.\r\n\r\nWe decided to launch a petition-like site that uses Twitter as the organizing method; using one of the very technologies that are impacted by Congressional Web use restrictions. We knew this had to be timely to have an impact, so the decision was made to have the Web site completed by the end of the day. That gave Kerry Mitchell, our fearless Creative Director, and I about six hours to get the site completed.\r\n\r\nSo how did it go?\r\n\r\n#### Twitter\r\n\r\nAs LOCT is a petition-like site, it is important to get a list of the people that are following the LOCT Twitter account. Twitter has a very nice API that makes it easy to pull information from the service. To get JSON list of people following your Twitter account, just send an HTTP GET request to <code>http://twitter.com/statuses/followers.json</code> using [HTTP Basic Authentication](http://en.wikipedia.org/wiki/Basic_authentication_scheme) with your username and password. You can also get a list of people following other user accounts from <code>http://twitter.com/statuses/followers/&lt;username&gt;.json</code>; no authentication necessary. We query Twitter for this information and cache it locally in a database.\r\n\r\nUnfortunately, due to Twitter's recent performance issues, many of the nicest features have either been limited or disabled, making it almost impossible to use Twitter exclusively for LOCT. We needed to get a list of tweets that mentioned LOCT, but couldn't with the current performance restrictions in place. If only there was another service that provided this functionality.\r\n\r\n#### And there is!\r\n\r\n[Summize](http://summize.com/) rocks. Based right here in the greater Washington metropolitan area, Summize is tweet search service that has one of the few direct feeds into every tweet that is twittered. They also have an awesome API that makes it dead simple to search all tweets. <code>http://summize.com/search.json?q=%23loct08</code>. That is all you need to get search results in JSON. Just like the list of followers from Twitter, the results are cached locally for Maximum Performance<sup>TM</sup>.\r\n\r\n#### I would follow Django if it had a Twitter account\r\n\r\nAs with almost all projects created by [Sunlight Labs](http://sunlightlabs.com), [Let Our Congress Tweet](http://letourcongresstweet.org/) is writting using [Django](http://djangoproject.com), a [Python](http://python.org) Web development framework. I love Django. It simplifies development by providing [object-relational mapping](http://en.wikipedia.org/wiki/Object-relational_mapping), templating, and other features in an unobtrusive way.\r\n\r\nDeveloping in Django is already quite rapid, but by reusing existing code we can develop at an unheard-of pace. Writing a reusable Django application is quite easy as it is nothing more than a standard Python module that can be used in any project.\r\n\r\n#### Feedinating the countryside\r\n\r\nA few weeks ago we released a new version of the [Sunlight Foundation](http://sunlightfoundation.com/) Web site. The old, infuriating Drupal installation was replaced with a slick Django application that was written in-house. One of the main features of the new site is feed aggregator that pulls in the recent blog posts from across the Sunlight-influenced transparency network. To accomplish this, we wrote _Feedinator_, a Django feed aggregator application that makes it easy to pull in feeds from multiple blogs and display them in different ways on a Web site.\r\n\r\nWe use Feedinator on Let Our Congress Tweet to pull in the feed of Tweets from the [LOTC08 twitter account](http://twitter.com/loct08/) and a [del.icio.us](http://del.icio.us/) feed of Web sites that have mentioned LOCT. By writing Feedinator in a way that makes it easily reusable, we were able to start incorporating feeds into LOCT in a matter of minutes.\r\n\r\nIf you would like to use Feedinator in your own project, you are in luck. We plan on releasing the code in the near future as well as the code for a few other Django applications and Python modules.\r\n\r\n#### Designers are useful\r\n\r\nWhile I was coding, Kerry took care of the design, CSS, and HTML. A few minutes of converting the HTML into Django templates and the site was up and running.\r\n\r\nSo that's it! After throwing in a few cron entries and some Apache configuration files, the site went live. We went from an idea to a production ready Web site in about six hours. Sure, the it isn't overly complicated, but we're proud of it nonetheless.", "date_published": "2008-07-11 16:07:59", "comment_count": 1, "slug": "from-idea-to-production-in-six-hours"}}, {"pk": 27, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 6, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "On Baseball and Congress", "excerpt": "<p>Modern baseball&#8217;s origins are something historians don&#8217;t have a good read on. If you look at the <a href=\"http://en.wikipedia.org/wiki/Origins_of_baseball\">Origins of Baseball</a> article on Wikipedia, you&#8217;ll see that we don&#8217;t know very much about where the rules came from, but it formalized somewhere around 1845 when the Knickerbocker Club of New York City began to play baseball against the New York Nine. In 1857 16 clubs finally sent delegates to a convention to standardize the rules and standardize America&#8217;s Pastime.</p>", "content": "<p>Modern baseball&#8217;s origins are something historians don&#8217;t have a good read on. If you look at the <a href=\"http://en.wikipedia.org/wiki/Origins_of_baseball\">Origins of Baseball</a> article on Wikipedia, you&#8217;ll see that we don&#8217;t know very much about where the rules came from, but it formalized somewhere around 1845 when the Knickerbocker Club of New York City began to play baseball against the New York Nine. In 1857 16 clubs finally sent delegates to a convention to standardize the rules and standardize America&#8217;s Pastime.</p>\r\n\r\n<p><a href=\"http://en.wikipedia.org/wiki/Baseball_statistics\">Baseball statistics</a> have their own story. A fellow named <a href=\"http://en.wikipedia.org/wiki/Henry_Chadwick_(writer)\">Henry Chadwick</a> was the first to start using statistics to judge a player&#8217;s performance. It was a few years after the sport was invented and formalized that this young journalist would give himself the goal of creating &#8220;numerical evidence as that would prove what players helped or hurt a team win.&#8221;</p>\r\n\r\n<p>It took nearly 100 years for baseball statistics to make it to the common man and woman. It wasn&#8217;t until 1951 when a researcher named Hy Turkin published the Encyclopedia of Baseball that used a computer to compile statistics for the first time. It wasn&#8217;t until 1977 that decent, predictive and objective statistical methods called <a href=\"http://en.wikipedia.org/wiki/Sabermetrics\">sabermetrics</a> were invented and distributed. Sabermetrics are the analytical methods that Theo Epstein used to break a curse and build a World Series winning Boston Red Sox. He even hired Sabermetrics&#8217; inventor to work for the Red Sox.</p>\r\n\r\n<p>It took over a century to invent and distribute the objective performance measuring statistics we use today to evaluate baseball players. To tell whether or not Greg Maddux is a better pitcher than Pedro Martinez. </p>\r\n\r\n<p>But though <a href=\"http://en.wikipedia.org/wiki/First_Continental_Congress\">Congress</a> has been around for nearly 234 years we still don&#8217;t have an objective way to tell whether or not my namesake, <a href=\"http://en.wikipedia.org/wiki/Henry_Clay\">Henry Clay</a> was as effective of a speaker as <a href=\"http://en.wikipedia.org/wiki/Nancy_Pelosi\">Nancy Pelosi</a>. This isn&#8217;t to say we haven&#8217;t been making up our own statistics. We&#8217;ve been compiling subjective <a href=\"http://www.lcv.org/scorecard/\">scorecards</a> for years. But we need a process of standardizing our statistics, publishing how they&#8217;re calculated, and we need to build a system for authenticating and delivering those results.</p>\r\n\r\n<p>It took us over 100 years to get good baseball stats, but this isn&#8217;t to say that the data didn&#8217;t exist or wasn&#8217;t being recorded. The data was still there. Here&#8217;s the full stats on the <a href=\"http://www.baseball-reference.com/teams/CHC/1876.shtml\">1876 Chicago White Stockings</a>. People were watching, keeping score, logging the games and recording the data and even making their own statistics out of it. But it was the process of standardizing the statistics, publishing how they&#8217;re to be calculated and sharing the results that made these <em>subjective</em> metrics effective.</p>\r\n\r\n<p>So what metrics out there are effective in evaluating our legislators? Off the top of my head, here&#8217;s some elementary ones:</p>\r\n\r\n<ol>\r\n<li><p>Attendance Percentage: The percent of time a member attends Congress when it is in session.</p></li>\r\n<li><p>Vote percentage: The percentage of time a member has voted when they&#8217;ve had the opportunity to do so.</p></li>\r\n<li><p>Sponsored Bills Per Term: The average number of bills sponsored and co-sponsored per term</p></li>\r\n<li><p>Sponsored Bills Passed Per Term: The average number of sponsored and co-sponsored bills passed per term</p></li>\r\n<li><p>Party percentage: The percentage of time the member votes with their political party</p></li>\r\n<li><p>Vote Victory Percentage: The percentage of time the member votes with a bill that passes.</p></li>\r\n</ol>\r\n\r\n<p>These are just obvious building blocks of a much more sophisticated statistical system. All these statistics exist right now. Sunlight&#8217;s partner, <a href=\"http://opencongress.org\">Open Congress</a> and <a href=\"http://govtrack.us\">GovTrack.us</a> and many more track their congressional statistics in their own way as do many others. In order to do it right we need:</p>\r\n\r\n<ol>\r\n<li><p>Standardization: We need to be calculating these things and naming these statistics the same way everywhere.</p></li>\r\n<li><p>Comparison: Statistics are not relevant unless they&#8217;re in context. We need to be able to create a ranked 1-535 list for every statistic we standardize and create</p></li>\r\n<li><p>Adoption: They need to be adopted and as pervasive as RBIs and ERAs.</p></li>\r\n<li><p>More: We need more statistics made from the data that congress generates that provide &#8220;numerical evidence&#8221; about the effectiveness of congress. We need our own &#8220;sabermetrics&#8221; that objectively evaluate whether or not a Member of Congress is a effective at representing those that chose to elect them. The ones I've listed aren't even close to being accurate predictors.</p></li>\r\n</ol>\r\n\r\n<p>So let this be a post to start a discussion amongst the transparency community about how we can begin standardizing our own objective statistics, making them useful and centralized. Let&#8217;s start working together to invent new statistics for how our members can be evaluated. </p>\r\n", "date_published": "2008-06-30 02:49:54", "comment_count": 3, "slug": "call-to-action-learn-from-baseball"}}, {"pk": 26, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 6, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Fun with CapitolWords", "excerpt": "We launched <a href=\"http://www.capitolwords.org\">Capitol Words</a> just a couple weeks ago and got a <a href=\"http://www.boingboing.net/2008/06/19/daily-congressional.html\">really</a> <a href=\"http://news.oreilly.com/2008/06/requesting-a-mashup.html\">great</a> <a href=\"http://www.metafilter.com/72702/Capitol-Words-US-Congress-In-A-Word-A-Day\">reception</a> from the blogs. I\u2019m two weeks in to my new duties as Director of Sunlight Labs and while I didn\u2019t have much (really, anything) to do with the project's success, I am really excited about it. With the <a href=\"http://www.capitolwords.org/api/\">CapitolWords API</a> we can start doing some interesting analysis of overall word-usage in Congress.", "content": "We launched <a href=\"http://www.capitolwords.org\">Capitol Words</a> just a couple weeks ago and got a <a href=\"http://www.boingboing.net/2008/06/19/daily-congressional.html\">really</a> <a href=\"http://news.oreilly.com/2008/06/requesting-a-mashup.html\">great</a> <a href=\"http://www.metafilter.com/72702/Capitol-Words-US-Congress-In-A-Word-A-Day\">reception</a> from the blogs. I\u2019m two weeks in to my new duties as Director of Sunlight Labs and while I didn\u2019t have much (really, anything) to do with the project's success, I am really excited about it. With the <a href=\"http://www.capitolwords.org/api/\">CapitolWords API</a> we can start doing some interesting analysis of overall word-usage in Congress.\r\n\r\nSome of this is obvious and you can see at the surface. Check out the screenshots below:\r\n<img src=\"http://img.skitch.com/20080629-c7ysh38su9w462su2ci9pbk1mj.jpg\" alt=\"June, 2005\" />\r\n\r\n<img src=\"http://img.skitch.com/20080629-x9ngxqcys8qndyk1wh1xcfitws.jpg\" alt=\"June, 2007\" />\r\n\r\n<img src=\"http://img.skitch.com/20080629-cdfu7h3rt82sre8tqc97b8ct92.jpg\" alt=\"June, 2008\" />\r\n\r\nJune seems to be predominantly about energy and oil. Septembers of even numbered years tend to be about security and intelligence. March tends to be about budgets and amounts.\r\n\r\nNeat! Josh wrote most of the code and handled the architecture of the system. Garrett who heads our <a href=\"Louis\">http://www.louisdb.org</a> project also had a big hand in concieving and building the application. Of course Kerry, our wonderful Creative Director helped make the user interface and designed the site. It is written in Django and MySQL. Great work guys!", "date_published": "2008-06-29 19:08:25", "comment_count": 0, "slug": "fun-with-capitolwords"}}, {"pk": 25, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 3, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Open Government Site Govtrack Goes Open Source", "excerpt": "", "content": "<img align=\"right\" title=\"GovTrack.US logo\" id=\"image39\" alt=\"GovTrack.US logo\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2008/04/picture-45.thumbnail.jpg\" /><img align=\"right\" title=\"OpenSource.org logo\" id=\"image40\" alt=\"OpenSource.org logo\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2008/04/picture-52.jpg\" />Josh Tauberer, one of the champs of opening U.S. Government data announced today he has made all of <a title=\"Govtrack.us\" href=\"http://govtrack.us\">Govtrack.us</a> available as open source.\r\n<blockquote>This week I made <a target=\"_blank\" href=\"http://www.govtrack.us/\">www.GovTrack.us</a> officially totally open source.\r\nGovTrack is a website that tracks U.S. federal legislation and also\r\nbuilds the only comprehensive open database of congressional\r\ninformation. While the data behind GovTrack has been provided in the\r\npublic domain for a number of years now, and has been successfully\r\npowering a bunch of other sites like OpenCongress, I've been playing\r\ncatch-up in getting the source code of the website opened up.</blockquote>\r\nRun at get the code! (<a title=\"Govtrack.us source code\" href=\"http://www.govtrack.us/source.xpd\">link</a>)", "date_published": "2008-04-03 14:17:44", "comment_count": 0, "slug": "open-government-site-govtrack-goes-open-source"}}, {"pk": 24, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 3, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Diagram of Financial Support for Public Government Sites", "excerpt": "", "content": "<img width=\"331\" height=\"247\" align=\"right\" alt=\"Diagram of financial support for public government sites\" title=\"Diagram of financial support for public government sites\" src=\"http://visiblegovernment.ca/images/blog/MoneyFlowSmall.PNG\" /><img width=\"337\" height=\"254\" align=\"right\" alt=\"Flows of data from government to non profit government sites\" title=\"Flows of data from government to non profit government sites\" src=\"http://visiblegovernment.ca/images/blog/DataFlowSmall.PNG\" />\r\n\r\nW0ot! Thanks for the excellent diagram WavingSparks.\r\n\r\nIt's very helpful to have such a <a href=\"http://waving.deadsquid.com/?p=29\">nice graphic explaining the data flows</a>. As you discovered, Sunlight is very interested in financially supporting and being part of making transparency information available.\r\n\r\nIn interest of transparency, Sunlight is also supporting <a title=\"Taxpayer for Common Sense\" href=\"http://www.taxpayer.net/\">Taxpayers for Common Sense</a> to update their website and offer smaller transparency grants, too (<a title=\"Sunlight Foundation Grants\" href=\"http://sunlightfoundation.com/grants\">http://sunlightfoundation.com/grants</a>). <a title=\"GovTrack\" href=\"http://govtrack.us\">GovTrack</a> is a pretty efficient and essential website and is nobody's weak link! For more related websites check out:\r\n<ul>\r\n\t<li><a title=\"Insanely Useful Government Data Sites\" href=\"http://sunlightfoundation.com/resources\">http://sunlightfoundation.com/resources</a></li>\r\n\t<li><a title=\"ProgrammableWeb Government Directory\" href=\"http://sunlightfoundation.com/resources\">http://www.programmableweb.com/government</a></li>\r\n</ul>\r\n(Link: <a href=\"http://waving.deadsquid.com/?p=29\">http://waving.deadsquid.com/?p=29)</a>", "date_published": "2008-03-14 22:59:19", "comment_count": 0, "slug": "38"}}, {"pk": 23, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 3, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Open Source Semantics Picking Up Speed", "excerpt": "", "content": "<img align=\"right\" alt=\"Screenshot of size of Metaweb WEX download\" title=\"Screenshot of size of Metaweb WEX download\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2008/02/picture-42.jpg\" />Metaweb has announced an open source release of structured data from Wikipedia. Via the email from get.theinfo email list:\r\n<blockquote><em>\"Hello from Metaweb. We've just released a GFDL licensed extraction of\r\nWikipedia in XML + relational form. Anyone is welcome to use it for\r\nany purpose...\"</em></blockquote>\r\nThis follows Reuters recent announcement of Open Calais API to extract people, places, things, and simple relationships from unstructured text. (We are experimenting with similar techniques of entity tagging via open protocols at Sunlight.) Metaweb's WEX's is 57GB of download-able structured data from the largest peer-production encyclopedia project ever.  The Semantic Web, so long discussed, is now beginning a virtuous cycle of innovation.  We are entering the age of <em>open source semantics</em>. Like compounding interest, Moore's Law, and exercise, results from the cycle of innovation around open source semantics will multiply quickly. If you thought Google circa 2007 is impressive, buckle your seatbelt and reach for your helmet. Things are about to move even faster.\r\n\r\nAddendum: DBPedia is another project extracting data from Wikipedia in the RDF format.\r\n\r\n<strong>Links</strong>: \u2022<a href=\"http://download.freebase.com/wex/\">Metaweb's WEX</a> \u2022<a href=\"http://www.opencalais.com/\">Open Calais</a> \u2022<a href=\"http://www.readwriteweb.com/archives/reuters_calais.php\">ReadWriteWeb on Open Calais</a> \u2022<a title=\"DBPedia\" href=\"http://dbpedia.org\">DBPedia</a>", "date_published": "2008-02-19 13:51:17", "comment_count": 0, "slug": "open-source-semantics-picking-up-speed"}}, {"pk": 22, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 3, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Google's Social Graph API", "excerpt": "", "content": "<img width=\"300\" align=\"right\" alt=\"Screenshot Social Graph API code page\" title=\"Screenshot Social Graph API code page\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2008/02/picture-31.jpg\" />The <a title=\"Social Graph API Code page\" href=\"http://code.google.com/apis/socialgraph/\">Social Graph API page</a> on Google code exemplifies the future of multimedia learning and why open source is being so productive relative to proprietary software development efforts. A two and half minute YouTube video introduces the concept. Links are included to documentation and examples. I've got code, examples, documentation, and even a human giving me a tutorial. (This one intro video could easily be expanded to series of step-by-step instructions.) Five developers might jump on the Social Graph API or 5,000. The right five might matter more than having 5,000 developers. What matters is two points. First, beneficial multimedia is no longer expensive to produce or distribute. In fact, it is becoming a basic skill of people everywhere. Second, the distributed nature of open source mentality encourages people to provide information in economical paired-down forms\u2014even if the code is not open source but merely an API.", "date_published": "2008-02-11 17:53:36", "comment_count": 0, "slug": "34"}}, {"pk": 21, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 4, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "Stay-at-home Servers", "excerpt": "[Microsoft propoganda targets](http://gizmodo.com/photogallery/microserveces08/) our most vulnerable citizens - *our children*!", "content": "[Microsoft propoganda targets](http://gizmodo.com/photogallery/microserveces08/) our most vulnerable citizens - *our children*!", "date_published": "2008-01-09 19:10:26", "comment_count": 0, "slug": "stay-at-home-servers"}}, {"pk": 20, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 5, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "polipoly - A tool for dealing with district boundary polygons", "excerpt": "Finding out what congressional district an address falls within can be a difficult problem.  One solution is to use the polipoly library that we have open sourced to check if a point falls within a polygon boundary of a congressional district.", "content": "Finding out what congressional district an address falls within can be a difficult problem. Typically you need a 9 digit zip code, and the US Postal Service does not make looking up the zip+4 easy.  Even with a zip+4, existing solutions are either expensive or inaccurate and sometimes both.\r\n\r\nIt turns out that the US census bureau publishes <a href=\"http://www.census.gov/geo/www/cob/cd110.html\">polygon files</a> defining the boundaries of all 435 congressional districts. Through geocoding services such as google maps we can easily convert an address to a latitude and longitude and therefore it is possible to determine what district an address lies within by simply testing what polygon it falls within.\r\n\r\nWe <a title=\"recently added a new API method\" href=\"http://sunlightlabs.com/blog/?p=29\">recently added a new API method</a> but one of the major drawbacks is that because it uses Google as a geocoding service we are limited to 50,000 calls a day.  We're aware that some organizations may want to do batch processing of their membership lists, which may means thousands of lookups at once.  In addition to using up all of our geocoding requests, calling a web service for this kind of batch processing isn't efficient.\r\n\r\nTo attempt to meet these challenges we are proud to announce <a title=\"polipoly\" href=\"http://code.google.com/p/polipoly/\">polipoly</a>.  Polipoly is a small python library that uses public domain census shapefiles to allow developers to write simple python scripts that can relate addresses to congressional districts.  And because you're running it with your own Google Maps API key, you are able to use all 50,000 geocoding requests a day.\r\n\r\nFor information on installation and usage head over to the project page. Example source for a web service that works just like <a target=\"_blank\" href=\"http://sunlightlabs.com/api/places.getDistrictFromAddress.php\">places.getDistrictFromAddress</a> has been released, as well as an example of processing a CSV file similar to the kind that could be exported from a database or spreadsheet.\r\n\r\nThis is the second project we have released on Google Code (the first being the <a href=\"http://code.google.com/p/sunlightadk/\">Sunlight ADK</a>) but much more is on the way in terms of open source.", "date_published": "2007-12-10 10:00:58", "comment_count": 0, "slug": "polipoly"}}, {"pk": 19, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 4, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "Crossposted from my personal blog", "excerpt": "If you like APIs and mashups, you should check out [ProgrammableWeb](http://www.programmableweb.com/) - it\u2019s a directory/advice/analysis site for all things API-ish.", "content": "If you like APIs and mashups, you should check out [ProgrammableWeb](http://www.programmableweb.com/) - it\u2019s a directory/advice/analysis site for all things API-ish.\r\n\r\nMost recently, they\u2019ve been working on building a subsection for [government](http://www.programmableweb.com/government) APIs, and have included 5 APIs that we're involved with, as well as APIs from MySociety.org and a whole lot more.\r\n\r\nAnd who knows, we may have another contest someday, involving mashups that combine these various APIs.", "date_published": "2007-11-13 20:43:46", "comment_count": 0, "slug": "crossposted-from-my-personal-blog"}}, {"pk": 18, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 3, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "New Address to Congressional District API Method", "excerpt": "", "content": "James Turk added a powerful address to congressional district method <a target=\"_blank\" href=\"http://sunlightlabs.com/api/places.getDistrictFromAddress.php\">places.getDistrictFromAddress</a> to the <a href=\"http://sunlightlabs.com/api/\">Sunlight Labs API</a>. The method geocodes a street address using Google's Map API and identifies the congressional district polygon in which the address's lat/long falls using <em>up-to-date</em> shape files from Census.gov.\r\n\r\nHere's an example call:\r\n<pre>http://api.sunlightlabs.com/places.getDistrictFromAddress?address=1818+N+St+NW+Washington,+DC&output=xml</pre>\r\nHere's an example result in XML (json also available):\r\n<pre>&lt;results&gt;\r\n&lt;address&gt;1818 N St NW Washington, DC&lt;/address&gt;\r\n&lt;latitude&gt;38.907231&lt;/latitude&gt;\r\n&lt;longitude&gt;-77.042149&lt;/longitude&gt;\r\n&lt;districts&gt;\r\n&lt;district state=\"DC\"&gt;98&lt;/district&gt;\r\n&lt;/districts&gt;\r\n&lt;/results&gt;</pre>\r\nWhen I started at Sunlight Labs in 2006, I heard companies paying annual fees for zipcode to congressional district translation databases and services. This should no longer be the case, at least for small doses of information. First, we are offering this API method. Second, our service uses official congressional district political boundaries now being (or by 2008 will be) updated in real-time by the Census as they learn of changes. So it should be possible for others to code such a service as well. The only cost should be related to number of addresses needing to be coded. Google reasonably imposes 15,000 calls-per-day limitation from same IP address on its service. So we might look into an alternate geocoding service so we can scale. Then again, we might also just fire up instances of EC2 to source the calls from different IP addresses, too.", "date_published": "2007-11-13 19:54:11", "comment_count": 1, "slug": "new-address-to-congressional-district-api-method"}}, {"pk": 17, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 1, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Sunlight API Development Kit", "excerpt": "When working on a prototype, it is often necessary to get a REST web service up and running quickly. It's easy enough to do, but the amount of code that is duplicated for each service can really increase the time to completion. To make the development of REST web services quicker and easier, we have developed the <a href=\"http://sunlightadk.googlecode.com\">Sunlight API Development Kit</a> or, as we affectionately call it, the <a href=\"http://sunlightadk.googlecode.com\">Sunlight ADK</a>. The ADK is a PHP framework that assists in the rapid development of REST web services. We've released the code under the LGPL license.", "content": "<p>When working on a prototype, it is often necessary to get a REST web service up and running quickly. It's easy enough to do, but the amount of code that is duplicated for each service can really increase the time to completion. To make the development of REST web services quicker and easier, we have developed the <a href=\"http://sunlightadk.googlecode.com\">Sunlight API Development Kit</a> or, as we affectionately call it, the <a href=\"http://sunlightadk.googlecode.com\">Sunlight ADK</a>. The ADK is a PHP framework that assists in the rapid development of REST web services. We've released the code under the LGPL license.</p>\r\n<h3>REST Framework</h3>\r\n<p>The core of the ADK is a framework for rapid web service development. You don't have to worry about receiving requests, building XML or JSON, or any other tedious code. All you need to do is write code to populate a RESTResponse object. We take it from there and generate XML or JSON from the object depending on the type of response the user requested.</p>\r\n<h3>Administration Application</h3>\r\n<p>So how do you manage all of the services you've written? We were nice enough to include a web based management application that can be used to manage users and service methods. To create a new method just fill in the method name and the name of the PHP class that implements the service. As long as your service class was dropped in the right folder, you'll be up and running with the method.</p>\r\n<h3>Key Management and Access Controls</h3>\r\n<p>The administration application can also be used to issue keys to users of the web service. You can place usage limits on each service method to limit the number of calls or amount of data that is transferred over a variable time period. You can also create methods that are accessible to a limited number of users. Of course if you prefer to have your service open to everyone then a simple configuration change will allow access to the services without a key.</p>\r\n<h3>Get it and contribute!</h3>\r\n<p>We hope you'll find the Sunlight ADK useful. <a href=\"http://sunlightadk.googlecode.com\">Get a copy now</a> and let us know what you think. If you would like to contribute to the project, which we hope you do, please contact us at <a href=\"mailto:labs@sunlightfoundation.com\">labs@sunlightfoundation.com</a>.", "date_published": "2007-10-19 18:33:45", "comment_count": 0, "slug": "sunlight-api-development-kit"}}, {"pk": 16, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 4, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "Mini-Grants", "excerpt": "If you're a developer-type person, and you are involved in some way in making a difference in the way people interact w/Congress, then you may want to check this out...", "content": "If you're a developer-type person, and you are involved in some way in making a difference in the way people interact w/Congress, then you may want to check this out:\r\n\r\n> The Sunlight Foundation/Network is offering grants of $1,000 to $5,000 for local groups that have creative ideas for changing the relationship between representatives and the people they represent. In addition, Sunlight also provides consulting support and some networking opportunities for its Grantees.\r\n\r\n> We encourage applications from existing small local nonprofits and websites, from offshoots of national groups, from individuals, and from informal groups of citizens. The grants will go towards funding and implementing original ideas that will create a better, more democratic relationship between government and citizens.\r\n\r\n> Projects will be judged on how closely they fit with Sunlight\u2019s mission of improving the relationship between citizens and their member of Congress through more transparency of information.  Example of people who have received grants in the past is located here: [http://www.sunlightfoundation.com/grants/] (if you scroll to half way there is the mini grant section).  The focus should be on shedding more light on what Congress does and how to improve the communication between citizens and Congress.  As a rule we do not award money for salaries but do for technology upgrades.\r\n\r\n> If you are interested in applying, please fill out the provided web form, [it is available on the Sunlight Foundation grants page](http://www.sunlightfoundation.com/grants/minigrantapplication/).  Please describe your project with a detailed description of how it fits in with Sunlight\u2019s mission and your goals for your project, an itemized budget (including the amount requested from Sunlight) and contact information. \r\n\r\n> If you have any questions feel free to contact <nisha@sunlightfoundation.com>.\r\n\r\nExamples - you are involved in communications/interacting with Congress, and... your laptop died, or you could really use another linux server for your organization, or you can't afford to go to an important conference, etc.\r\n\r\nMaybe we can help!", "date_published": "2007-10-11 15:39:26", "comment_count": 0, "slug": "mini-grants"}}, {"pk": 15, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 1, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Sunlight Labs API changes", "excerpt": "Recent updates to the Sunlight API data.", "content": "<p>Recent updates to the Sunlight API data:</p>\r\n<h4>Deleted Members of Congress</h4>\r\n<ul>\r\n<li>Paul Gillmor (fakeopenID148)</li>\r\n<li>Juanita Millender-McDonald (fakeopenID272)</li>\r\n<li>Charles Norwood (fakeopenID294)</li>\r\n<li>Craig Thomas (fakeopenID528)</li>\r\n</ul>\r\n<h4>Added Members of Congress</h4>\r\n<ul>\r\n<li>John Barraso (fakeopenID600)</li>\r\n<li>Madeleine Bordallo (fakeopenID601)</li>\r\n<li>Paul Broun (fakeopenID602)</li>\r\n<li>Donna Marie Christensen (fakeopenID603)</li>\r\n<li>Eni Fa'aua'a Hunkin (fakeopenID604)</li>\r\n<li>Luis Fortuno (fakeopenID605)</li>\r\n<li>Eleanor Norton (fakeopenID607)</li>\r\n<li>Laura Richardson (fakeopenID608)</li>\r\n</ul>", "date_published": "2007-09-28 20:55:02", "comment_count": 0, "slug": "sunlight-labs-api-changes"}}, {"pk": 14, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 4, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "European Transparency", "excerpt": "Our comrades-in-arms on the Euro side of the Atlantic have their own transparency advocates and meetups.  Here's an interesting one, called [Berlin In August](http://people.oii.ox.ac.uk/escher/2007/08/16/berlin-in-august-summary/).", "content": "Our comrades-in-arms on the Euro side of the Atlantic have their own transparency advocates and meetups.\u00a0 Here's an interesting one, called [Berlin In August](http://people.oii.ox.ac.uk/escher/2007/08/16/berlin-in-august-summary/).\r\n\r\nTwo links in particular caught my eye:\r\n\r\n* [Good Practices for Transparency Sites](http://berlininaugust.politik-digital.de/index.php/Good_Practice)\r\n* [What is Still Missing](http://berlininaugust.politik-digital.de/index.php/What_is_still_missing)\r\n\r\nEnjoy!", "date_published": "2007-09-27 14:52:30", "comment_count": 0, "slug": "european-transparency"}}, {"pk": 13, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 4, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "Digg EarmarkWatch", "excerpt": "If you are a digg user, please consider helping us get the word out about our latest tool:\r\n\r\n[http://digg.com/politics/Citizens_Here_is_your_chance_to_investigate_Congress_Find_The_Pork](http://digg.com/politics/Citizens_Here_is_your_chance_to_investigate_Congress_Find_The_Pork)", "content": "If you are a digg user, please consider helping us get the word out about our latest tool:\r\n\r\n[http://digg.com/politics/Citizens_Here_is_your_chance_to_investigate_Congress_Find_The_Pork](http://digg.com/politics/Citizens_Here_is_your_chance_to_investigate_Congress_Find_The_Pork)", "date_published": "2007-09-26 13:43:51", "comment_count": 0, "slug": "digg-earmarkwatch"}}, {"pk": 12, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 4, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "EarmarkWatch", "excerpt": "Our social app to identify and clean up the data around government earmarks - [EarmarkWatch](\"http://www.earmarkwatch.org/) is live.  Props to Kerry and James for all the hard work they did in getting it ready.", "content": "Our social app to identify and clean up the data around government earmarks - [EarmarkWatch](\"http://www.earmarkwatch.org/) is live.\u00a0 Props to Kerry and James for all the hard work they did in getting it ready.", "date_published": "2007-09-25 13:00:59", "comment_count": 0, "slug": "earmarkwatch"}}, {"pk": 11, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 1, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Mashup the web", "excerpt": "One of our focuses here at Sunlight Labs is to demonstrate how open data enables citizens to be engaged and informed on how Congress works. We do this by creating mashups that make information from a variety of sources easy to manipulate and understand. I recently gave a talk at a <a href=\"http://www.heritage.org/press/carr/bootcamp.cfm\">CARR Boot Camp</a> on using the web to work with data. While not directly related to Congressional transparency, the following example from the talk is a good demonstration of how an end user can work with open data without the assistance of software developers.", "content": "<p>One of our focuses here at Sunlight Labs is to demonstrate how open data enables citizens to be engaged and informed on how Congress works. We do this by creating mashups that make information from a variety of sources easy to manipulate and understand. I recently gave a talk at a <a href=\"http://www.heritage.org/press/carr/bootcamp.cfm\">CARR Boot Camp</a> on using the web to work with data. While not directly related to Congressional transparency, the following example from the talk is a good demonstration of how an end user can work with open data without the assistance of software developers.</p>\r\n<p>We'll start with a site I highly value, <a href=\"http://dcfoodies.com/\">DC Foodies</a>. The site has great reviews of restaurants in the DC area and other related blog posts. Most of the review entries contain the address of the restaurant being reviewed. By itself this is of little value. We can read the post and see the address, but that's about it. Fortunately the site has an RSS feed.</p>\r\n<p>Let's take the RSS feed and run it through Yahoo's fantastic Pipes service. Pipes lets you take RSS and other feeds and execute various operations on the data. The site provides a dead simple visual editor that is used to apply the operatons and filters to the data. For example:</p>\r\n<p><img id=\"image20\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2007/09/picture-1.png\" alt=\"DCFoodies.com in Yahoo Pipes\" /></p>\r\n<p>The pipe in the image above takes the DC Foodies RSS feed, geocodes any location data found in the posts, and filters out entries that do not contain geographic information. Pipes adds custom elements to the RSS feed that contain the latitude and longitude of the restaurants based on the addresses in the reviews. How cool is that? It gets even better.</p>\r\n<p>Among the various formats in which you can output your modified feed, <a href=\"http://code.google.com/apis/kml/documentation/\">KML</a> is specifically suited to syndication of geographic data. We can take the <a href=\"http://pipes.yahoo.com/pipes/pipe.run?_id=GnHiiblc3BGoaGgXCR2yXQ&_render=kml\">URL of the KML output</a> and paste it into the search box of <a href=\"http://maps.google.com\">Google Maps</a>. Click the Search Maps button and Google will <a href=\"http://maps.google.com/maps?f=q&hl=en&geocode=&q=http:%2F%2Fpipes.yahoo.com%2Fpipes%2Fpipe.run%3F_id%3DGnHiiblc3BGoaGgXCR2yXQ%26_render%3Dkml&ie=UTF8&ll=38.996959,-77.02748&spn=0.032352,0.056047&z=14&om=1\">plot the locations</a> from the KML feed on the map. Click on one of the map markers and you will see a summary of the original blog post.</p>\r\n<p>New tools and technologies are allowing end users to mash up content without needing the assitance of software developers. Building open platforms that work on open formats is key to allowing end user manipulation of data across disparate resources.</p>", "date_published": "2007-09-20 19:54:09", "comment_count": 0, "slug": "mashup-the-web"}}, {"pk": 10, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 1, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Refresh DC: Web Widgets: What, Why, and How", "excerpt": "Refresh is a community of web designers, developers, and other new media professionals working together to refresh the creative, technical, and professional aspects of their trades in greater Washington, DC.", "content": "<p style=\"background-color: #324B83; padding: 15px;\"><img src=\"http://refresh-dc.org/images/logo.png\" alt=\"Refresh DC logo\" /></p>\r\n<p>It's mid-month which means it's time for another meeting of <a href=\"http://refresh-dc.org\">Refresh DC</a>. <a href=\"http://www.willmeyer.com/\">Will Meyer</a> from <a href=\"http://www.clearspring.com/\">Clearspring</a> will lead an interactive discussion on web widgets in general, and best practices specifically, with some examples as appropriate.</p>\r\n<p>Refresh is a community of web designers, developers, and other new media professionals working together to refresh the creative, technical, and professional aspects of their trades in greater Washington, DC.</p>\r\n<p>These events are a great place to meet incredibly creative and highly intelligent web people in the DC area. Refresh has brought out an vigorous community that few people knew existed in this town known more for government contracting and politics than web technologies. It's Silicon Valley of the east...or at least getting there.</p>", "date_published": "2007-09-20 18:43:09", "comment_count": 0, "slug": "refresh-dc-web-widgets-what-why-and-how"}}, {"pk": 9, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 4, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "Howdy", "excerpt": "I'm John Brothers, and I'm the new CTO of the lab.", "content": "Thanks for reading the SunlightLabs Blog!\u00a0 I'm John Brothers, and I'm the new CTO of the lab.\r\n\r\n We're going to be doing a lot more with this blog over the coming months, so look for articles on events, releases of our open source projects, tutorials, advice, commentary and a whole lot more!", "date_published": "2007-09-20 15:07:41", "comment_count": 0, "slug": "howdy"}}, {"pk": 8, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 1, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "Using RSS for House Committee Schedule Feeds", "excerpt": "Do you want congressional committee schedule information in a standard syndication format? We sure would and this is what we propose.", "content": "Based on John Wonderlich's work on the [Open House Project](http://theopenhouseproject.com) and Joshua Ruihley's work on [Open Hearings](http://openhearings.org), we here in the [Labs](http://sunlightlabs.com) decided to write a 'dream feed' as an example of an RSS format congress could use to syndicate their committee meeting schedule.\r\n\r\n[http://sunlightlabs.com/projects/openhouse/schedule-v01.xml](http://sunlightlabs.com/projects/openhouse/schedule-v01.xml \"link to feed\")\r\n\r\n    <?xml version=\"1.0\"?>\r\n    <rss version=\"2.0\"\r\n        xmlns:xcal=\"urn:ietf:params:xml:ns:xcal\"\r\n        xmlns:enc=\"http://www.solitude.dk/syndication/enclosures/\">\r\n      <channel>\r\n        <title>House Committee Schedule</title>\r\n        <link>http://house.gov/schedule/</link>\r\n        <description>Master schedule for house committees.</description>\r\n        <language>en-us</language>\r\n        <lastBuildDate>Fri, 06 Jul 2007 09:39:21 EDT</lastBuildDate>\r\n        <docs>http://www.rssboard.org/rss-2-0-9</docs>\r\n        <generator>House Schedule Generator 1.0</generator>\r\n        <webMaster>schedule@house.gov</webMaster>\r\n        <ttl>3600</ttl>\r\n        <item>\r\n          <title>Subcommittee on General Farm Commodities and Risk Management</title>\r\n          <link>http://agriculture.house.gov/hearings/schedule.html</link>\r\n          <description>To review trading of energy-based derivatives.</description>\r\n          <pubDate>Fri, 06 Jul 2007 09:39:21 EDT</pubDate>\r\n          <guid>http://agriculture.house.gov/hearings#200707121000</guid>\r\n          <category domain=\"http://house.gov/schedule/type\">hearing</category>\r\n          <category domain=\"http://house.gov/schedule/visibility\">public</category>\r\n          <xcal:organizer cn=\"House Committee on Agriculture\">http://agriculture.house.gov</xcal:organizer>\r\n          <xcal:location>1300 Longworth House Office Building, Washington, DC</xcal:location>\r\n          <xcal:dtstart>2007-07-12T10:00:00Z</xcal:dtstart>\r\n          <xcal:dtend>2007-07-12T11:00:00Z</xcal:dtend>\r\n          <xcal:attendee role=\"CHAIRMAN\" cn=\"Rep. John Doe\">mailto:john.doe@house.gov</xcal:attendee>\r\n          <xcal:attendee role=\"OPT-PARTICIPANT\" cn=\"Rep. Joe Smith\">mailto:joe.smith@house.gov</xcal:attendee>\r\n          <xcal:attendee role=\"X-WITNESS\" cn=\"Dan Johnson\">mailto:djohnson@company.com</xcal:attendee>\r\n          <enc:enclosure title=\"Live Media\">\r\n            <enc:link type=\"video/quicktime\" length=\"11534336\"\r\n              url=\"http://agriculture.house.gov/hearings/200707121000.mov\" />\r\n            <enc:link type=\"audio/mpeg\" length=\"11534336\"\r\n              url=\"http://agriculture.house.gov/hearings/200707121000.mp3\" />\r\n          </enc:enclosure>\r\n          <enc:enclosure title=\"Archived Media\">\r\n            <enc:link type=\"video/quicktime\" length=\"11534336\"\r\n              url=\"http://agriculture.house.gov/hearings/archive/200707121000.mov\" />\r\n            <enc:link type=\"audio/mpeg\" length=\"11534336\"\r\n              url=\"http://agriculture.house.gov/hearings/archive/200707121000.mp3\" />\r\n          </enc:enclosure>\r\n          <enc:enclosure title=\"Agenda\">\r\n            <enc:link type=\"application/pdf\" length=\"11534\"\r\n              url=\"http://agriculture.house.gov/hearings/200707121000_agenda.pdf\" />\r\n          </enc:enclosure>\r\n        </item>\r\n      </channel>\r\n    </rss>\r\n\r\n\r\nThe feed was written to the [RSS 2.0.9 spec](http://www.rssboard.org/rss-2-0-9 \"specification\"). Since RSS is meant for syndication, two additional modules, elements belonging to a namespace, were used to add the information that was needed. The [xCal XML specification](http://tools.ietf.org/html/draft-royer-calsch-xcal-03 \"iCalendar in XML\") was used to add event data to the feed. xCal is an XML representation of the widely used iCalendar format. The format is fairly straighforward except for the organizer and attendee elements. The specification requires the value of these elements to be a URI rather than a string naming the individual. The URI can be any identifier, but is typically the email address of the individual. To include the name of the individual, a cn attribute must be added to the tag. The attendee element has a role attribute that describes the role in which the attendee will be serving. We've added a custom X-WITNESS role to identify witnesses that are testifying before a committee.\r\n\r\nAdditionally, we have a need to add multiple enclosures to the feed to represent different types of media: audio and video of events and documents related to the event. Since the RSS 2.0.9 spec is a bit unclear as to whether multiple enclosures are supported we decided to use Andreas Pedersen's [multiple enclosure extension](http://www.solitude.dk/archives/20050208-0045/ \"multiple enclosures in RSS\"). It's fairly straightforward and allows us to link to multiple types of related documents and media.\r\n\r\nYour feedback is welcome and desired! We'd like to make this sample feed as good as it can be to serve as an example of the Right Way to syndicate congressional schedules so any fixes or additions are quite appreciated. We'll post all changes to the blog. ", "date_published": "2007-07-10 20:32:01", "comment_count": 2, "slug": "using-rss-for-house-committee-schedule-feeds"}}, {"pk": 7, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 3, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "CiviCRM integrating SunlightLabs API", "excerpt": "", "content": "David  announced <a href=\"http://civicrm.org/node/205\">CiviCRM integration with our SunlightLabs API</a> today. This is more exciting stuff. In an email to us, David noted the business implications:\r\n<blockquote>\r\n<ol>\r\n\t<li><em>People could automatically email all the people in the database within a particular district.</em></li>\r\n\t<li><em>You can have a tab in a contact record automatically populated with political information.</em></li>\r\n</ol>\r\n</blockquote>\r\nIn his blog post, David complimented the documentation of <a href=\"http://sunlightlabs.com/api\">the SunlightLabs API</a> (to which we have to thank <a href=\"http://taggel.com/\">Labs alumni Dr. Carl Anderson</a> who also created the first version of the API modeling it and the documentation from Flickr's excellent API). He also politely pointed out our need to blog a bit more (this is a start!) and suggested some more improvements.\r\n\r\nThe <a href=\"http://sunlighfoundation.com\">SunlightFoundation.com website</a> runs on Drupal and CiviCRM, so we ourselves will be a beneficiary of this integration. W0ot!", "date_published": "2007-07-05 16:44:13", "comment_count": 0, "slug": "civicrm-integrating-sunlightlabs-api"}}, {"pk": 6, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 2, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Ajaj: Ajax2.0", "excerpt": "Is Ajaj the new Ajax?", "content": "Is Ajaj the new Ajax?\r\n\r\n<em><strong>Ajax: Asynchronous Javascript and XML</strong></em>. I wonder though how many people actually use XML in web2.0 applications. After all, an XMLHttpRequest can return both responseXML and responseText. My bet is that text, including <strong>*J*</strong>SON, is used <em>far</em> more frequently than XML.\r\n\r\nIf one can do all the data processing server-side, send it back as HTML text to the browser, and the browser needs only to set some innerHTML, why bother with XML? Well, you respond, \"what if I have structured data and different pieces of data need to go to different parts of the page: a title here, an image URL over there, some content here. Then, innerHTML won't work. You need to send over data plus descriptive metadata so that the browser code knows what it is.\" While, true, I am still not convinced that XML is used that much. First, one can send different chunks of data in simple responseText. For instance, one might send back a comma- or pipe-delimited string. The javascript does a split on that character and so long as the correct ordering of those data pieces is preserved, the data can be separated and sent where it needs to go in the page. For more complex data, or for a more robust data transfer one can use JSON in the responseText.\r\n\r\nI've been developing <a href=\"http://sunlightlabs.com/api/\">an API for the labs</a> recently and each method can produce JSON and XML. It was really striking to me just how easy JSON is to consume in the browser. One gets all the benefits of metadata markup, but it is both very compact and extraordinarily simple to  reference those data items in Javascript. For instance, I send\r\n<div style=\"padding: 10px; background: #ffcc66 none repeat scroll 0% 50%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial\">{ name : \"Joe Doe\", title : \"VP, Engineering\", phone : \"13-456-7890\"}</div>\r\n<em>[<strong>Update</strong> : as the comment below points out, it should have read: { \"name\" : \"Joe Doe\", \"title\" : \"VP, Engineering\", \"phone\" : \"13-456-7890\"}] </em>\r\n\r\nin my responseText. In Javascript I need only do the following:\r\n<div style=\"padding: 10px; background: #ffcc66 none repeat scroll 0% 50%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial\">var obj = eval( '(' + XMHttpRequest.responseText + ')' );</div>\r\nand now I reference those individual data items as obj.name, obj.title and obj.phone. Could it be any simpler?\r\n\r\nWell, I thought that until I really started playing around with JSON and found that it has frustrating limits, ones that JSON.org does not highlight <strike>(dare I say lie?)</strike> in their documentation.\r\n\r\nTake the following code:\r\n<div style=\"padding: 10px; background: #ffcc66 none repeat scroll 0% 50%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial\">var j1 = \"{ data: \\\"ab\\\"}\";\r\nvar obj1 = eval( \"(\" + j1 + \")\" );\r\nalert(obj1.data);</div>\r\nwhat is alerted? \"ab\". Correct.\r\n\r\nHow about this:\r\n<div style=\"padding: 10px; background: #ffcc66 none repeat scroll 0% 50%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial\">var j1 = \"{ data: \\\"a\\nb\\\"}\";\r\nvar obj1 = eval( \"(\" + j1 + \")\" );\r\nalert(obj1.data);</div>\r\nWhat is alerted? \"a\\nb\". Correct? Wrong. It bails.\r\n\r\n<strong><em>JSON, or rather the Javascript parser, cannot handle \\n.\r\n</em></strong>\r\nThat's strange I swear that \\n is shown on JSON's wonderfully easy to understand documentation. Let's take a look:\r\n\r\n<img width=\"468\" height=\"357\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2007/03/string.gif\" />\r\n\r\nYes, that's clear. JSON.org says that '\\n' is allowed in JSON. <a href=\"http://framework.zend.com/issues/browse/ZF-504\">I was not the only one to find this \\n limitation</a>:\r\n<div style=\"padding: 10px; background: #ffcc66 none repeat scroll 0% 50%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial\">\"As for the second, regarding newline characters, http://www.json.org/ says they are valid JSON notation\"</div>\r\n<div style=\"padding: 10px; background: #ffcc66 none repeat scroll 0% 50%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial\">Sort of, but it also states that \"It is based on a subset of the JavaScript Programming Language, Standard ECMA-262 3rd Edition - December 1999\", which clearly forbids them (par. 7.3)</div>\r\n<strike>So it is a lie then. JSON does not support \\n.</strike>\r\n\r\n<strike>Not only is this annoying but, in my mind, it does limit JSON usage.  I cannot  use it to transport arbitrary chunks of text.</strike>\r\n\r\n<hr /><strong>UPDATE</strong>: so at here Ajaxworld in NYC, I just asked Douglas Crockford, inventor of JSON, about this. So, it turns out that \\n needs to be escaped properly. Thus, the newline character should have been \\\\n\r\nThat is,\r\n<div style=\"padding: 10px; background: #ffcc66 none repeat scroll 0% 50%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial\">var j1 = \"{ data: \\\"a\\\\nb\\\"}\";\r\nvar obj1 = eval( \"(\" + j1 + \")\" );\r\nalert(obj1.data);</div>\r\ndoes work. JSON can handle arbitrary chunks of text so long as unicode characters are properly escaped. Whoops sorry about that. Off to eat some humble pie...\r\n<hr />", "date_published": "2007-03-11 16:26:33", "comment_count": 1, "slug": "ajaj-ajax20"}}, {"pk": 5, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 2, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Instant APIs: no code necessary, just add mouseclicks", "excerpt": "APIs and mashups are the bread and butter of the Sunlight Labs. In fact, the official name of the the Labs is actually The Sunlight <em>Mashup</em> Labs, so it is perhaps not surprising that we headed off to the <a href=\"http://wiki.mashupcamp.com/index.php/WhosComingToMashupCamp3\">MashupCamp 3</a> at MIT, Cambridge a couple of weeks ago to geek out and to see the latest happenings, trends and players in the world of mashups and technology.", "content": "<img id=\"image10\" alt=\"dapper logo\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2007/02/dappit-flower-small-multicolor.thumbnail.gif\" />\r\n\r\n<img width=\"125\" height=\"23\" id=\"image11\" alt=\"openkapow\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2007/02/kapowlogo.thumbnail.jpg\" />\r\n\r\nAPIs and mashups are the bread and butter of the Sunlight Labs. In fact, the official name of the the Labs is actually The Sunlight <em>Mashup</em> Labs, so it is perhaps not surprising that we headed off to the <a href=\"http://wiki.mashupcamp.com/index.php/WhosComingToMashupCamp3\">MashupCamp 3</a> at MIT, Cambridge a couple of weeks ago to geek out and to see the latest happenings, trends and players in the world of mashups and technology.\r\n\r\nThere was a lot of cool stuff demoed, debated and hacked at the camp but one theme that really stood out for me was tools that made scraping easier and more effective than ever before. In short, tools to create instant APIs, without programming \u2014 yes, really.\r\n\r\nTo be more concrete: suppose there is a website that you visit every day and you read the headlines. However, you prefer to read these stories in your favorite RSS reader but this website does not provide an RSS feed. You can use these tools to create such a feed yourself.\r\n\r\nOr, suppose you want to buy a second-hand iBook from Craigslist and are not in a rush; you would rather wait for the right deal. Well, you could manually check the listings everyday but you would rather be alerted if an iBook comes up for sale within a certain distance from home and under a certain price. To do this programmatically, you need to grab the listings for computer ads and then you can parse them to see in any match your criteria. You can use these tools to create an API for Craigslist computer ads, under a search query of \"iBook\", in a matter of minutes.\r\n\r\nAnything that is presented on a webpage, certainly a static webpage, is something that could be scraped. The problem is that scraping is not easy. Quality of HTML structure varies considerably. While some sites always use valid W3C XHTML, use CSS with lots of descriptive class names and sensible divs that divide different sections of the page, many do not. Do a view source of a random <a href=\"http://profile.myspace.com/index.cfm?fuseaction=user.viewprofile&friendid=154222679&MyToken=a044c8f0-98ad-403d-bbb9-08528fe02798\">myspace page</a> and you will see what I mean: it is a huge mess.\r\n\r\nSo, to scrape a page when it does not have a clear or consistent structure, or to scrape a page that requires a login (say, your AOL buddy list) is not at all easy. It can be done but it takes time and, moreover, because one is scraping who knows how long that scraper will last. The content provider makes a change in page structure and your scraper has to be reworked. Finally, one has to be a coder: you need to use tools such as curl or php to get the page contents, you may need tools such as html parsers to navigate the content, and you may need competency in regex to pull out precisely what you need from the page. Finally, you may actually need a server to create a page to display the final, desired content; and not everyone has such resources. (We will see below how one  can dispense with this latter requirement.) Any tools that provide instant APIs from arbitrary webpages, especially for non-progammers, has to be good news.\r\n\r\nTwo organizations in this space who demoed at Mashup Camp are <a href=\"http://www.dappit.com/\">Dapper</a> and <a href=\"http://openkapow.com/\">OpenKapow</a>. Now that I've had a chance to play around with these technologies, I want to take this opportunity here to review them, giving an unbiased critique.\r\n\r\n<strong>Dapper</strong>\r\n\r\nDapper is a US startup based in Israel founded by Eran Shir and Jon Aizen and is currently in open beta. While all services are free at the moment, large projects or commercial uses of Dapper may be charged in the future. However, if Dapper works as well as it is claimed then I can see many organizations making good use of Dapper and saving in development costs even in a fee-based structure. Why do I say this? What precisely is Dapper?\r\n\r\nDapper is a web-based tool and service for scraping, creating APIs from potentially any webpage and then generating output of that API in a variety of formats including HTML, XML, RSS and even Google Maps.\r\n\r\nOne starts by entering a URL and the webpage is displayed within Dapper. One can click through different pages and add each page to a \"basket\". Thus, one could add say pages 1, 2, 3, 4 of a blog. Dapper then analyzes these pages to work out the structure, say what is a static header and footer and what is dynamic content. One then gets to \"play\". That is, clicking on a story title (should) highlights all other story titles in the page. As such, one is confident that Dapper will grab the correct content from the page when you specify precisely what you want your API, or \"Dapp\", should do.\r\n\r\nI have to say that I had mixed success with this. While Dapper correctly identified the story titles for techcrunch.com and http://www.followthemoney.org/Newsroom/index.phtml, it did not do so on sunlightfoundation.com and it could not work out my intention of grabbing rows or columns on http://opensecrets.org/orgs/list.asp?order=A (but it could get the org titles). However, this is still a beta and Dapper certainly does work: there are hundreds of user-created Dapps that one can browse and use.\r\n\r\nAfter playing, one can then progress to creating the API proper: one clicks on a desired element, gives it a name and one can then define a group, e.g. specify that this story title, this author name and this number of diggs are all related as one unit. After that, one can preview the API, i.e. check what is pulled out by the API. If all is well, one is done. Then the real fun begins...\r\n\r\nDapper provides the API in a impressive variety of formats: XML, HTML, RSS, Alerts, iCalendar (transforms the output of a Dapp into an iCalendar which can be used in Google Calendar, Sunbird, iCal, and other programs), Google maps (places locations directly onto a map), Google gadgets, Netvibes, image loop, email, link to another Dapp, CSV, JSON, YAML, XSL, and fork it as another Dapp. This APIs are published and hosted at Dapper (hence one doesn't need their own server to provide an API). Dapper also allows one to define parameters for the API such as a {query} or {page}.\r\n\r\nSome of the resultant URLs are mess though. So, Dapp allows one to define a service that provides a nice clean URL. That is, instead of say http://www.dappit.com/RunDapp?dappName=sunlightlabs&v=1&thisparam=y&thatparam=z... one can instead provide users with say http://www.dappit.com/services/sunlightlabs.\r\n\r\nAnother nice feature is their AggregatorAid that allows one to pool Dapps into one single service. Thus, if one has a suite of different individual search Dapps (Google, digg, reddit, etc) one can provide a single query that will aggregate the results as a single API. The only restriction is that each that individual Dapp must have the same query parameter name (I hope that this is relaxed in later versions).\r\n\r\nFinally, Dapper provides secure, authenticated login into sites. Thus, if you want to scrape your AOL buddies or comments on your Facebook page, Dapper allows you to create an API without exposing your username or password.\r\n\r\nOverall, Dapper is a clean, slick instant API generator that has some very nice features: it is web-based, has a clean interface, and provides API output in the majority of formats that anyone would want. It is beta, it is not perfect, but then it is free. One really can get a basic API scraped from a page, published, and up and running in literally 5 minutes. This is why I say that I can see a valid business model here. Organizations, especially non-profits such as ourselves, can try Dapper first to create a given API, and only if it doesn't work then create the API from scratch or outsource which will certainly be more time consuming and expensive.\r\n\r\n<strong>OpenKapow</strong>\r\n\r\nOpen Kapow, also in beta, founded by  \t      Stefan Andreasen in Denmark, is part of the larger Kapow Techologies which appears to have a large range of corporate clients and a number offices on both sides of \"the pond\".\r\n\r\nOpenKapow has the same goal of instant APIs as Dapper but takes a different approach. For a start, while Dapper is web-based, OpenKapow requires a large \"RoboMaker\" download (100+ Mb) for windows or linux. [So, as a mac guy that cuts me out. If one takes a look around at any hacker or mashup conference you will find a sea of macs, and these are the guys who are most likely to do such mashups. While macs are UNIX underneath the linux version does not install on macs.]\r\nInstalling the RoboMaker on windows, one is presented with a sophisticated Java-based tool for mashups. As in Dapper, a webpage is loaded and displayed within the tool and one can select various elements of the page. RoboMaker has a nice DOM JTreeView that one can explore the structure of the page and one can click various elements and they will be highlighted and at the same time, the selected item is also shown in an HTML viewer. Together, the three panels provide a better understanding of the page structure than Dapper.\r\nOne the right are a series of controls for selecting tags based on name, type, conditions etc. There really are a very large number of features and controls that (I would image) provide a very granular approach to scraping. Therein, however, lies the problem.\r\n\r\nThere were so many controls, so many right click context menus, that it was not at all obvious for the newbie where to begin. I failed to select all the h1 tags of the page despite trying various combinations of select *.h1 etc. I saw OpenKapow in action at mashupcamp and I did see Stefan demo this creating an API for Google search. It looked very easy...if you know what you are doing. I really do believe that OpenKapow is a more sophisticated tool that probably can deal with edge cases that Dapper cannot. It is a tool for hackers/programmers who must invest some time into understanding the tool (and getting the thing downloaded and installed) but it is certainly not a quick and easy hacker tool. While both will generate APIs without true coding, I feel that Dapper is an quick-and-dirty, intuitive 90% solution while OpenKapow is a 90%+ tool for more serious projects.\r\n\r\nPerhaps I have a short attention span but I have to admit that I gave up trying to get something basic working on OpenKapow. Besides, even if i did get it working, the output formats for OpenKapow are far more limited: (X)HTML, XML, REST, and RSS.\r\n\r\n<strong>Conclusions </strong>\r\n\r\nDapper especially allows one to try a quick hack. If that doesn't work then one could resort to writing it oneself or using a tool such as OpenKapow. OpenKapow looks as though it would be worth the effort to invest in learning all of the features if one was going to be doing lots of scraping and mashups, or in a corporate environment where one needed a very granular mashup that needed to work all the time. I think that <em>both</em> of these represent an exciting new era for mashups. They both represent programming-free tools for instant, shareable APIs. They both involve a reasonable GUI to select elements rather than needing to poke around the underlying HTML itself. As such, the bar for mashing up is significantly lowered.", "date_published": "2007-02-01 19:37:36", "comment_count": 0, "slug": "instant-apis-no-code-necessary-just-add-mouseclicks"}}, {"pk": 4, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 2, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Why offline is the new online", "excerpt": "The history of computing has some interesting trends. First it was all about mainframes. To get anything computed you had to logon to some remote machine so large that if filled a room. Then came along the desktop; desktop apps were (and arguably still are) it. In the 1990's, however, things got pushed onto the web, web <em>sites</em> were the thing, and now with the rise of Ajax, web <em>apps</em> are becoming the new it.", "content": "<img width=\"113\" height=\"79\" alt=\"apollo logo\" title=\"apollo logo\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2007/01/apollologo.jpg\" />The history of computing has some interesting trends. First it was all about mainframes. To get anything computed you had to logon to some remote machine so large that if filled a room. Then came along the desktop; desktop apps were (and arguably still are) it. In the 1990's, however, things got pushed onto the web, web <em>sites</em> were the thing, and now with the rise of Ajax, web <em>apps</em> are becoming the new it. Applications such as <a href=\"http://www.google.com/support/writely/\">writely</a> (now \"google docs\") or <a href=\"http://www.google.com/googlespreadsheets/tour1.html\">google spreadsheets</a> that have the look, feel and responsiveness of traditional desktop applications.  I want to predict the next segment of this zig-zag computing trend: offline web apps on your desktop.\r\n\r\nIn the last couple of months there have been some very interesting developments. Adobe have staked their claim in this area with the announcement of <a href=\"http://labs.adobe.com/wiki/index.php/Apollo\">Apollo</a>, Mozilla is already in this arena with <a href=\"http://developer.mozilla.org/en/docs/XULRunner\">xulrunner</a>, and recently <a href=\"http://www.sitepen.com/blog/2007/01/02/the-dojo-offline-toolkit/\">dojo</a> announced a forthcoming offline library. Before we get into the specifics though, I want to dig a little deeper into some background and discuss the influence of widgets.\r\n\r\nWidgets are really hot now, small focussed applications that serve one or two useful purposes: show the local weather, a simple RSS reader, show what is playing on iTunes etc. The problem is that they are tied to particular browser\u2014<a href=\"http://widgets.opera.com/\">opera widgets</a> only work for the opera browser\u2014or tied to a particular operating system\u2014apple's <a href=\"http://www.apple.com/macosx/features/dashboard/\">dashboard widgets</a> only work on a mac\u2014or they must be served through a third party host (e.g. <a href=\"http://www.widgetbox.com/\">widgetbox</a>). This sucks for all involved: developers always want to write once, run anywhere but instead they have to write and maintain slightly different versions for lots of different platforms. Users don't want to search through some complicated table to work out which one they need to download. What we all want is a universal \"click to download and install\" widget, that does what it says, and that does what it says in the same way on all platforms with the same code. We want a simple app that will float on the window, do its thing.\r\n\r\nDesktop app are tied to native libraries for good reason: performance. Fast, efficient Cocoa apps for mac, .net for for windows etc. While Java claims to be universal, it is very slow to launch, there are still UI inconsistencies across platforms, and it can never reach the same degree of performance as native apps for compute-intense apps such as rendering and video CODECs. Widgets are different however. Their functions are typically so simple that it makes no difference. To grab and display the latest blog post entry's title from your favorite blog is hardly going to stress the CPU. So, they can be written in universal language such as HTML+CSS+JS.\r\nThe stupid thing is that widgets are all written in the same language: HTML+CSS+JS. Opera widgets and Apple's dashboard widgets are essentially the same under the hood but they just cannot run on each other platforms. Within 6 months we are, however, going to see more universal widgets and desktop apps written in web technology.\r\nAdobe is starting to promote its forthcoming Apollo platform. Apollo will provide libraries for offline storage, will have the ability to read and write to the desktop's file system (which is difficult to achieve in true web apps because of the built in security model), and will have libraries for flash, HTML and PDFs. So to make sure we are clear here: you will be able to write a web app in HTML and have it run universally on all platforms: windows, mac and linux, and further, the HTML can be rendered through the flash viewer so that it is virtually guaranteed to look and operate exactly the same on all of those platforms. As a developer, one can write once, make it generally available for download as an .air file, have it work on all platforms. Not only that but Apollo apps can continuously check for connectivity: you shut down your computer because you are getting onto a plane, no problem. After take off, you open it up and continue as before. Moreover, all of these offline activities or transactions are stacked up locally and processed online as soon as connectivity is restored. You can continue to work offline and it will seamlessly sync to online when it can. Oh, and did I say all of this will be free? [They will make their money through selling more Adobe Flex licenses to developers.] Apollo will be available later this year but the main feature they need to fix is distribution. Currently, one has to download an Apollo runtime and an apollo installer. This is not likely to engender uptake by the mainstream Joe Doe end user. However, unsurprisingly, they do have plans to distribute through the flash player so the back end installation will be seamless.\r\n\r\nMozilla already in this space through xulrunner. So, \"xulrunner --install -app gmail\" is all that is needed to create a desktop app for gmail. This will provide a local executable that will launch a window to gmail. This can be running independently of any browser. This is from Mozilla so of courser there are <a href=\"http://www.mozilla.com/en-US/firefox/\">Firefox</a> libraries running behind the scenes but the point is that I can have a persistent desktop application running up in say my top right corner of my screen, always there, always keeping me informed of whatever I want to keep informed about, without me having to have firefox or any other browser running and have that window or tab showing. This too will allow developers to write desktop applications in HTML+CSS+JS that run offline. Given the number of developers who know these technologies, this could be a huge boon for desktop app innovation. Again, this allows a write once run anywhere [but with less guarantees about consistent look and feel] and has local SQLlite database for local storage.\r\nDojo too is likely to be another important player. They are working on an <a href=\"http://www.sitepen.com/blog/2007/01/02/the-dojo-offline-toolkit/\">offline toolkit</a>, a \"small, cross-platform, generic       download that enables web applications to work offline.\" It will be cross platform, cross-browser, connectivity detecting, secure, with local storage. Sound familiar?\r\n\r\nLet me stress that one is not going to be able to take an existing complex, server-side PHP+MySQL web application, type a single command and have it instantly available as a desktop application. (Besides, would really trust your code, even if encrypted, out there on everyone's desktop?) However, this is an exciting area for web-innovation. Watch this space in the next six months. More and more of the web will be seeping onto your desktop. This is why I say offline is the new online.", "date_published": "2007-01-21 15:57:21", "comment_count": 0, "slug": "why-offline-is-the-new-online"}}, {"pk": 3, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 2, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "The convergence of Ajax and Flash", "excerpt": "While reading a couple of posts in Ajaxian about  <a href=\"http://ajaxian.com/archives/jsflickrslideshow-sliding-through-flickr\">hacking the canvas tag with a flickr stream</a> and a <a href=\"http://ajaxian.com/archives/smooth-gallery-10-released\">mootools-based image gallery stream</a> it struck me just how sophisticated some Ajax-based UIs have become, so much so that it has become harder to tell what is and is not flash.", "content": "<img width=\"184\" height=\"134\" align=\"top\" alt=\"smooth gallery\" title=\"smooth gallery\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2007/01/smoothgallery10.jpg\" />\r\n\r\nWhile reading a couple of posts in Ajaxian about  <a href=\"http://ajaxian.com/archives/jsflickrslideshow-sliding-through-flickr\">hacking the canvas tag with a flickr stream</a> and a <a href=\"http://ajaxian.com/archives/smooth-gallery-10-released\">mootools-based image gallery stream</a> it struck me just how sophisticated some Ajax-based UIs have become, so much so that it has become harder to tell what is and is not flash.\r\n\r\nThat is, not so long ago, flash was the only viable option for a variety of interactivity including drag and drop and sophisticated animation. However, over the last year a slew of Ajax libraries have been released such as <a href=\"http://dojotoolkit.org/\">dojo</a>, <a href=\"http://developer.yahoo.com/yui/\">Yahoo! UI</a>, <a href=\"http://en.wikipedia.org/wiki/Canvas_(HTML_element)\">canvas tag</a> has spread, especially with the release of <a href=\"http://excanvas.sourceforge.net/\">excanvas</a> that hacks this SVG type functionality for internet explorer. Now one can draw and animate curves in DHTML.\r\n\r\nNow don't get me wrong, flash is a great innovative, product (sometimes abused where the whole website is flash and you cannot link to anything) has a very thin client, and most importantly looks and feels the same on different browsers (without all those frustrating conditional CSS IE hacks). For this reason it will hold a strong position for a while to come yet, but I do feel that we will continue to see increasing innovation in Ajax libraries and a convergence of DHTML websites to have a flash-like look and feel.", "date_published": "2007-01-08 17:54:12", "comment_count": 0, "slug": "the-convergence-of-ajax-and-flash"}}, {"pk": 1, "model": "blogdor.post", "fields": {"status": "draft", "last_updated": "2008-12-04 17:56:22", "author": 2, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Cool. Sunlight Now in Second Life (Thanks to an API...)", "excerpt": "", "content": "<!-- start content_top block if on homepage -->\r\n<!-- end content_top -->\r\n<!-- start main content -->\r\n<!-- begin content -->\r\n<div class=\"node\">\r\n<div class=\"blogcontent\"><object width=\"425\" height=\"350\">\r\n<param value=\"http://www.youtube.com/v/8FxlGNA5Q-I\" name=\"movie\" />\r\n<param value=\"transparent\" name=\"wmode\" /></object>Here is a cool development. Steve Nelson is displaying information on members of Congress inside Second Life (<a href=\"http://slurl.com/secondlife/Capitol%20Hill%201/173/17/30/\">SLurl location</a>) using the Sunlight Labs's still-in-beta API (Application Programming Interface).\r\nSteve is entering his \"U.S. House of Representatives Info Center\" in <a href=\"http://sunlightfoundation.com/mashup\">Sunlight's first Web 2.0 Mash-Up contest</a> (deadline April 15). I'm blogging it  because it so wonderfully illustrates the phrase Sun Microsystems, Inc.'s CEO Jonathan Schwartz used to explain why Sun, a publically traded Fortune 500 company, decided to embraced open source: \"Openness is an accelerant.\"  <img src=\"/files/slch-new-med.jpg\" />  <em>Openness is an accelerant.</em> None of us in the Labs know the first thing about programming Second Life. Yet, because we have an open web service API for certain data on Congressional Representatives at <a href=\"http://sunlightlabs.com/api/\">SunlightlLabs.com/API</a>, someone else could.  As the screen shot shows, Second Life members approach Steve's Info Center and then type their zip code into chat. Steve  programmed his Info Center to go over the web to SunlightLabs's API and fetch back a photo of the appropriate representative and links to related pages on different accountability web sites.\r\nOur openness accelerated Steve's bringing data we compiled to new users in a new context with new tools. Regardless of your opinion of 3D worlds, <em>how cool is that?</em>  To get a bit more geeky, two types of openness enable Steve's Info Center Mash-Up: 1) the openness of web service APIs and 2) the openness Second Life provides to its \"citizens\" to create virtual objects.\r\nSunlight Labs rockstar Dr. Carl Anderson created a web service API to a database that cross references zip codes with congressional districts with the IDs different web sites use to use publish dynamic content on members of Congress from their particular databases. Having an API means other developers -- like Steve Nelson -- can access the cross-referencing we've already done directly from their computer programs thereby accelerating their ability to make new applications with this data. Our API open goodness is then paired with the openness of Second Life's platform that allows members to create and populate its virtual world with structures and objects much the same way AOL's members created and populated AOL with community bulletin boards and chat rooms.\r\nSteve created his interactive congressional display within SL's Capitol Hill, a place for political information he co-created with others. SL's openness accelerates its development by its members.  (To experience a different but equally cool Web 2.0 mash-up of congressional data in your standard browser, see <a href=\"http://www.tetonpost.com/citycon\">www.tetonpost.com/citycon</a>.)  If you are citizen of Second Life, <a href=\"http://slurl.com/secondlife/Capitol%20Hill%201/173/17/30/\">visit Steve's Info Center</a>. If you are a developer -- Second Life or Web 2.0 -- use openness as an accelerant to your own ideas for mashing-up congressional information and enter our <a href=\"http://sunlightfoundation.com/mashup\">Mash Up Contest</a> before the April 15, 2007 deadline for chance to win $2,000. We are eager to see what's next!</div>\r\n</div>", "date_published": null, "comment_count": 0, "slug": ""}}, {"pk": 2, "model": "blogdor.post", "fields": {"status": "draft", "last_updated": "2008-12-04 17:56:22", "author": 3, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Machine Found Data vs Human Entered Data", "excerpt": "", "content": "Friends, developers, NGOs, lend me your eyes;\r\nI come to explain data visualizations, not to praise them.\r\nThe evil that men do lives after them;\r\nThe data can oft  be inferred from their documents.\r\n-- Mark Anthony, if he did data visualizations\r\n\r\nThis post considers if the current state of technology makes \"machine found data\" more useful and economical over \"human entered data\" and if machine found data is a better bet for developers and non-profits to place their efforts for the immediate future. (Yes, it probably is, for those not wanting to read the entire post.)\r\n\r\nMost of the data visualizations I have seen lately have the following in common:\r\n* uses machine found data or data received from another party\r\n* large sample set\r\n* frequently includes time\r\n* data extracted from other occurring activities\r\n* trends rather than search\r\n\r\nWhen I think of the datasets which non-profits in the government oversight and money and politics arena, I see the following in common:\r\n* human entered data or data requiring human standardization\r\n* small to medium sample sets\r\n* linear aggregations\r\n* duration aggregation but little trend analysis\r\n* data created specifically for searching\r\n\r\nMy hypothesis is the most significant determinant of creating data visualizations is the choice of whether to work with machine found data or to work with human entered data. I believe the affordances and mindset of working with machine found data tends to lead toward meaningful data visualizations while working with human entered data tends lead away from data visualization and toward anecdotal ___\r\n\r\n\r\n\r\n\r\nI've been thinking about this for a while, but two visualizations on the Dow Jones Insight blog (via O'Reilly Blog) set me to writing.  Both visualizations count words in documents to make pretty pictures. Or to say it more technically: Both visualizations use machine-based textual analysis to extract information for statistical comparisons. \r\n\r\nthat use textual analysis of media coverage to generate a statistical analysis that can be visualized. In other words, Dow Jones counts words to make pretty pictures. \r\n\r\nCounting words is a great way to\r\n\r\n\r\n\r\n\r\n. The first visualizes candidate speeches. The second visualizes press sentiment toward candidates. Both are examples of what I would describe as machine found data. Machine found data is structured data generated by a computer from existing digital content. Read this little tidbit from the Down Jones post:\r\n\r\n\"The system considered 65,374 press documents and found 26,435 of them to contain either favorable or unfavorable language dominating in reference to a candidate.\"\r\n\r\n\r\n\r\n\r\n\r\n", "date_published": null, "comment_count": 0, "slug": ""}}, {"pk": 35, "model": "blogdor.post", "fields": {"status": "draft", "last_updated": "2009-01-09 18:02:19", "author": 3, "timestamp": "2009-01-09 18:02:19", "markup": "none", "title": "Weekly Lab Report Wrap-Up", "excerpt": "Test test", "content": "<u>GENERAL NEWS</u>\r\n\r\n<b>Faster Memcache! Kill! Kill!</b> Another nail in the traditional relational databases coffin...the powerful, scalable tool  MemcacheDB is now available. W0ot!\r\n\r\n<b>We Are All Mario.</b> Clay Johnson discovered World9's Mario-sound effect and happily bounced around the office. \r\n\r\n<b>NYT API 4 MoC.</b> NYTimes releases a ... wait for it ... API for Members of Congress. Very cool. Terms of Service with a low-five figure call limit, less cool. Sunlight's Labs API ninja James Turk reviews.\r\n\r\n<b>New Congress, New Data. No Problem.</b> Sunlight Labs gets updated in a single day with the new members of Congress thanks to James' work tying the Labs API to authoritative sources.\r\n\r\n<b>Tweet-of-the-Week</b>\r\n\r\n<u>Maven Tim's Restaurant</u>\r\nChina Lee, Silver Spring Maryland. As promised, great Dim Sum served in the afternoon. Waited an hour to be seated last Sunday.\r\n\r\n\r\n\r\n", "date_published": null, "comment_count": 0, "slug": "weekly-lab-report-01-09-09"}}, {"pk": 36, "model": "blogdor.post", "fields": {"status": "draft", "last_updated": "2009-01-10 15:21:25", "author": 6, "timestamp": "2009-01-10 15:21:25", "markup": "markdown", "title": "Fixing Your Seat at the Table", "excerpt": "", "content": "Early on in the Presidential Transition, [http://change.gov](Change.gov) announced an incredibly compelling, never-before-done process: [http://change.gov/open_government/yourseatatthetable](Your Seat at the Table). They announced that every document that the transition team received in a meeting where there was three or more attendees would be posted online.\r\n\r\nLet me be clear: that was an *awesome* step and the change.gov team should be commended for taking it.\r\n\r\nThat being said it looks like the change.gov team is learning as they go.  I respect that: being the first people to try this stuff means you've got to. The process has been less than perfect, and I hope what they're learning as a community is that being transparent requires a lot of work, and often opens the door to a lot more requests. I cannot imagine the time and expense they've occurred building such a project for the American people. Here's some things I've noticed.\r\n\r\nAs of today:\r\n1.  Weeks of documents are missing\r\n2.  When you click on \"page 2\" of the [http://change.gov/open_government/yourseatatthetable](main page) you receive an error; [http://change.gov/S=5ca7c563bed3878feb30bbec035a51e83fd3a690/open_government/yourseatatthetable/P10/](The page you requested is not available right now.)\r\n3. Several of the documents on the front page have dates in the future. \r\n4. You can only search once every 15 seconds. \r\n\r\nI'm *certain* that there's a strong amount of effort being put on this by the administration and they're taking it very seriously, but I suspect that it will never be good enough. Transparency's a big job!\r\n\r\nWhat's interesting though, is that there's probably a much easier solution for them that would require a lot less work: \r\n\r\n1. Standardize the names of the documents, and put them on an FTP server or an index-free http page somewhere \r\n2. update that FTP server when documents come in and are sanitized. \r\n3. write a blog post telling us it is there. In the blog post, say \"people are encouraged to build their own search interfaces and browsing interfaces to the data as they see fit\" \r\n\r\nThe result? Instead of having a functionally broken [http://change.gov/open_government/yourseatatthetable](page) you'll likely have the [http://sunlightlabs.com](Sunlight Labs) scrambling to build an interface, [http://watchdog.net](Watchdog.net) parsing the data and incorporating it, and media organizations like the New York Times working on their own too. Plus, you'd earn praise from the tech community for being participatory and more open.\r\n\r\nThe point being: This transparency business gets really tough if you make it tough, but if you're looking for the easy way out, there's likely a much simpler solution. And often the simpler ones end up being more sophisticated and impressive anyhow.\r\n\r\n", "date_published": null, "comment_count": 0, "slug": "seat_at_the_table"}}, {"pk": 51, "model": "blogdor.post", "fields": {"status": "draft", "last_updated": "2009-02-02 10:36:34", "author": 6, "timestamp": "2009-02-02 10:36:34", "markup": "markdown", "title": "Legislative Transparency", "excerpt": "", "content": "We work hard to make congressional data more transparent and available to the world, and often times we forget how good we've got it. Because if you look towards many other legislative bodies, they aren't doing nearly as good of a job as the U.S. Congress.\r\n\r\nState legislatures have become quite a passion of mine lately. A lot of the legislation passed that truly effects our lives is passed not at the federal level but at the state and local level, and wouldn't it be great to be able to see that?\r\n\r\nI've started building the [http://wiki.sunlightlabs.com/index.php/State_Legislation_Page](State Legislation Page) and so far I've added about 8 states. Others have added a few more taking us to 15. ", "date_published": null, "comment_count": 0, "slug": "legislative-transparency"}}]
