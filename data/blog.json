[{"pk": 31, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 1, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "Under the hood of the DNC and RNC convention sites", "excerpt": "I took a few minutes this morning to look at the technology that powers the DNC and RNC convention web sites. It is always interesting to see what technological decisions different organizations take when they are trying to accomplish similar goals.", "content": "I took a few minutes this morning to look at the technology that powers the DNC and RNC convention web sites. It is always interesting to see what technological decisions different organizations take when they are trying to accomplish similar goals.\r\n\r\n#### Democratic National Convention\r\n\r\n<a href=\"http://demconvention.com/\">http://demconvention.com/</a>\r\n\r\nThe URL for the web site was registered with <a href=\"https://www.domaindiscover.com/\">Domain Discover</a>. The whois information lists the registrant as:\r\n\r\nDemocratic National Committee  \r\n430 S. Capitol St. S.E.  \r\nWashington, DC 20003  \r\n\r\nFor all of the domain squatters out there, the domain expires in November, so keep an eye out. My best guess is that the servers are hosted by <a href=\"http://www.verizonbusiness.com/\">Verizon Business</a>. <a href=\"http://www.servint.net/\">ServInt</a>, based in McLean, VA, provides the hosting for the name servers which are also used by dnc.org and democrats.org, among others.\r\n\r\nThe DNC convention web site is served from an <a href=\"http://httpd.apache.org/\">Apache</a> 2.0.52 web server running on <a href=\"http://www.redhat.com/\">Red Hat Linux</a>. <a href=\"http://www.silverstripe.com\">SilverStripe</a>, a PHP-based, open source content management system, is used to power the site. The main page of the site has a XHTML 1.0 Transitional doctype, but is served as text/html and is not valid XHTML. The site is laid out using divs to define logical elements within the page and CSS to position the elements.\r\n\r\nThe following are screenshots of the page with JavaScript disabled and with both JavaScript and CSS disabled.\r\n\r\n<a href=\"http://blog.sunlightlabs.com/wp-content/uploads/2008/09/dnc_noscript.png\"><img class=\"aligncenter size-thumbnail wp-image-48\" title=\"DNC, no script\" src=\"http://blog.sunlightlabs.com/wp-content/uploads/2008/09/dnc_noscript.png\" alt=\"DNC, no script\" width=\"150\" height=\"115\" /></a>\r\n\r\n<a href=\"http://blog.sunlightlabs.com/wp-content/uploads/2008/09/dnc_noscript_nostyle.png\"><img class=\"aligncenter size-thumbnail wp-image-49\" title=\"DNC, no script, no style\" src=\"http://blog.sunlightlabs.com/wp-content/uploads/2008/09/dnc_noscript_nostyle.png\" alt=\"DNC, no script, no style\" width=\"57\" height=\"150\" /></a>\r\n\r\n#### Republican National Convention\r\n\r\n<a href=\"http://gopconvention2008.com/\">http://gopconvention08.com/</a>\r\n\r\nThe RNC registered their convention's domain with <a href=\"http://www.godaddy.com\">GoDaddy</a>. The registrant is listed as:\r\n\r\nRoman Buhler  \r\n4056 41st Street North  \r\nMcClain [__sic__], Virginia 22101  \r\n\r\n<a href=\"http://www.opensecrets.org/indivs/search.php?name=Buhler&amp;state=VA&amp;zip=&amp;employ=&amp;cand=&amp;c2008=Y&amp;sort=N&amp;capcode=4425k&amp;submit=Submit\">Mr. Buhler</a> is the president of <a href=\"http://www.opensecrets.org/lobby/firmsum.php?lname=Roman+Buhler+%26+Assoc&amp;year=2008\">Roman Buhler &amp; Associates</a>, a lobbying firm based in McLean, VA. Mr. Buhler also <a href=\"http://www.legistorm.com/person/Roman_Buhler/48145.html\">served as counsel</a> of the House Administration Committee from 1989 to 2003. Hosting for the web servers and DNS is provided by <a href=\"http://www.smartechcorp.net/\">Smartech</a>, based in Chattanooga, TN.\r\n\r\nThe web site is served by <a href=\"http://www.iis.net/\">Microsoft IIS</a> 6.0 using <a href=\"http://microsoft.com/\">Microsoft's</a> <a href=\"http://www.asp.net/\">ASP.Net</a> language. The main page declares a generic XHTML namespace, but does not declare a doctype, is served as text/xml, and does not validate as XHTML. The site is laid out using HTML tables.\r\n\r\nThe following are screenshots of the page with JavaScript disabled and with both JavaScript and CSS disabled.\r\n\r\n<a href=\"http://blog.sunlightlabs.com/wp-content/uploads/2008/09/rnc_noscript.png\"><img class=\"aligncenter size-thumbnail wp-image-50\" title=\"RNC, no script\" src=\"http://blog.sunlightlabs.com/wp-content/uploads/2008/09/rnc_noscript.png\" alt=\"RNC, no script\" width=\"150\" height=\"134\" /></a>\r\n\r\n<a href=\"http://blog.sunlightlabs.com/wp-content/uploads/2008/09/rnc_noscript_nostyle.png\"><img class=\"aligncenter size-thumbnail wp-image-51\" title=\"RNC, no script, no style\" src=\"http://blog.sunlightlabs.com/wp-content/uploads/2008/09/rnc_noscript_nostyle.png\" alt=\"RNC, no script, no style\" width=\"150\" height=\"53\" /></a>", "date_published": "2008-09-02 20:20:40", "comment_count": 1, "slug": "under-the-hood-of-the-dnc-and-rnc-convention-sites"}}, {"pk": 30, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 6, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "Google Spreadsheet and the Sunlight Labs API", "excerpt": "James is finishing up a tweak to the Sunlight Labs API that allows for fairly sophisticated search for members of Congress, it isn't \"published\" yet but it is active so if you want to experiment you're welcome to try it out, but for now it is \"unofficial\".", "content": "James is finishing up a tweak to the Sunlight Labs API that allows for fairly sophisticated search for members of Congress, it isn't \"published\" yet but it is active so if you want to experiment you're welcome to try it out, but for now it is \"unofficial\".\r\n\r\nHere's the deal: We wanted a better way for people to search for members, as members of congress are often times referred to by different names-- think \"Ted Kennedy,\" \"Edward Kennedy,\" \"Teddy Kennedy\" etc. Whether it is nicknames or typos, it makes analyzing data difficult if names are not standardized.\r\n\r\nWe're not saying we've come up with a complete solution to name standardization or even congressional name standardization, but we've got a simple solution that might make some lives easier. To demonstrate, we'll use Google Spreadsheets. Follow along at home!\r\n\r\n__Step 1:__ Get an API key from Sunlight Labs [here](http://services.sunlightlabs.com/api/ \"Sunlight Labs API\").\r\n\r\n__Step 2:__ Create a Google Spreadsheet\r\n\r\n__Step 3:__ Name the columns of your spreadsheet \"Member\", \"Firstname\", \"Lastname\" so it looks like this:\r\n\r\n![](http://plusoneme.com/step1.jpg)\r\n\r\n__Step 4:__ Let's add some wacky mispellings and some semi-dirty data like below:\r\n\r\n![](http://plusoneme.com/step2.jpg)\r\n\r\n__Step 5:__ Here's where it gets fun. We'll use the importXML function in Google Spreadsheets to take the values of our dirty data and send them to the Sunlight Labs API, and get a firstname. Enter this code into your spreadsheet:\r\n\r\n    =importXML(\"http://services.sunlightlabs.com/api/legislators.search.xml?apikey=YOURAPIKEYHERE&amp;name=\"&amp;A2,\"//firstname\")\r\n\r\nSee below for an example:\r\n\r\n![](http://plusoneme.com/step3.jpg)\r\n\r\n__Step 6:__ Do the same for the last name column, but change your call to parse the lastname, like so:\r\n\r\n    =importXML(\"http://services.sunlightlabs.com/api/legislators.search.xml?apikey=YOURAPIKEYHERE&amp;name=\"&amp;A2,\"//lastname\")\r\n\r\n__Step 7:__ Finish it up! Select the first values of those newly processed columns and fill in the rest of the values like so:\r\n\r\n![](http://plusoneme.com/step4.jpg)\r\n\r\nNeat! Clean easy name cleanup in your spreadsheet!", "date_published": "2008-08-21 20:55:38", "comment_count": 0, "slug": "google-spreadsheet-and-the-sunlight-labs-api"}}, {"pk": 29, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 5, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "\"Cool project, what CMS did you guys use?\"", "excerpt": "Sunlight Labs is often asked \"What CMS do you use?\". James discusses our development philosophy and the drawbacks of CMS selection.", "content": "#### Pass223.com\r\n\r\nI recently worked on [Pass223.com](http://pass223.com \"Pass 223\"), a simple site that urges the Senate to pass a piece of legislation that requires the Senate to adhere to the same electronic financial disclosure rules in place for representatives and presidential candidates.\r\n\r\nPass223.com is similar to that of hundreds of related action sites: choose a legislator, call them, report results, repeat.\u00a0 I wrote the code, our esteemed creative director Kerry did the bulk of the design, and various others here at Sunlight helped to refine the concept and wording of the site and call script.\r\n\r\nIt was a bit surprising seeing how positive the [feedback](http://www.dailykos.com/story/2008/8/6/9501/66080/422/562998) has been for such a simple site.\u00a0 A number of people have been pointing to Pass223 as an example of how this type of thing should be done.  Most of that credit goes to the team that worked together to revise the awesomely straightforward script.\r\n\r\nThe other question that has come up is what content management system (CMS) Pass223 was done on and what legislative database it was built on top of.  This made me think about the other reason Pass223 was able to come together the way that it did, the tools used behind the scene.\r\n\r\n#### When all you have is a hammer...\r\n\r\nIt seems, especially in the nonprofit world where developers are sparse, that content management systems like [Drupal](http://drupal.org/ \"Drupal :(\") are considered the solution to every problem that arises.\u00a0 Because Content Management Systems can not possibly do everything that organizations want they are left with two options: attempt to mangle the CMS to do things it was never intended to do, or alternatively not get what they actually want.\u00a0 Because of the difficulty in dealing with the massive codebases of most CMSs, they often find themselves accepting both results.\u00a0 A great deal of time is therefore sunk into a project and in the end things still don't work quite how they were planned.\r\n\r\nThe supposed benefit of a CMS is the speed of deployment and ease of use, but as [Jeremy's recent post about LetOurCongressTweet](http://blog.sunlightlabs.com/2008/07/11/from-idea-to-production-in-six-hours/ \"LOCT post\") mentioned, we are able to rapidly create sites without the use of a CMS.\u00a0 And in reality, struggling to fit an innovative project or idea into the rigid structure of a CMS is not easy nor fast.\r\n\r\nA better use of the time and money spent maintaining and modifying complex CMS installations would be to spend that time learning and deploying sites in a framework such as [Django](http://djangoproject.com Django :)\") or [Ruby on Rails](http://www.rubyonrails.org/ \"Ruby on Rails\").\r\n\r\n#### Perfectionists with Deadlines\r\n\r\nDjango in particular was created to solve this problem.  Working with a bloated CMS forces you to make a decision between getting what you want and getting something fast, and more often than not you wind up with neither.  It is because of this that a team originally working at the [Lawrence Journal-World](http://www2.ljworld.com/) newspaper built Django to meet their needs as \"perfectionists with deadlines.\"\r\n\r\nFrameworks like Django provide all of the pieces of commonly used functionality, user registration, Object-Relational mappers to avoid dealing with the database directly, caching, and a ton more.  All of these pieces are given to you without any mandate that they must all be used, they are simply building blocks from which your particular project can choose to use or not.  A large site such as  [EarmarkWatch](http://earmarkwatch.org) may need complex user profiles, whereas it is possible to eschew all of the unneeded modules and build something as quick and simple as Pass223.com.\r\n\r\nUltimately, unless some CMS already provides exactly what you want, it is far easier to build a project from reusable components within a framework than to attempting to teach an old CMS a new trick.  One of the reasons that Pass223.com seems to impress people used to looking at the typical contact-your-legislator forms is because we had the flexibility to build what we wanted.", "date_published": "2008-08-07 18:04:44", "comment_count": 4, "slug": "cool-project-what-cms-did-you-guys-use"}}, {"pk": 28, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 1, "timestamp": "2008-12-04 17:56:22", "markup": "markdown", "title": "From Idea to Production in Six Hours", "excerpt": "We recently decided to launch a petition-like site that uses Twitter as the organizing method; using one of the very technologies that are impacted by Congressional Web use restrictions. We knew this had to be timely to have an impact, so the decision was made to have the Web site completed by the end of the day. That gave Kerry Mitchell, our fearless Creative Director, and I about six hours to get the site completed.", "content": "There has been some [commotion](http://blog.sunlightfoundation.com/2008/07/10/let-our-congress-tweet/) over the past few days regarding Congresstional Web use restrictions. The rules are inadequate for the current state of the Web and must be rewritten to reflect changes in technology. Republicans and Democrats have been going back-and-forth over proposed changes to the existing rules; one side claiming the other is trying to stifle their communication. While they keep on bickering, we wanted to raise awareness of these Web use restrictions and get people involved.\r\n\r\nWe decided to launch a petition-like site that uses Twitter as the organizing method; using one of the very technologies that are impacted by Congressional Web use restrictions. We knew this had to be timely to have an impact, so the decision was made to have the Web site completed by the end of the day. That gave Kerry Mitchell, our fearless Creative Director, and I about six hours to get the site completed.\r\n\r\nSo how did it go?\r\n\r\n#### Twitter\r\n\r\nAs LOCT is a petition-like site, it is important to get a list of the people that are following the LOCT Twitter account. Twitter has a very nice API that makes it easy to pull information from the service. To get JSON list of people following your Twitter account, just send an HTTP GET request to <code>http://twitter.com/statuses/followers.json</code> using [HTTP Basic Authentication](http://en.wikipedia.org/wiki/Basic_authentication_scheme) with your username and password. You can also get a list of people following other user accounts from <code>http://twitter.com/statuses/followers/&lt;username&gt;.json</code>; no authentication necessary. We query Twitter for this information and cache it locally in a database.\r\n\r\nUnfortunately, due to Twitter's recent performance issues, many of the nicest features have either been limited or disabled, making it almost impossible to use Twitter exclusively for LOCT. We needed to get a list of tweets that mentioned LOCT, but couldn't with the current performance restrictions in place. If only there was another service that provided this functionality.\r\n\r\n#### And there is!\r\n\r\n[Summize](http://summize.com/) rocks. Based right here in the greater Washington metropolitan area, Summize is tweet search service that has one of the few direct feeds into every tweet that is twittered. They also have an awesome API that makes it dead simple to search all tweets. <code>http://summize.com/search.json?q=%23loct08</code>. That is all you need to get search results in JSON. Just like the list of followers from Twitter, the results are cached locally for Maximum Performance<sup>TM</sup>.\r\n\r\n#### I would follow Django if it had a Twitter account\r\n\r\nAs with almost all projects created by [Sunlight Labs](http://sunlightlabs.com), [Let Our Congress Tweet](http://letourcongresstweet.org/) is writting using [Django](http://djangoproject.com), a [Python](http://python.org) Web development framework. I love Django. It simplifies development by providing [object-relational mapping](http://en.wikipedia.org/wiki/Object-relational_mapping), templating, and other features in an unobtrusive way.\r\n\r\nDeveloping in Django is already quite rapid, but by reusing existing code we can develop at an unheard-of pace. Writing a reusable Django application is quite easy as it is nothing more than a standard Python module that can be used in any project.\r\n\r\n#### Feedinating the countryside\r\n\r\nA few weeks ago we released a new version of the [Sunlight Foundation](http://sunlightfoundation.com/) Web site. The old, infuriating Drupal installation was replaced with a slick Django application that was written in-house. One of the main features of the new site is feed aggregator that pulls in the recent blog posts from across the Sunlight-influenced transparency network. To accomplish this, we wrote _Feedinator_, a Django feed aggregator application that makes it easy to pull in feeds from multiple blogs and display them in different ways on a Web site.\r\n\r\nWe use Feedinator on Let Our Congress Tweet to pull in the feed of Tweets from the [LOTC08 twitter account](http://twitter.com/loct08/) and a [del.icio.us](http://del.icio.us/) feed of Web sites that have mentioned LOCT. By writing Feedinator in a way that makes it easily reusable, we were able to start incorporating feeds into LOCT in a matter of minutes.\r\n\r\nIf you would like to use Feedinator in your own project, you are in luck. We plan on releasing the code in the near future as well as the code for a few other Django applications and Python modules.\r\n\r\n#### Designers are useful\r\n\r\nWhile I was coding, Kerry took care of the design, CSS, and HTML. A few minutes of converting the HTML into Django templates and the site was up and running.\r\n\r\nSo that's it! After throwing in a few cron entries and some Apache configuration files, the site went live. We went from an idea to a production ready Web site in about six hours. Sure, the it isn't overly complicated, but we're proud of it nonetheless.", "date_published": "2008-07-11 16:07:59", "comment_count": 1, "slug": "from-idea-to-production-in-six-hours"}}, {"pk": 27, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 6, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "On Baseball and Congress", "excerpt": "", "content": "<p>Modern baseball&#8217;s origins are something historians don&#8217;t have a good read on. If you look at the <a href=\"http://en.wikipedia.org/wiki/Origins_of_baseball\">Origins of Baseball</a> article on Wikipedia, you&#8217;ll see that we don&#8217;t know very much about where the rules came from, but it formalized somewhere around 1845 when the Knickerbocker Club of New York City began to play baseball against the New York Nine. In 1857 16 clubs finally sent delegates to a convention to standardize the rules and standardize America&#8217;s Pastime.</p>\r\n\r\n<p><a href=\"http://en.wikipedia.org/wiki/Baseball_statistics\">Baseball statistics</a> have their own story. A fellow named <a href=\"http://en.wikipedia.org/wiki/Henry_Chadwick_(writer)\">Henry Chadwick</a> was the first to start using statistics to judge a player&#8217;s performance. It was a few years after the sport was invented and formalized that this young journalist would give himself the goal of creating &#8220;numerical evidence as that would prove what players helped or hurt a team win.&#8221;</p>\r\n\r\n<p>It took nearly 100 years for baseball statistics to make it to the common man and woman. It wasn&#8217;t until 1951 when a researcher named Hy Turkin published the Encyclopedia of Baseball that used a computer to compile statistics for the first time. It wasn&#8217;t until 1977 that decent, predictive and objective statistical methods called <a href=\"http://en.wikipedia.org/wiki/Sabermetrics\">sabermetrics</a> were invented and distributed. Sabermetrics are the analytical methods that Theo Epstein used to break a curse and build a World Series winning Boston Red Sox. He even hired Sabermetrics&#8217; inventor to work for the Red Sox.</p>\r\n\r\n<p>It took over a century to invent and distribute the objective performance measuring statistics we use today to evaluate baseball players. To tell whether or not Greg Maddux is a better pitcher than Pedro Martinez. </p>\r\n\r\n<p>But though <a href=\"http://en.wikipedia.org/wiki/First_Continental_Congress\">Congress</a> has been around for nearly 234 years we still don&#8217;t have an objective way to tell whether or not my namesake, <a href=\"http://en.wikipedia.org/wiki/Henry_Clay\">Henry Clay</a> was as effective of a speaker as <a href=\"http://en.wikipedia.org/wiki/Nancy_Pelosi\">Nancy Pelosi</a>. This isn&#8217;t to say we haven&#8217;t been making up our own statistics. We&#8217;ve been compiling subjective <a href=\"http://www.lcv.org/scorecard/\">scorecards</a> for years. But we need a process of standardizing our statistics, publishing how they&#8217;re calculated, and we need to build a system for authenticating and delivering those results.</p>\r\n\r\n<p>It took us over 100 years to get good baseball stats, but this isn&#8217;t to say that the data didn&#8217;t exist or wasn&#8217;t being recorded. The data was still there. Here&#8217;s the full stats on the <a href=\"http://www.baseball-reference.com/teams/CHC/1876.shtml\">1876 Chicago White Stockings</a>. People were watching, keeping score, logging the games and recording the data and even making their own statistics out of it. But it was the process of standardizing the statistics, publishing how they&#8217;re to be calculated and sharing the results that made these <em>subjective</em> metrics effective.</p>\r\n\r\n<p>So what metrics out there are effective in evaluating our legislators? Off the top of my head, here&#8217;s some elementary ones:</p>\r\n\r\n<ol>\r\n<li><p>Attendance Percentage: The percent of time a member attends Congress when it is in session.</p></li>\r\n<li><p>Vote percentage: The percentage of time a member has voted when they&#8217;ve had the opportunity to do so.</p></li>\r\n<li><p>Sponsored Bills Per Term: The average number of bills sponsored and co-sponsored per term</p></li>\r\n<li><p>Sponsored Bills Passed Per Term: The average number of sponsored and co-sponsored bills passed per term</p></li>\r\n<li><p>Party percentage: The percentage of time the member votes with their political party</p></li>\r\n<li><p>Vote Victory Percentage: The percentage of time the member votes with a bill that passes.</p></li>\r\n</ol>\r\n\r\n<p>These are just obvious building blocks of a much more sophisticated statistical system. All these statistics exist right now. Sunlight&#8217;s partner, <a href=\"http://opencongress.org\">Open Congress</a> and <a href=\"http://govtrack.us\">GovTrack.us</a> and many more track their congressional statistics in their own way as do many others. In order to do it right we need:</p>\r\n\r\n<ol>\r\n<li><p>Standardization: We need to be calculating these things and naming these statistics the same way everywhere.</p></li>\r\n<li><p>Comparison: Statistics are not relevant unless they&#8217;re in context. We need to be able to create a ranked 1-535 list for every statistic we standardize and create</p></li>\r\n<li><p>Adoption: They need to be adopted and as pervasive as RBIs and ERAs.</p></li>\r\n<li><p>More: We need more statistics made from the data that congress generates that provide &#8220;numerical evidence&#8221; about the effectiveness of congress. We need our own &#8220;sabermetrics&#8221; that objectively evaluate whether or not a Member of Congress is a effective at representing those that chose to elect them. The ones I've listed aren't even close to being accurate predictors.</p></li>\r\n</ol>\r\n\r\n<p>So let this be a post to start a discussion amongst the transparency community about how we can begin standardizing our own objective statistics, making them useful and centralized. Let&#8217;s start working together to invent new statistics for how our members can be evaluated. </p>\r\n", "date_published": "2008-06-30 02:49:54", "comment_count": 3, "slug": "call-to-action-learn-from-baseball"}}, {"pk": 26, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 6, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Fun with CapitolWords", "excerpt": "", "content": "We launched <a href=\"http://www.capitolwords.org\">Capitol Words</a> just a couple weeks ago and got a <a href=\"http://www.boingboing.net/2008/06/19/daily-congressional.html\">really</a> <a href=\"http://news.oreilly.com/2008/06/requesting-a-mashup.html\">great</a> <a href=\"http://www.metafilter.com/72702/Capitol-Words-US-Congress-In-A-Word-A-Day\">reception</a> from the blogs. I\u2019m two weeks in to my new duties as Director of Sunlight Labs and while I didn\u2019t have much (really, anything) to do with the project's success, I am really excited about it. With the <a href=\"http://www.capitolwords.org/api/\">CapitolWords API</a> we can start doing some interesting analysis of overall word-usage in Congress.\r\n\r\nSome of this is obvious and you can see at the surface. Check out the screenshots below:\r\n<img src=\"http://img.skitch.com/20080629-c7ysh38su9w462su2ci9pbk1mj.jpg\" alt=\"June, 2005\" />\r\n\r\n<img src=\"http://img.skitch.com/20080629-x9ngxqcys8qndyk1wh1xcfitws.jpg\" alt=\"June, 2007\" />\r\n\r\n<img src=\"http://img.skitch.com/20080629-cdfu7h3rt82sre8tqc97b8ct92.jpg\" alt=\"June, 2008\" />\r\n\r\nJune seems to be predominantly about energy and oil. Septembers of even numbered years tend to be about security and intelligence. March tends to be about budgets and amounts.\r\n\r\nNeat! Josh wrote most of the code and handled the architecture of the system. Garrett who heads our <a href=\"Louis\">http://www.louisdb.org</a> project also had a big hand in concieving and building the application. Of course Kerry, our wonderful Creative Director helped make the user interface and designed the site. It is written in Django and MySQL. Great work guys!", "date_published": "2008-06-29 19:08:25", "comment_count": 0, "slug": "fun-with-capitolwords"}}, {"pk": 25, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 3, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Open Government Site Govtrack Goes Open Source", "excerpt": "", "content": "<img align=\"right\" title=\"GovTrack.US logo\" id=\"image39\" alt=\"GovTrack.US logo\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2008/04/picture-45.thumbnail.jpg\" /><img align=\"right\" title=\"OpenSource.org logo\" id=\"image40\" alt=\"OpenSource.org logo\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2008/04/picture-52.jpg\" />Josh Tauberer, one of the champs of opening U.S. Government data announced today he has made all of <a title=\"Govtrack.us\" href=\"http://govtrack.us\">Govtrack.us</a> available as open source.\r\n<blockquote>This week I made <a target=\"_blank\" href=\"http://www.govtrack.us/\">www.GovTrack.us</a> officially totally open source.\r\nGovTrack is a website that tracks U.S. federal legislation and also\r\nbuilds the only comprehensive open database of congressional\r\ninformation. While the data behind GovTrack has been provided in the\r\npublic domain for a number of years now, and has been successfully\r\npowering a bunch of other sites like OpenCongress, I've been playing\r\ncatch-up in getting the source code of the website opened up.</blockquote>\r\nRun at get the code! (<a title=\"Govtrack.us source code\" href=\"http://www.govtrack.us/source.xpd\">link</a>)", "date_published": "2008-04-03 14:17:44", "comment_count": 0, "slug": "open-government-site-govtrack-goes-open-source"}}, {"pk": 24, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 3, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "", "excerpt": "", "content": "<img width=\"331\" height=\"247\" align=\"right\" alt=\"Diagram of financial support for public government sites\" title=\"Diagram of financial support for public government sites\" src=\"http://visiblegovernment.ca/images/blog/MoneyFlowSmall.PNG\" /><img width=\"337\" height=\"254\" align=\"right\" alt=\"Flows of data from government to non profit government sites\" title=\"Flows of data from government to non profit government sites\" src=\"http://visiblegovernment.ca/images/blog/DataFlowSmall.PNG\" />\r\n\r\nW0ot! Thanks for the excellent diagram WavingSparks.\r\n\r\nIt's very helpful to have such a <a href=\"http://waving.deadsquid.com/?p=29\">nice graphic explaining the data flows</a>. As you discovered, Sunlight is very interested in financially supporting and being part of making transparency information available.\r\n\r\nIn interest of transparency, Sunlight is also supporting <a title=\"Taxpayer for Common Sense\" href=\"http://www.taxpayer.net/\">Taxpayers for Common Sense</a> to update their website and offer smaller transparency grants, too (<a title=\"Sunlight Foundation Grants\" href=\"http://sunlightfoundation.com/grants\">http://sunlightfoundation.com/grants</a>). <a title=\"GovTrack\" href=\"http://govtrack.us\">GovTrack</a> is a pretty efficient and essential website and is nobody's weak link! For more related websites check out:\r\n<ul>\r\n\t<li><a title=\"Insanely Useful Government Data Sites\" href=\"http://sunlightfoundation.com/resources\">http://sunlightfoundation.com/resources</a></li>\r\n\t<li><a title=\"ProgrammableWeb Government Directory\" href=\"http://sunlightfoundation.com/resources\">http://www.programmableweb.com/government</a></li>\r\n</ul>\r\n(Link: <a href=\"http://waving.deadsquid.com/?p=29\">http://waving.deadsquid.com/?p=29)</a>", "date_published": "2008-03-14 22:59:19", "comment_count": 0, "slug": "38"}}, {"pk": 23, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 3, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Open Source Semantics Picking Up Speed", "excerpt": "", "content": "<img align=\"right\" alt=\"Screenshot of size of Metaweb WEX download\" title=\"Screenshot of size of Metaweb WEX download\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2008/02/picture-42.jpg\" />Metaweb has announced an open source release of structured data from Wikipedia. Via the email from get.theinfo email list:\r\n<blockquote><em>\"Hello from Metaweb. We've just released a GFDL licensed extraction of\r\nWikipedia in XML + relational form. Anyone is welcome to use it for\r\nany purpose...\"</em></blockquote>\r\nThis follows Reuters recent announcement of Open Calais API to extract people, places, things, and simple relationships from unstructured text. (We are experimenting with similar techniques of entity tagging via open protocols at Sunlight.) Metaweb's WEX's is 57GB of download-able structured data from the largest peer-production encyclopedia project ever.  The Semantic Web, so long discussed, is now beginning a virtuous cycle of innovation.  We are entering the age of <em>open source semantics</em>. Like compounding interest, Moore's Law, and exercise, results from the cycle of innovation around open source semantics will multiply quickly. If you thought Google circa 2007 is impressive, buckle your seatbelt and reach for your helmet. Things are about to move even faster.\r\n\r\nAddendum: DBPedia is another project extracting data from Wikipedia in the RDF format.\r\n\r\n<strong>Links</strong>: \u2022<a href=\"http://download.freebase.com/wex/\">Metaweb's WEX</a> \u2022<a href=\"http://www.opencalais.com/\">Open Calais</a> \u2022<a href=\"http://www.readwriteweb.com/archives/reuters_calais.php\">ReadWriteWeb on Open Calais</a> \u2022<a title=\"DBPedia\" href=\"http://dbpedia.org\">DBPedia</a>", "date_published": "2008-02-19 13:51:17", "comment_count": 0, "slug": "open-source-semantics-picking-up-speed"}}, {"pk": 22, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 3, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "", "excerpt": "", "content": "<img width=\"300\" align=\"right\" alt=\"Screenshot Social Graph API code page\" title=\"Screenshot Social Graph API code page\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2008/02/picture-31.jpg\" />The <a title=\"Social Graph API Code page\" href=\"http://code.google.com/apis/socialgraph/\">Social Graph API page</a> on Google code exemplifies the future of multimedia learning and why open source is being so productive relative to proprietary software development efforts. A two and half minute YouTube video introduces the concept. Links are included to documentation and examples. I've got code, examples, documentation, and even a human giving me a tutorial. (This one intro video could easily be expanded to series of step-by-step instructions.) Five developers might jump on the Social Graph API or 5,000. The right five might matter more than having 5,000 developers. What matters is two points. First, beneficial multimedia is no longer expensive to produce or distribute. In fact, it is becoming a basic skill of people everywhere. Second, the distributed nature of open source mentality encourages people to provide information in economical paired-down forms\u2014even if the code is not open source but merely an API.", "date_published": "2008-02-11 17:53:36", "comment_count": 0, "slug": "34"}}, {"pk": 21, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 4, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Stay-at-home Servers", "excerpt": "", "content": "<a target=\"_blank\" href=\"http://gizmodo.com/photogallery/microserveces08/\">Microsoft propoganda targets</a> our most vulnerable citizens - <em>our children</em>!", "date_published": "2008-01-09 19:10:26", "comment_count": 0, "slug": "stay-at-home-servers"}}, {"pk": 20, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 5, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "polipoly - A tool for dealing with district boundary polygons", "excerpt": "", "content": "Finding out what congressional district an address falls within can be a difficult problem. Typically you need a 9 digit zip code, and the US Postal Service does not make looking up the zip+4 easy.  Even with a zip+4, existing solutions are either expensive or inaccurate and sometimes both.\r\n\r\nIt turns out that the US census bureau publishes <a href=\"http://www.census.gov/geo/www/cob/cd110.html\">polygon files</a> defining the boundaries of all 435 congressional districts. Through geocoding services such as google maps we can easily convert an address to a latitude and longitude and therefore it is possible to determine what district an address lies within by simply testing what polygon it falls within.\r\n\r\nWe <a title=\"recently added a new API method\" href=\"http://sunlightlabs.com/blog/?p=29\">recently added a new API method</a> but one of the major drawbacks is that because it uses Google as a geocoding service we are limited to 50,000 calls a day.  We're aware that some organizations may want to do batch processing of their membership lists, which may means thousands of lookups at once.  In addition to using up all of our geocoding requests, calling a web service for this kind of batch processing isn't efficient.\r\n\r\nTo attempt to meet these challenges we are proud to announce <a title=\"polipoly\" href=\"http://code.google.com/p/polipoly/\">polipoly</a>.  Polipoly is a small python library that uses public domain census shapefiles to allow developers to write simple python scripts that can relate addresses to congressional districts.  And because you're running it with your own Google Maps API key, you are able to use all 50,000 geocoding requests a day.\r\n\r\nFor information on installation and usage head over to the project page. Example source for a web service that works just like <a target=\"_blank\" href=\"http://sunlightlabs.com/api/places.getDistrictFromAddress.php\">places.getDistrictFromAddress</a> has been released, as well as an example of processing a CSV file similar to the kind that could be exported from a database or spreadsheet.\r\n\r\nThis is the second project we have released on Google Code (the first being the <a href=\"http://code.google.com/p/sunlightadk/\">Sunlight ADK</a>) but much more is on the way in terms of open source.", "date_published": "2007-12-10 10:00:58", "comment_count": 0, "slug": "polipoly"}}, {"pk": 19, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 4, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Crossposted from my personal blog", "excerpt": "", "content": "<div class=\"storycontent\">If you like APIs and mashups, you should check out <a target=\"_blank\" href=\"http://www.programmableweb.com/\">ProgrammableWeb</a> - it\u2019s a directory/advice/analysis site for all things API-ish.\r\n\r\nMost recently, they\u2019ve been working on building a subsection for <a target=\"_blank\" href=\"http://www.programmableweb.com/government\">government</a> APIs, and have included 5 APIs that we're involved with, as well as APIs from MySociety.org and a whole lot more.\r\n\r\nAnd who knows, we may have another contest someday, involving mashups that combine these various APIs.</div>", "date_published": "2007-11-13 20:43:46", "comment_count": 0, "slug": "crossposted-from-my-personal-blog"}}, {"pk": 18, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 3, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "New Address to Congressional District API Method", "excerpt": "", "content": "James Turk added a powerful address to congressional district method <a target=\"_blank\" href=\"http://sunlightlabs.com/api/places.getDistrictFromAddress.php\">places.getDistrictFromAddress</a> to the <a href=\"http://sunlightlabs.com/api/\">Sunlight Labs API</a>. The method geocodes a street address using Google's Map API and identifies the congressional district polygon in which the address's lat/long falls using <em>up-to-date</em> shape files from Census.gov.\r\n\r\nHere's an example call:\r\n<pre>http://api.sunlightlabs.com/places.getDistrictFromAddress?address=1818+N+St+NW+Washington,+DC&output=xml</pre>\r\nHere's an example result in XML (json also available):\r\n<pre>&lt;results&gt;\r\n&lt;address&gt;1818 N St NW Washington, DC&lt;/address&gt;\r\n&lt;latitude&gt;38.907231&lt;/latitude&gt;\r\n&lt;longitude&gt;-77.042149&lt;/longitude&gt;\r\n&lt;districts&gt;\r\n&lt;district state=\"DC\"&gt;98&lt;/district&gt;\r\n&lt;/districts&gt;\r\n&lt;/results&gt;</pre>\r\nWhen I started at Sunlight Labs in 2006, I heard companies paying annual fees for zipcode to congressional district translation databases and services. This should no longer be the case, at least for small doses of information. First, we are offering this API method. Second, our service uses official congressional district political boundaries now being (or by 2008 will be) updated in real-time by the Census as they learn of changes. So it should be possible for others to code such a service as well. The only cost should be related to number of addresses needing to be coded. Google reasonably imposes 15,000 calls-per-day limitation from same IP address on its service. So we might look into an alternate geocoding service so we can scale. Then again, we might also just fire up instances of EC2 to source the calls from different IP addresses, too.", "date_published": "2007-11-13 19:54:11", "comment_count": 1, "slug": "new-address-to-congressional-district-api-method"}}, {"pk": 17, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 1, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Sunlight API Development Kit", "excerpt": "", "content": "<p>When working on a prototype, it is often necessary to get a REST web service up and running quickly. It's easy enough to do, but the amount of code that is duplicated for each service can really increase the time to completion. To make the development of REST web services quicker and easier, we have developed the <a href=\"http://sunlightadk.googlecode.com\">Sunlight API Development Kit</a> or, as we affectionately call it, the <a href=\"http://sunlightadk.googlecode.com\">Sunlight ADK</a>. The ADK is a PHP framework that assists in the rapid development of REST web services. We've released the code under the LGPL license.</p>\r\n<h3>REST Framework</h3>\r\n<p>The core of the ADK is a framework for rapid web service development. You don't have to worry about receiving requests, building XML or JSON, or any other tedious code. All you need to do is write code to populate a RESTResponse object. We take it from there and generate XML or JSON from the object depending on the type of response the user requested.</p>\r\n<h3>Administration Application</h3>\r\n<p>So how do you manage all of the services you've written? We were nice enough to include a web based management application that can be used to manage users and service methods. To create a new method just fill in the method name and the name of the PHP class that implements the service. As long as your service class was dropped in the right folder, you'll be up and running with the method.</p>\r\n<h3>Key Management and Access Controls</h3>\r\n<p>The administration application can also be used to issue keys to users of the web service. You can place usage limits on each service method to limit the number of calls or amount of data that is transferred over a variable time period. You can also create methods that are accessible to a limited number of users. Of course if you prefer to have your service open to everyone then a simple configuration change will allow access to the services without a key.</p>\r\n<h3>Get it and contribute!</h3>\r\n<p>We hope you'll find the Sunlight ADK useful. <a href=\"http://sunlightadk.googlecode.com\">Get a copy now</a> and let us know what you think. If you would like to contribute to the project, which we hope you do, please contact us at <a href=\"mailto:labs@sunlightfoundation.com\">labs@sunlightfoundation.com</a>.", "date_published": "2007-10-19 18:33:45", "comment_count": 0, "slug": "sunlight-api-development-kit"}}, {"pk": 16, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 4, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Mini-Grants", "excerpt": "", "content": "If you're a developer-type person, and you are involved in some way in making a difference in the way people interact w/Congress, then you may want to check this out:\r\n<blockquote>\r\n<p class=\"MsoPlainText\"><font size=\"3\" face=\"Times New Roman\"><span style=\"font-size: 12pt\">The Sunlight Foundation/Network is offering grants of $1,000 to $5,000 for local groups that have creative ideas for changing the relationship between representatives and the people they represent. In addition, Sunlight also provides consulting support and some networking opportunities for its Grantees.</span></font></p>\r\n<p class=\"MsoPlainText\"><font size=\"3\" face=\"Times New Roman\"><span style=\"font-size: 12pt\"> </span></font></p>\r\n<p class=\"MsoPlainText\"><font size=\"3\" face=\"Times New Roman\"><span style=\"font-size: 12pt\">We encourage applications from existing small local nonprofits and websites, from offshoots of national groups, from individuals, and from informal groups of citizens. The grants will go towards funding and implementing original ideas that will create a better, more democratic relationship between government and citizens.  </span></font></p>\r\n<p class=\"MsoPlainText\"><font size=\"3\" face=\"Times New Roman\"><span style=\"font-size: 12pt\"> </span></font></p>\r\n<p class=\"MsoPlainText\"><font size=\"3\" face=\"Times New Roman\"><span style=\"font-size: 12pt\">Projects will be judged on how closely they fit with Sunlight\u2019s mission of improving the relationship between citizens and their member of Congress through more transparency of information.  Example of people who have received grants in the past is located here: </span></font><a title=\"http://www.sunlightfoundation.com/grants\" href=\"http://www.sunlightfoundation.com/grants\">http://www.sunlightfoundation.com/grants</a><font size=\"3\" face=\"Times New Roman\"><span style=\"font-size: 12pt\"> (if you scroll to half way there is the mini grant section).  The focus should be on shedding more light on what Congress does and how to improve the communication between citizens and Congress.  As a rule we do not award money for salaries but do for technology upgrades.  </span></font></p>\r\n<p class=\"MsoPlainText\"><font size=\"3\" face=\"Times New Roman\"><span style=\"font-size: 12pt\"> </span></font></p>\r\n<p class=\"MsoPlainText\"><font size=\"3\" face=\"Times New Roman\"><span style=\"font-size: 12pt\">If you are interested in applying, please fill out the provided web form, <a target=\"_blank\" href=\"http://www.sunlightfoundation.com/grants/minigrantapplication\">it is available on the Sunlight Foundation grants page</a>.  Please describe your project with a detailed description of how it fits in with Sunlight\u2019s mission and your goals for your project, an itemized budget (including the amount requested from Sunlight) and contact information. </span></font></p>\r\n<p class=\"MsoPlainText\"><font size=\"3\" face=\"Times New Roman\"><span style=\"font-size: 12pt\"> </span></font></p>\r\n<font size=\"3\" face=\"Times New Roman\"><span style=\"font-size: 12pt\">If you have any questions feel free to contact\u00a0 </span></font><font size=\"3\" face=\"Times New Roman\"><span style=\"font-size: 12pt\">nisha @ sunlightfoundation.com  </span></font><font size=\"3\" face=\"Times New Roman\"><span style=\"font-size: 12pt\">.</span></font></blockquote>\r\nExamples - you are involved in communications/interacting with Congress, and...\r\n\r\nyour laptop died, or you could really use another linux server for your organization, or you can't afford to go to an important conference, etc.\r\n\r\nMaybe we can help!", "date_published": "2007-10-11 15:39:26", "comment_count": 0, "slug": "mini-grants"}}, {"pk": 15, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 1, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Sunlight Labs API changes", "excerpt": "", "content": "<p>Recent updates to the Sunlight API data:</p>\r\n<h4>Deleted Members of Congress</h4>\r\n<ul>\r\n<li>Paul Gillmor (fakeopenID148)</li>\r\n<li>Juanita Millender-McDonald (fakeopenID272)</li>\r\n<li>Charles Norwood (fakeopenID294)</li>\r\n<li>Craig Thomas (fakeopenID528)</li>\r\n</ul>\r\n<h4>Added Members of Congress</h4>\r\n<ul>\r\n<li>John Barraso (fakeopenID600)</li>\r\n<li>Madeleine Bordallo (fakeopenID601)</li>\r\n<li>Paul Broun (fakeopenID602)</li>\r\n<li>Donna Marie Christensen (fakeopenID603)</li>\r\n<li>Eni Fa'aua'a Hunkin (fakeopenID604)</li>\r\n<li>Luis Fortuno (fakeopenID605)</li>\r\n<li>Eleanor Norton (fakeopenID607)</li>\r\n<li>Laura Richardson (fakeopenID608)</li>\r\n</ul>", "date_published": "2007-09-28 20:55:02", "comment_count": 0, "slug": "sunlight-labs-api-changes"}}, {"pk": 14, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 4, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "European Transparency", "excerpt": "", "content": "Our comrades-in-arms on the Euro side of the Atlantic have their own transparency advocates and meetups.\u00a0 Here's an interesting one, called <a target=\"_blank\" href=\"http://people.oii.ox.ac.uk/escher/2007/08/16/berlin-in-august-summary/\">Berlin In August</a>\r\nTwo links in particular caught my eye:\u00a0 <a target=\"_blank\" href=\"http://berlininaugust.politik-digital.de/index.php/Good_Practice\">Good Practices for Transparency Sites</a> and <a target=\"_blank\" href=\"http://berlininaugust.politik-digital.de/index.php/What_is_still_missing\">What is Still Missing</a>\r\nEnjoy!", "date_published": "2007-09-27 14:52:30", "comment_count": 0, "slug": "european-transparency"}}, {"pk": 13, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 4, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Digg EarmarkWatch", "excerpt": "", "content": "If you are a digg user, please consider helping us get the word out about our latest tool:\r\n\r\n<a target=\"_blank\" href=\"http://digg.com/politics/Citizens_Here_is_your_chance_to_investigate_Congress_Find_The_Pork\">http://digg.com/politics/Citizens_Here_is_your_chance_to_investigate_Congress_Find_The_Pork</a>", "date_published": "2007-09-26 13:43:51", "comment_count": 0, "slug": "digg-earmarkwatch"}}, {"pk": 12, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 4, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "EarmarkWatch", "excerpt": "", "content": "Our social app to identify and clean up the data around government earmarks - <a target=\"_blank\" href=\"http://www.earmarkwatch.org/\">EarmarkWatch</a> is live.\u00a0 Props to Kerry and James for all the hard work they did in getting it ready.", "date_published": "2007-09-25 13:00:59", "comment_count": 0, "slug": "earmarkwatch"}}, {"pk": 11, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 1, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Mashup the web", "excerpt": "", "content": "<p>One of our focuses here at Sunlight Labs is to demonstrate how open data enables citizens to be engaged and informed on how Congress works. We do this by creating mashups that make information from a variety of sources easy to manipulate and understand. I recently gave a talk at a <a href=\"http://www.heritage.org/press/carr/bootcamp.cfm\">CARR Boot Camp</a> on using the web to work with data. While not directly related to Congressional transparency, the following example from the talk is a good demonstration of how an end user can work with open data without the assistance of software developers.</p>\r\n<p>We'll start with a site I highly value, <a href=\"http://dcfoodies.com/\">DC Foodies</a>. The site has great reviews of restaurants in the DC area and other related blog posts. Most of the review entries contain the address of the restaurant being reviewed. By itself this is of little value. We can read the post and see the address, but that's about it. Fortunately the site has an RSS feed.</p>\r\n<p>Let's take the RSS feed and run it through Yahoo's fantastic Pipes service. Pipes lets you take RSS and other feeds and execute various operations on the data. The site provides a dead simple visual editor that is used to apply the operatons and filters to the data. For example:</p>\r\n<p><img id=\"image20\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2007/09/picture-1.png\" alt=\"DCFoodies.com in Yahoo Pipes\" /></p>\r\n<p>The pipe in the image above takes the DC Foodies RSS feed, geocodes any location data found in the posts, and filters out entries that do not contain geographic information. Pipes adds custom elements to the RSS feed that contain the latitude and longitude of the restaurants based on the addresses in the reviews. How cool is that? It gets even better.</p>\r\n<p>Among the various formats in which you can output your modified feed, <a href=\"http://code.google.com/apis/kml/documentation/\">KML</a> is specifically suited to syndication of geographic data. We can take the <a href=\"http://pipes.yahoo.com/pipes/pipe.run?_id=GnHiiblc3BGoaGgXCR2yXQ&_render=kml\">URL of the KML output</a> and paste it into the search box of <a href=\"http://maps.google.com\">Google Maps</a>. Click the Search Maps button and Google will <a href=\"http://maps.google.com/maps?f=q&hl=en&geocode=&q=http:%2F%2Fpipes.yahoo.com%2Fpipes%2Fpipe.run%3F_id%3DGnHiiblc3BGoaGgXCR2yXQ%26_render%3Dkml&ie=UTF8&ll=38.996959,-77.02748&spn=0.032352,0.056047&z=14&om=1\">plot the locations</a> from the KML feed on the map. Click on one of the map markers and you will see a summary of the original blog post.</p>\r\n<p>New tools and technologies are allowing end users to mash up content without needing the assitance of software developers. Building open platforms that work on open formats is key to allowing end user manipulation of data across disparate resources.</p>", "date_published": "2007-09-20 19:54:09", "comment_count": 0, "slug": "mashup-the-web"}}, {"pk": 10, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 1, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Refresh DC: Web Widgets: What, Why, and How", "excerpt": "", "content": "<p style=\"background-color: #324B83; padding: 15px;\"><img src=\"http://refresh-dc.org/images/logo.png\" alt=\"Refresh DC logo\" /></p>\r\n<p>It's mid-month which means it's time for another meeting of <a href=\"http://refresh-dc.org\">Refresh DC</a>. <a href=\"http://www.willmeyer.com/\">Will Meyer</a> from <a href=\"http://www.clearspring.com/\">Clearspring</a> will lead an interactive discussion on web widgets in general, and best practices specifically, with some examples as appropriate.</p>\r\n<p>Refresh is a community of web designers, developers, and other new media professionals working together to refresh the creative, technical, and professional aspects of their trades in greater Washington, DC.</p>\r\n<p>These events are a great place to meet incredibly creative and highly intelligent web people in the DC area. Refresh has brought out an vigorous community that few people knew existed in this town known more for government contracting and politics than web technologies. It's Silicon Valley of the east...or at least getting there.</p>", "date_published": "2007-09-20 18:43:09", "comment_count": 0, "slug": "refresh-dc-web-widgets-what-why-and-how"}}, {"pk": 9, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 4, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Howdy", "excerpt": "", "content": "Thanks for reading the SunlightLabs Blog!\u00a0 I'm John Brothers, and I'm the new CTO of the lab.\u00a0\u00a0 We're going to be doing a lot more with this blog over the coming months, so look for articles on events, releases of our open source projects, tutorials, advice, commentary and a whole lot more!", "date_published": "2007-09-20 15:07:41", "comment_count": 0, "slug": "howdy"}}, {"pk": 8, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 1, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Using RSS for House Committee Schedule Feeds", "excerpt": "", "content": "Based on John Wonderlich's work on the <a href=\"http://theopenhouseproject.com\" title=\"open house project\">Open House Project</a> and Joshua Ruihley's work on <a href=\"http://openhearings.org/live\" title=\"open hearings\">Open Hearings</a>, we here in the <a href=\"http://sunlightlabs.com\" title=\"Sunlight Labs\">Labs</a> decided to write a 'dream feed' as an example of an RSS format congress could use to syndicate their committee meeting schedule.\r\n\r\n<a href=\"http://sunlightlabs.com/projects/openhouse/schedule-v01.xml\" title=\"link to feed\" type=\"text/xml\">http://sunlightlabs.com/projects/openhouse/schedule-v01.xml</a>\r\n\r\n<code><pre>\r\n&lt;?xml version=\"1.0\"?&gt;\r\n&lt;rss version=\"2.0\"\r\n    xmlns:xcal=\"urn:ietf:params:xml:ns:xcal\"\r\n    xmlns:enc=\"http://www.solitude.dk/syndication/enclosures/\"&gt;\r\n  &lt;channel&gt;\r\n    &lt;title&gt;House Committee Schedule&lt;/title&gt;\r\n    &lt;link&gt;http://house.gov/schedule/&lt;/link&gt;\r\n    &lt;description&gt;Master schedule for house committees.&lt;/description&gt;\r\n    &lt;language&gt;en-us&lt;/language&gt;\r\n    &lt;lastBuildDate&gt;Fri, 06 Jul 2007 09:39:21 EDT&lt;/lastBuildDate&gt;\r\n    &lt;docs&gt;http://www.rssboard.org/rss-2-0-9&lt;/docs&gt;\r\n    &lt;generator&gt;House Schedule Generator 1.0&lt;/generator&gt;\r\n    &lt;webMaster&gt;schedule@house.gov&lt;/webMaster&gt;\r\n    &lt;ttl&gt;3600&lt;/ttl&gt;\r\n    &lt;item&gt;\r\n      &lt;title&gt;Subcommittee on General Farm Commodities and Risk Management&lt;/title&gt;\r\n      &lt;link&gt;http://agriculture.house.gov/hearings/schedule.html&lt;/link&gt;\r\n      &lt;description&gt;To review trading of energy-based derivatives.&lt;/description&gt;\r\n      &lt;pubDate&gt;Fri, 06 Jul 2007 09:39:21 EDT&lt;/pubDate&gt;\r\n      &lt;guid&gt;http://agriculture.house.gov/hearings#200707121000&lt;/guid&gt;\r\n      &lt;category domain=\"http://house.gov/schedule/type\"&gt;hearing&lt;/category&gt;\r\n      &lt;category domain=\"http://house.gov/schedule/visibility\"&gt;public&lt;/category&gt;\r\n      &lt;xcal:organizer cn=\"House Committee on Agriculture\"&gt;http://agriculture.house.gov&lt;/xcal:organizer&gt;\r\n      &lt;xcal:location&gt;1300 Longworth House Office Building, Washington, DC&lt;/xcal:location&gt;\r\n      &lt;xcal:dtstart&gt;2007-07-12T10:00:00Z&lt;/xcal:dtstart&gt;\r\n      &lt;xcal:dtend&gt;2007-07-12T11:00:00Z&lt;/xcal:dtend&gt;\r\n      &lt;xcal:attendee role=\"CHAIRMAN\" cn=\"Rep. John Doe\"&gt;mailto:john.doe@house.gov&lt;/xcal:attendee&gt;\r\n      &lt;xcal:attendee role=\"OPT-PARTICIPANT\" cn=\"Rep. Joe Smith\"&gt;mailto:joe.smith@house.gov&lt;/xcal:attendee&gt;\r\n      &lt;xcal:attendee role=\"X-WITNESS\" cn=\"Dan Johnson\"&gt;mailto:djohnson@company.com&lt;/xcal:attendee&gt;\r\n      &lt;enc:enclosure title=\"Live Media\"&gt;\r\n        &lt;enc:link type=\"video/quicktime\" length=\"11534336\"\r\n          url=\"http://agriculture.house.gov/hearings/200707121000.mov\" /&gt;\r\n        &lt;enc:link type=\"audio/mpeg\" length=\"11534336\"\r\n          url=\"http://agriculture.house.gov/hearings/200707121000.mp3\" /&gt;\r\n      &lt;/enc:enclosure&gt;\r\n      &lt;enc:enclosure title=\"Archived Media\"&gt;\r\n        &lt;enc:link type=\"video/quicktime\" length=\"11534336\"\r\n          url=\"http://agriculture.house.gov/hearings/archive/200707121000.mov\" /&gt;\r\n        &lt;enc:link type=\"audio/mpeg\" length=\"11534336\"\r\n          url=\"http://agriculture.house.gov/hearings/archive/200707121000.mp3\" /&gt;\r\n      &lt;/enc:enclosure&gt;\r\n      &lt;enc:enclosure title=\"Agenda\"&gt;\r\n        &lt;enc:link type=\"application/pdf\" length=\"11534\"\r\n          url=\"http://agriculture.house.gov/hearings/200707121000_agenda.pdf\" /&gt;\r\n      &lt;/enc:enclosure&gt;\r\n    &lt;/item&gt;\r\n  &lt;/channel&gt;\r\n&lt;/rss&gt;\r\n</pre>\r\n</code>\r\n\r\n\r\nThe feed was written to the <a href=\"http://www.rssboard.org/rss-2-0-9\" title=\"specification\">RSS 2.0.9 spec</a>. Since RSS is meant for syndication, two additional modules, elements belonging to a namespace, were used to add the information that was needed. The <a href=\"http://tools.ietf.org/html/draft-royer-calsch-xcal-03\" title=\"iCalendar in XML\">xCal XML specification</a> was used to add event data to the feed. xCal is an XML representation of the widely used iCalendar format. The format is fairly straighforward except for the organizer and attendee elements. The specification requires the value of these elements to be a URI rather than a string naming the individual. The URI can be any identifier, but is typically the email address of the individual. To include the name of the individual, a cn attribute must be added to the tag. The attendee element has a role attribute that describes the role in which the attendee will be serving. We've added a custom X-WITNESS role to identify witnesses that are testifying before a committee.\r\n\r\n\r\nAdditionally, we have a need to add multiple enclosures to the feed to represent different types of media: audio and video of events and documents related to the event. Since the RSS 2.0.9 spec is a bit unclear as to whether multiple enclosures are supported we decided to use Andreas Pedersen's <a href=\"http://www.solitude.dk/archives/20050208-0045/\" title=\"multiple enclosures in RSS\">multiple enclosure extension</a>. It's fairly straightforward and allows us to link to multiple types of related documents and media.\r\n\r\nYour feedback is welcome and desired! We'd like to make this sample feed as good as it can be to serve as an example of the Right Way to syndicate congressional schedules so any fixes or additions are quite appreciated. We'll post all changes to the blog. ", "date_published": "2007-07-10 20:32:01", "comment_count": 2, "slug": "using-rss-for-house-committee-schedule-feeds"}}, {"pk": 7, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 3, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "CiviCRM integrating SunlightLabs API", "excerpt": "", "content": "David  announced <a href=\"http://civicrm.org/node/205\">CiviCRM integration with our SunlightLabs API</a> today. This is more exciting stuff. In an email to us, David noted the business implications:\r\n<blockquote>\r\n<ol>\r\n\t<li><em>People could automatically email all the people in the database within a particular district.</em></li>\r\n\t<li><em>You can have a tab in a contact record automatically populated with political information.</em></li>\r\n</ol>\r\n</blockquote>\r\nIn his blog post, David complimented the documentation of <a href=\"http://sunlightlabs.com/api\">the SunlightLabs API</a> (to which we have to thank <a href=\"http://taggel.com/\">Labs alumni Dr. Carl Anderson</a> who also created the first version of the API modeling it and the documentation from Flickr's excellent API). He also politely pointed out our need to blog a bit more (this is a start!) and suggested some more improvements.\r\n\r\nThe <a href=\"http://sunlighfoundation.com\">SunlightFoundation.com website</a> runs on Drupal and CiviCRM, so we ourselves will be a beneficiary of this integration. W0ot!", "date_published": "2007-07-05 16:44:13", "comment_count": 0, "slug": "civicrm-integrating-sunlightlabs-api"}}, {"pk": 6, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 2, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Ajaj: Ajax2.0", "excerpt": "", "content": "Is Ajaj the new Ajax?\r\n\r\n<em><strong>Ajax: Asynchronous Javascript and XML</strong></em>. I wonder though how many people actually use XML in web2.0 applications. After all, an XMLHttpRequest can return both responseXML and responseText. My bet is that text, including <strong>*J*</strong>SON, is used <em>far</em> more frequently than XML.\r\n\r\nIf one can do all the data processing server-side, send it back as HTML text to the browser, and the browser needs only to set some innerHTML, why bother with XML? Well, you respond, \"what if I have structured data and different pieces of data need to go to different parts of the page: a title here, an image URL over there, some content here. Then, innerHTML won't work. You need to send over data plus descriptive metadata so that the browser code knows what it is.\" While, true, I am still not convinced that XML is used that much. First, one can send different chunks of data in simple responseText. For instance, one might send back a comma- or pipe-delimited string. The javascript does a split on that character and so long as the correct ordering of those data pieces is preserved, the data can be separated and sent where it needs to go in the page. For more complex data, or for a more robust data transfer one can use JSON in the responseText.\r\n\r\nI've been developing <a href=\"http://sunlightlabs.com/api/\">an API for the labs</a> recently and each method can produce JSON and XML. It was really striking to me just how easy JSON is to consume in the browser. One gets all the benefits of metadata markup, but it is both very compact and extraordinarily simple to  reference those data items in Javascript. For instance, I send\r\n<div style=\"padding: 10px; background: #ffcc66 none repeat scroll 0% 50%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial\">{ name : \"Joe Doe\", title : \"VP, Engineering\", phone : \"13-456-7890\"}</div>\r\n<em>[<strong>Update</strong> : as the comment below points out, it should have read: { \"name\" : \"Joe Doe\", \"title\" : \"VP, Engineering\", \"phone\" : \"13-456-7890\"}] </em>\r\n\r\nin my responseText. In Javascript I need only do the following:\r\n<div style=\"padding: 10px; background: #ffcc66 none repeat scroll 0% 50%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial\">var obj = eval( '(' + XMHttpRequest.responseText + ')' );</div>\r\nand now I reference those individual data items as obj.name, obj.title and obj.phone. Could it be any simpler?\r\n\r\nWell, I thought that until I really started playing around with JSON and found that it has frustrating limits, ones that JSON.org does not highlight <strike>(dare I say lie?)</strike> in their documentation.\r\n\r\nTake the following code:\r\n<div style=\"padding: 10px; background: #ffcc66 none repeat scroll 0% 50%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial\">var j1 = \"{ data: \\\"ab\\\"}\";\r\nvar obj1 = eval( \"(\" + j1 + \")\" );\r\nalert(obj1.data);</div>\r\nwhat is alerted? \"ab\". Correct.\r\n\r\nHow about this:\r\n<div style=\"padding: 10px; background: #ffcc66 none repeat scroll 0% 50%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial\">var j1 = \"{ data: \\\"a\\nb\\\"}\";\r\nvar obj1 = eval( \"(\" + j1 + \")\" );\r\nalert(obj1.data);</div>\r\nWhat is alerted? \"a\\nb\". Correct? Wrong. It bails.\r\n\r\n<strong><em>JSON, or rather the Javascript parser, cannot handle \\n.\r\n</em></strong>\r\nThat's strange I swear that \\n is shown on JSON's wonderfully easy to understand documentation. Let's take a look:\r\n\r\n<img width=\"468\" height=\"357\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2007/03/string.gif\" />\r\n\r\nYes, that's clear. JSON.org says that '\\n' is allowed in JSON. <a href=\"http://framework.zend.com/issues/browse/ZF-504\">I was not the only one to find this \\n limitation</a>:\r\n<div style=\"padding: 10px; background: #ffcc66 none repeat scroll 0% 50%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial\">\"As for the second, regarding newline characters, http://www.json.org/ says they are valid JSON notation\"</div>\r\n<div style=\"padding: 10px; background: #ffcc66 none repeat scroll 0% 50%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial\">Sort of, but it also states that \"It is based on a subset of the JavaScript Programming Language, Standard ECMA-262 3rd Edition - December 1999\", which clearly forbids them (par. 7.3)</div>\r\n<strike>So it is a lie then. JSON does not support \\n.</strike>\r\n\r\n<strike>Not only is this annoying but, in my mind, it does limit JSON usage.  I cannot  use it to transport arbitrary chunks of text.</strike>\r\n\r\n<hr /><strong>UPDATE</strong>: so at here Ajaxworld in NYC, I just asked Douglas Crockford, inventor of JSON, about this. So, it turns out that \\n needs to be escaped properly. Thus, the newline character should have been \\\\n\r\nThat is,\r\n<div style=\"padding: 10px; background: #ffcc66 none repeat scroll 0% 50%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial\">var j1 = \"{ data: \\\"a\\\\nb\\\"}\";\r\nvar obj1 = eval( \"(\" + j1 + \")\" );\r\nalert(obj1.data);</div>\r\ndoes work. JSON can handle arbitrary chunks of text so long as unicode characters are properly escaped. Whoops sorry about that. Off to eat some humble pie...\r\n<hr />", "date_published": "2007-03-11 16:26:33", "comment_count": 1, "slug": "ajaj-ajax20"}}, {"pk": 5, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 2, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Instant APIs: no code necessary, just add mouseclicks", "excerpt": "", "content": "<img id=\"image10\" alt=\"dapper logo\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2007/02/dappit-flower-small-multicolor.thumbnail.gif\" />\r\n\r\n<img width=\"125\" height=\"23\" id=\"image11\" alt=\"openkapow\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2007/02/kapowlogo.thumbnail.jpg\" />\r\n\r\nAPIs and mashups are the bread and butter of the Sunlight Labs. In fact, the official name of the the Labs is actually The Sunlight <em>Mashup</em> Labs, so it is perhaps not surprising that we headed off to the <a href=\"http://wiki.mashupcamp.com/index.php/WhosComingToMashupCamp3\">MashupCamp 3</a> at MIT, Cambridge a couple of weeks ago to geek out and to see the latest happenings, trends and players in the world of mashups and technology.\r\n\r\nThere was a lot of cool stuff demoed, debated and hacked at the camp but one theme that really stood out for me was tools that made scraping easier and more effective than ever before. In short, tools to create instant APIs, without programming \u2014 yes, really.\r\n\r\nTo be more concrete: suppose there is a website that you visit every day and you read the headlines. However, you prefer to read these stories in your favorite RSS reader but this website does not provide an RSS feed. You can use these tools to create such a feed yourself.\r\n\r\nOr, suppose you want to buy a second-hand iBook from Craigslist and are not in a rush; you would rather wait for the right deal. Well, you could manually check the listings everyday but you would rather be alerted if an iBook comes up for sale within a certain distance from home and under a certain price. To do this programmatically, you need to grab the listings for computer ads and then you can parse them to see in any match your criteria. You can use these tools to create an API for Craigslist computer ads, under a search query of \"iBook\", in a matter of minutes.\r\n\r\nAnything that is presented on a webpage, certainly a static webpage, is something that could be scraped. The problem is that scraping is not easy. Quality of HTML structure varies considerably. While some sites always use valid W3C XHTML, use CSS with lots of descriptive class names and sensible divs that divide different sections of the page, many do not. Do a view source of a random <a href=\"http://profile.myspace.com/index.cfm?fuseaction=user.viewprofile&friendid=154222679&MyToken=a044c8f0-98ad-403d-bbb9-08528fe02798\">myspace page</a> and you will see what I mean: it is a huge mess.\r\n\r\nSo, to scrape a page when it does not have a clear or consistent structure, or to scrape a page that requires a login (say, your AOL buddy list) is not at all easy. It can be done but it takes time and, moreover, because one is scraping who knows how long that scraper will last. The content provider makes a change in page structure and your scraper has to be reworked. Finally, one has to be a coder: you need to use tools such as curl or php to get the page contents, you may need tools such as html parsers to navigate the content, and you may need competency in regex to pull out precisely what you need from the page. Finally, you may actually need a server to create a page to display the final, desired content; and not everyone has such resources. (We will see below how one  can dispense with this latter requirement.) Any tools that provide instant APIs from arbitrary webpages, especially for non-progammers, has to be good news.\r\n\r\nTwo organizations in this space who demoed at Mashup Camp are <a href=\"http://www.dappit.com/\">Dapper</a> and <a href=\"http://openkapow.com/\">OpenKapow</a>. Now that I've had a chance to play around with these technologies, I want to take this opportunity here to review them, giving an unbiased critique.\r\n\r\n<strong>Dapper</strong>\r\n\r\nDapper is a US startup based in Israel founded by Eran Shir and Jon Aizen and is currently in open beta. While all services are free at the moment, large projects or commercial uses of Dapper may be charged in the future. However, if Dapper works as well as it is claimed then I can see many organizations making good use of Dapper and saving in development costs even in a fee-based structure. Why do I say this? What precisely is Dapper?\r\n\r\nDapper is a web-based tool and service for scraping, creating APIs from potentially any webpage and then generating output of that API in a variety of formats including HTML, XML, RSS and even Google Maps.\r\n\r\nOne starts by entering a URL and the webpage is displayed within Dapper. One can click through different pages and add each page to a \"basket\". Thus, one could add say pages 1, 2, 3, 4 of a blog. Dapper then analyzes these pages to work out the structure, say what is a static header and footer and what is dynamic content. One then gets to \"play\". That is, clicking on a story title (should) highlights all other story titles in the page. As such, one is confident that Dapper will grab the correct content from the page when you specify precisely what you want your API, or \"Dapp\", should do.\r\n\r\nI have to say that I had mixed success with this. While Dapper correctly identified the story titles for techcrunch.com and http://www.followthemoney.org/Newsroom/index.phtml, it did not do so on sunlightfoundation.com and it could not work out my intention of grabbing rows or columns on http://opensecrets.org/orgs/list.asp?order=A (but it could get the org titles). However, this is still a beta and Dapper certainly does work: there are hundreds of user-created Dapps that one can browse and use.\r\n\r\nAfter playing, one can then progress to creating the API proper: one clicks on a desired element, gives it a name and one can then define a group, e.g. specify that this story title, this author name and this number of diggs are all related as one unit. After that, one can preview the API, i.e. check what is pulled out by the API. If all is well, one is done. Then the real fun begins...\r\n\r\nDapper provides the API in a impressive variety of formats: XML, HTML, RSS, Alerts, iCalendar (transforms the output of a Dapp into an iCalendar which can be used in Google Calendar, Sunbird, iCal, and other programs), Google maps (places locations directly onto a map), Google gadgets, Netvibes, image loop, email, link to another Dapp, CSV, JSON, YAML, XSL, and fork it as another Dapp. This APIs are published and hosted at Dapper (hence one doesn't need their own server to provide an API). Dapper also allows one to define parameters for the API such as a {query} or {page}.\r\n\r\nSome of the resultant URLs are mess though. So, Dapp allows one to define a service that provides a nice clean URL. That is, instead of say http://www.dappit.com/RunDapp?dappName=sunlightlabs&v=1&thisparam=y&thatparam=z... one can instead provide users with say http://www.dappit.com/services/sunlightlabs.\r\n\r\nAnother nice feature is their AggregatorAid that allows one to pool Dapps into one single service. Thus, if one has a suite of different individual search Dapps (Google, digg, reddit, etc) one can provide a single query that will aggregate the results as a single API. The only restriction is that each that individual Dapp must have the same query parameter name (I hope that this is relaxed in later versions).\r\n\r\nFinally, Dapper provides secure, authenticated login into sites. Thus, if you want to scrape your AOL buddies or comments on your Facebook page, Dapper allows you to create an API without exposing your username or password.\r\n\r\nOverall, Dapper is a clean, slick instant API generator that has some very nice features: it is web-based, has a clean interface, and provides API output in the majority of formats that anyone would want. It is beta, it is not perfect, but then it is free. One really can get a basic API scraped from a page, published, and up and running in literally 5 minutes. This is why I say that I can see a valid business model here. Organizations, especially non-profits such as ourselves, can try Dapper first to create a given API, and only if it doesn't work then create the API from scratch or outsource which will certainly be more time consuming and expensive.\r\n\r\n<strong>OpenKapow</strong>\r\n\r\nOpen Kapow, also in beta, founded by  \t      Stefan Andreasen in Denmark, is part of the larger Kapow Techologies which appears to have a large range of corporate clients and a number offices on both sides of \"the pond\".\r\n\r\nOpenKapow has the same goal of instant APIs as Dapper but takes a different approach. For a start, while Dapper is web-based, OpenKapow requires a large \"RoboMaker\" download (100+ Mb) for windows or linux. [So, as a mac guy that cuts me out. If one takes a look around at any hacker or mashup conference you will find a sea of macs, and these are the guys who are most likely to do such mashups. While macs are UNIX underneath the linux version does not install on macs.]\r\nInstalling the RoboMaker on windows, one is presented with a sophisticated Java-based tool for mashups. As in Dapper, a webpage is loaded and displayed within the tool and one can select various elements of the page. RoboMaker has a nice DOM JTreeView that one can explore the structure of the page and one can click various elements and they will be highlighted and at the same time, the selected item is also shown in an HTML viewer. Together, the three panels provide a better understanding of the page structure than Dapper.\r\nOne the right are a series of controls for selecting tags based on name, type, conditions etc. There really are a very large number of features and controls that (I would image) provide a very granular approach to scraping. Therein, however, lies the problem.\r\n\r\nThere were so many controls, so many right click context menus, that it was not at all obvious for the newbie where to begin. I failed to select all the h1 tags of the page despite trying various combinations of select *.h1 etc. I saw OpenKapow in action at mashupcamp and I did see Stefan demo this creating an API for Google search. It looked very easy...if you know what you are doing. I really do believe that OpenKapow is a more sophisticated tool that probably can deal with edge cases that Dapper cannot. It is a tool for hackers/programmers who must invest some time into understanding the tool (and getting the thing downloaded and installed) but it is certainly not a quick and easy hacker tool. While both will generate APIs without true coding, I feel that Dapper is an quick-and-dirty, intuitive 90% solution while OpenKapow is a 90%+ tool for more serious projects.\r\n\r\nPerhaps I have a short attention span but I have to admit that I gave up trying to get something basic working on OpenKapow. Besides, even if i did get it working, the output formats for OpenKapow are far more limited: (X)HTML, XML, REST, and RSS.\r\n\r\n<strong>Conclusions </strong>\r\n\r\nDapper especially allows one to try a quick hack. If that doesn't work then one could resort to writing it oneself or using a tool such as OpenKapow. OpenKapow looks as though it would be worth the effort to invest in learning all of the features if one was going to be doing lots of scraping and mashups, or in a corporate environment where one needed a very granular mashup that needed to work all the time. I think that <em>both</em> of these represent an exciting new era for mashups. They both represent programming-free tools for instant, shareable APIs. They both involve a reasonable GUI to select elements rather than needing to poke around the underlying HTML itself. As such, the bar for mashing up is significantly lowered.", "date_published": "2007-02-01 19:37:36", "comment_count": 0, "slug": "instant-apis-no-code-necessary-just-add-mouseclicks"}}, {"pk": 4, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 2, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "Why offline is the new online", "excerpt": "", "content": "<img width=\"113\" height=\"79\" alt=\"apollo logo\" title=\"apollo logo\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2007/01/apollologo.jpg\" />The history of computing has some interesting trends. First it was all about mainframes. To get anything computed you had to logon to some remote machine so large that if filled a room. Then came along the desktop; desktop apps were (and arguably still are) it. In the 1990's, however, things got pushed onto the web, web <em>sites</em> were the thing, and now with the rise of Ajax, web <em>apps</em> are becoming the new it. Applications such as <a href=\"http://www.google.com/support/writely/\">writely</a> (now \"google docs\") or <a href=\"http://www.google.com/googlespreadsheets/tour1.html\">google spreadsheets</a> that have the look, feel and responsiveness of traditional desktop applications.  I want to predict the next segment of this zig-zag computing trend: offline web apps on your desktop.\r\n\r\nIn the last couple of months there have been some very interesting developments. Adobe have staked their claim in this area with the announcement of <a href=\"http://labs.adobe.com/wiki/index.php/Apollo\">Apollo</a>, Mozilla is already in this arena with <a href=\"http://developer.mozilla.org/en/docs/XULRunner\">xulrunner</a>, and recently <a href=\"http://www.sitepen.com/blog/2007/01/02/the-dojo-offline-toolkit/\">dojo</a> announced a forthcoming offline library. Before we get into the specifics though, I want to dig a little deeper into some background and discuss the influence of widgets.\r\n\r\nWidgets are really hot now, small focussed applications that serve one or two useful purposes: show the local weather, a simple RSS reader, show what is playing on iTunes etc. The problem is that they are tied to particular browser\u2014<a href=\"http://widgets.opera.com/\">opera widgets</a> only work for the opera browser\u2014or tied to a particular operating system\u2014apple's <a href=\"http://www.apple.com/macosx/features/dashboard/\">dashboard widgets</a> only work on a mac\u2014or they must be served through a third party host (e.g. <a href=\"http://www.widgetbox.com/\">widgetbox</a>). This sucks for all involved: developers always want to write once, run anywhere but instead they have to write and maintain slightly different versions for lots of different platforms. Users don't want to search through some complicated table to work out which one they need to download. What we all want is a universal \"click to download and install\" widget, that does what it says, and that does what it says in the same way on all platforms with the same code. We want a simple app that will float on the window, do its thing.\r\n\r\nDesktop app are tied to native libraries for good reason: performance. Fast, efficient Cocoa apps for mac, .net for for windows etc. While Java claims to be universal, it is very slow to launch, there are still UI inconsistencies across platforms, and it can never reach the same degree of performance as native apps for compute-intense apps such as rendering and video CODECs. Widgets are different however. Their functions are typically so simple that it makes no difference. To grab and display the latest blog post entry's title from your favorite blog is hardly going to stress the CPU. So, they can be written in universal language such as HTML+CSS+JS.\r\nThe stupid thing is that widgets are all written in the same language: HTML+CSS+JS. Opera widgets and Apple's dashboard widgets are essentially the same under the hood but they just cannot run on each other platforms. Within 6 months we are, however, going to see more universal widgets and desktop apps written in web technology.\r\nAdobe is starting to promote its forthcoming Apollo platform. Apollo will provide libraries for offline storage, will have the ability to read and write to the desktop's file system (which is difficult to achieve in true web apps because of the built in security model), and will have libraries for flash, HTML and PDFs. So to make sure we are clear here: you will be able to write a web app in HTML and have it run universally on all platforms: windows, mac and linux, and further, the HTML can be rendered through the flash viewer so that it is virtually guaranteed to look and operate exactly the same on all of those platforms. As a developer, one can write once, make it generally available for download as an .air file, have it work on all platforms. Not only that but Apollo apps can continuously check for connectivity: you shut down your computer because you are getting onto a plane, no problem. After take off, you open it up and continue as before. Moreover, all of these offline activities or transactions are stacked up locally and processed online as soon as connectivity is restored. You can continue to work offline and it will seamlessly sync to online when it can. Oh, and did I say all of this will be free? [They will make their money through selling more Adobe Flex licenses to developers.] Apollo will be available later this year but the main feature they need to fix is distribution. Currently, one has to download an Apollo runtime and an apollo installer. This is not likely to engender uptake by the mainstream Joe Doe end user. However, unsurprisingly, they do have plans to distribute through the flash player so the back end installation will be seamless.\r\n\r\nMozilla already in this space through xulrunner. So, \"xulrunner --install -app gmail\" is all that is needed to create a desktop app for gmail. This will provide a local executable that will launch a window to gmail. This can be running independently of any browser. This is from Mozilla so of courser there are <a href=\"http://www.mozilla.com/en-US/firefox/\">Firefox</a> libraries running behind the scenes but the point is that I can have a persistent desktop application running up in say my top right corner of my screen, always there, always keeping me informed of whatever I want to keep informed about, without me having to have firefox or any other browser running and have that window or tab showing. This too will allow developers to write desktop applications in HTML+CSS+JS that run offline. Given the number of developers who know these technologies, this could be a huge boon for desktop app innovation. Again, this allows a write once run anywhere [but with less guarantees about consistent look and feel] and has local SQLlite database for local storage.\r\nDojo too is likely to be another important player. They are working on an <a href=\"http://www.sitepen.com/blog/2007/01/02/the-dojo-offline-toolkit/\">offline toolkit</a>, a \"small, cross-platform, generic       download that enables web applications to work offline.\" It will be cross platform, cross-browser, connectivity detecting, secure, with local storage. Sound familiar?\r\n\r\nLet me stress that one is not going to be able to take an existing complex, server-side PHP+MySQL web application, type a single command and have it instantly available as a desktop application. (Besides, would really trust your code, even if encrypted, out there on everyone's desktop?) However, this is an exciting area for web-innovation. Watch this space in the next six months. More and more of the web will be seeping onto your desktop. This is why I say offline is the new online.", "date_published": "2007-01-21 15:57:21", "comment_count": 1, "slug": "why-offline-is-the-new-online"}}, {"pk": 3, "model": "blogdor.post", "fields": {"status": "public", "last_updated": "2008-12-04 17:56:22", "author": 2, "timestamp": "2008-12-04 17:56:22", "markup": "none", "title": "The convergence of Ajax and Flash", "excerpt": "", "content": "<img width=\"184\" height=\"134\" align=\"top\" alt=\"smooth gallery\" title=\"smooth gallery\" src=\"http://sunlightlabs.com/blog/wp-content/uploads/2007/01/smoothgallery10.jpg\" />\r\n\r\nWhile reading a couple of posts in Ajaxian about  <a href=\"http://ajaxian.com/archives/jsflickrslideshow-sliding-through-flickr\">hacking the canvas tag with a flickr stream</a> and a <a href=\"http://ajaxian.com/archives/smooth-gallery-10-released\">mootools-based image gallery stream</a> it struck me just how sophisticated some Ajax-based UIs have become, so much so that it has become harder to tell what is and is not flash.\r\n\r\nThat is, not so long ago, flash was the only viable option for a variety of interactivity including drag and drop and sophisticated animation. However, over the last year a slew of Ajax libraries have been released such as <a href=\"http://dojotoolkit.org/\">dojo</a>, <a href=\"http://developer.yahoo.com/yui/\">Yahoo! UI</a>, <a href=\"http://en.wikipedia.org/wiki/Canvas_(HTML_element)\">canvas tag</a> has spread, especially with the release of <a href=\"http://excanvas.sourceforge.net/\">excanvas</a> that hacks this SVG type functionality for internet explorer. Now one can draw and animate curves in DHTML.\r\n\r\nNow don't get me wrong, flash is a great innovative, product (sometimes abused where the whole website is flash and you cannot link to anything) has a very thin client, and most importantly looks and feels the same on different browsers (without all those frustrating conditional CSS IE hacks). For this reason it will hold a strong position for a while to come yet, but I do feel that we will continue to see increasing innovation in Ajax libraries and a convergence of DHTML websites to have a flash-like look and feel.", "date_published": "2007-01-08 17:54:12", "comment_count": 0, "slug": "the-convergence-of-ajax-and-flash"}}]
